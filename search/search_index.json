{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Haoyang's Paper Collections 2021-2022 2021-2022 September Reading Reports Literature Review about NeRF A good overview about NeRF can be found at Instant-NGP 1 Encoding Frequency encodings Parametric encodings Spares parametric encodings Network NeRF NeRF-W Mip-NeRF Instant-NGP 2020 Oct 2020 October Reading Reports Stochastic Bundle Adjustment for Efficient and Scalable 3D Reconstruction This paper introduces an approach to solve the reduced camera system (RCS) efficiently. It formulates the problem into a quadratic programming first by applying point splitting. Then, change constrained relaxation is used to solve the problem partialy. It corrects the direction of the optimization when the \\(\\lambda\\) in LM algorithm is large by approximating \\(\\mathbf{H}_\\lambda\\) as \\(\\text{diag}(\\mathbf{H}_\\lambda)\\) . Stochastic graph clustering is used to split the whole problem into several sub-problems. 2020 Sep 2020 September Reading Reports GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized Gaussian process map reconstruction This work uses Gaussian process (GP) map reconstruction to provide state estimate and mapping. Each cell with valid map points are trained into three functions in the form of GP based on the relationship between the coordinates. Test locations for each function are set with fixed intervals to form a layer with a set of samples . A principled down-sample method is used to accelerate the training of GP. MLE based method taking the distance of projected target and source samples to the matched direction is utilized to perform alignment and refinement with the global map. Deep Probabilistic Feature-metric Tracking This paper first uses two-view to extract the spatial and temporal correlation between two frames by a spatio-temporal encoder pyramid network \\(\\phi_\\theta\\) . Then, feature encoder branch \\(\\phi_F\\) and the uncertainty encoder branch \\(\\phi_\\sigma\\) extract the feature and uncertainty, respectively. Inverse compositional formulation is used to facilitate the Gaussian Newton optimization. Combined with ICP and coarse-to-fine optimization, the network is trained by 3D EndPoint-Error (EPE) as the training loss. MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem This paper uses null-space projection, which solves the tangent space residuals linearly and followed by GN optimization. The nonsingular covariance is calculated by null-space matrix transformation. Distributed Variable-Baseline Stereo SLAM from two UAVs This paper introduces a method of two UAV stereo SLAM. It uses Asynchronous-parallel Alternating Direction Method of Multipliers (ADMM) to perform collaborative update of states. It contains EKF-based Pose Tracking, Mapping and Distributed Optimization Back-End, where UWB distance-measurements are used as a constraint and Z-spline is used for pose interpolation. Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry This paper extracts planes in the images and use RANSAC to filter the outlier. The parameterization makes the points and lines on the planes be represented by the plane coefficients, which is in \\(S^2 + \\mathbb{R}\\) (update the tangent space \\(S^2\\) ). M\u00fcller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989. \u21a9","title":"Home"},{"location":"#welcome-to-haoyangs-paper-collections","text":"2021-2022","title":"Welcome to Haoyang's Paper Collections"},{"location":"#2021-2022-september-reading-reports","text":"","title":"2021-2022 September Reading Reports"},{"location":"#literature-review-about-nerf","text":"A good overview about NeRF can be found at Instant-NGP 1","title":"Literature Review about NeRF"},{"location":"#encoding","text":"Frequency encodings Parametric encodings Spares parametric encodings","title":"Encoding"},{"location":"#network","text":"NeRF NeRF-W Mip-NeRF Instant-NGP 2020 Oct","title":"Network"},{"location":"#2020-october-reading-reports","text":"","title":"2020 October Reading Reports"},{"location":"#stochastic-bundle-adjustment-for-efficient-and-scalable-3d-reconstruction","text":"This paper introduces an approach to solve the reduced camera system (RCS) efficiently. It formulates the problem into a quadratic programming first by applying point splitting. Then, change constrained relaxation is used to solve the problem partialy. It corrects the direction of the optimization when the \\(\\lambda\\) in LM algorithm is large by approximating \\(\\mathbf{H}_\\lambda\\) as \\(\\text{diag}(\\mathbf{H}_\\lambda)\\) . Stochastic graph clustering is used to split the whole problem into several sub-problems. 2020 Sep","title":"Stochastic Bundle Adjustment for Efficient and Scalable 3D Reconstruction"},{"location":"#2020-september-reading-reports","text":"","title":"2020 September Reading Reports"},{"location":"#gp-slam-real-time-3d-lidar-slam-based-on-improved-regionalized-gaussian-process-map-reconstruction","text":"This work uses Gaussian process (GP) map reconstruction to provide state estimate and mapping. Each cell with valid map points are trained into three functions in the form of GP based on the relationship between the coordinates. Test locations for each function are set with fixed intervals to form a layer with a set of samples . A principled down-sample method is used to accelerate the training of GP. MLE based method taking the distance of projected target and source samples to the matched direction is utilized to perform alignment and refinement with the global map.","title":"GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized Gaussian process map reconstruction"},{"location":"#deep-probabilistic-feature-metric-tracking","text":"This paper first uses two-view to extract the spatial and temporal correlation between two frames by a spatio-temporal encoder pyramid network \\(\\phi_\\theta\\) . Then, feature encoder branch \\(\\phi_F\\) and the uncertainty encoder branch \\(\\phi_\\sigma\\) extract the feature and uncertainty, respectively. Inverse compositional formulation is used to facilitate the Gaussian Newton optimization. Combined with ICP and coarse-to-fine optimization, the network is trained by 3D EndPoint-Error (EPE) as the training loss.","title":"Deep Probabilistic Feature-metric Tracking"},{"location":"#mlpnp-a-real-time-maximum-likelihood-solution-to-the-perspective-n-point-problem","text":"This paper uses null-space projection, which solves the tangent space residuals linearly and followed by GN optimization. The nonsingular covariance is calculated by null-space matrix transformation.","title":"MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem"},{"location":"#distributed-variable-baseline-stereo-slam-from-two-uavs","text":"This paper introduces a method of two UAV stereo SLAM. It uses Asynchronous-parallel Alternating Direction Method of Multipliers (ADMM) to perform collaborative update of states. It contains EKF-based Pose Tracking, Mapping and Distributed Optimization Back-End, where UWB distance-measurements are used as a constraint and Z-spline is used for pose interpolation.","title":"Distributed Variable-Baseline Stereo SLAM from two UAVs"},{"location":"#co-planar-parametrization-for-stereo-slam-and-visual-inertial-odometry","text":"This paper extracts planes in the images and use RANSAC to filter the outlier. The parameterization makes the points and lines on the planes be represented by the plane coefficients, which is in \\(S^2 + \\mathbb{R}\\) (update the tangent space \\(S^2\\) ). M\u00fcller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989. \u21a9","title":"Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry"},{"location":"2021-2022/","text":"2021-2022 September Reading Reports Literature Review about NeRF A good overview about NeRF can be found at Instant-NGP 1 Encoding Frequency encodings Parametric encodings Spares parametric encodings Network NeRF NeRF-W Mip-NeRF Instant-NGP M\u00fcller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989. \u21a9","title":"2021-2022"},{"location":"2021-2022/#2021-2022-september-reading-reports","text":"","title":"2021-2022 September Reading Reports"},{"location":"2021-2022/#literature-review-about-nerf","text":"A good overview about NeRF can be found at Instant-NGP 1","title":"Literature Review about NeRF"},{"location":"2021-2022/#encoding","text":"Frequency encodings Parametric encodings Spares parametric encodings","title":"Encoding"},{"location":"2021-2022/#network","text":"NeRF NeRF-W Mip-NeRF Instant-NGP M\u00fcller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. arXiv preprint arXiv:2201.05989. \u21a9","title":"Network"},{"location":"2018/2018%20Apr/","text":"2018 April Reading Reports CamOdoCal Monocular VO Initial Estimate of Camera - Odometry Transform Unified projection model proposed by Mei is used. Both of the intrinsic of the cameras and extrinsic parameters between cameras and odometry are calculated and SURF features are used. Hand-eye problem for initialization: \\[ q^{O_{i+1}}_{O_{i}}q^{O}_{C}=q^{O}_{C}q^{C_{i+1}}_{C_{i}} (1) \\\\ (R^{O_{i+1}}_{O_{i}} - I)t^O_C=s_jR^O_Ct^{C_{i+1}}_{C_{i}}-t^{O_{i+1}}_{O_{i}} (2) \\\\ q^{O_{i+1}}_{C_{i}}=q^{O_{i+1}}_{C_{i}} \\\\ t^{O_{i+1}}_{O_iC_i}-t^{O_{i+1}}_{O_{i+1}C_{i+1}}=s_jt^{O_{i+1}}_{C_{i+1}C_{i}}-t^{O_{i+1}}_{O_{i+1}O_i} \\] Assume that \\(q^{O_{i+1}}_{O_{i}}\u200b\\) has only rotation on z axis. Then \\(q_z\u200b\\) and \\(q^{O_{i+1}}_{O_{i}}\u200b\\) are commutable, which can be solved by the planar hand eye calibration for initialization (https://github.com/hengli/camodocal/blob/master/src/calib/PlanarHandEyeCalibration.cc). For \\(q_{yx}\\) , we have two constraints, \\(x_{q_{yx}}y_{q_{yx}}=-z_{q_{yx}}w_{q_{yx}}\\) and \\(q_{yx}^Tq_{yx}=1\\) . These two equations will constraint the two least eigen vectors from SVD. \\(q_{yx} = \u03bb_1v_3 +\u03bb_2v_4\\) , in the program above, it is solved by the solveQuadraticEquation function, which solves \\(s = \\frac{\u03bb_1}{\u03bb_2}, as^2+bs+c=0\\) and \\((sv_3+v_4)^2=t^2, \u03bb_2 = 1/t, \u03bb_1 = s/t\\) . Use (2) to obtain the initial values of yaw (chose the the best hypothesis from the least square problem), 2D translations and scales. 3D Point Triangulation to find feature correspondences and Ceres to optimize the image reprojection error (bundle adjustment) across all frames. Finding Local Inter-Camera Feature Point Correspondences Keep local frame history. The image pair is rectified on a common image plane which corresponds to the average rotation between the first camera\u2019s pose and the second camera\u2019s pose Loop Closures Most of the loops are last frame in one monocular VO segment for a particular camera, and the first frame in the next monocular VO segment for the same camera. Full Bundle Adjustment Optimizes all intrinsics, extrinsics, odometry poses, and 3D scene points. MSCKF 2.0 MSCKF Brief Introduction The effects of features for the linearization are avoided by multiply left nullspace of \\(H_f\\) . The feature in all the images are used for calculating the final position of the feature. The feature point is not in the state vector. MSCKF 2.0, MSCKF, EKF-SLAM Comparison It shows that MSCKF and EKF-SLAM has inconsistency. Global parameterization of the rotation \\(^I_GR \\approx \\hat{^I_GR}(\\mathbf{I}_3 - \\lfloor\\delta^G\\tilde{\\theta}\\rfloor_\\times)\\) , which removes the orientation error from the observable matrix. The authors also use first estimate Jacobian for not letting errors increase the rank of the observable matrix (a novel closed-form expression for the IMU error-state transition matrix and fixed linearization states). Compare to the original MSCKF, it also includes extrinsic parameters. The Monte-Carlo simulations part can be adapted for other tests. Having a linearized system model with appropriate observability properties is more important than using re-linearization to better approximate the nonlinear measurement models. The uncertainty of yaw and position are underestimated by MSCKF and FLS. Probabilistic Surfel Fusion for Dense LiDAR Mapping Main idea Dual surfel maps are used, ellipsoid surfel map (ESM) is used to do localization, while disk surfel map (DSM) is used to fuse the map. Uncertainty model is used to represent the surfel. Surfel uncertainty and fusion are the keys of the paper. For ESM, the paper doesn't discuss a lot. For DSM, the data association is done by first octree-based nearest neighbor search algorithm, then thresholds-based method to pick the correspondences. The position and normal of the surfel are updated by by Bayesian filtering, including the mean and covariance. Unstable surfels which are not observed for a certain period of time is removed when revisited. The tests are performed under synthetic and real environments. It shows that the method have less noise in the fused map. Continuous 3D Scan-Matching with a Spinning 2D Laser Main idea This paper uses 3D spinning lidar sensor on a skid-steer loader vehicle to produce quality map of outdoor scenes and estimate the vehicle trajectory. The shape parameters are from mean and covariance and eigenvalues. Matching is based on ICP with first compute correspondences (centroid, smallest eigenvector and largest eigenvector), then estimate the trajectories with three constraints: match constraints, smoothness constraints and initial constraints. The industrial environment test is compared to 2D laser SLAM system, while off-road environment test is compared to closed-loop results (appearance-based methods). Box plots are used to visualize the translational and rotational errors. Generalized-ICP This can be thought of as \u2018plane-to-plane\u2019. Outperform both standard ICP and point-to-plane and to be more robust to incorrect correspondences. More expressive probabilistic models. Addition of outlier terms, measurement noise, and other probabilistic techniques to increase robustness. G-ICP aims to take into account structure. ICP Compute correspondences between the two scans. Compute a transformation which minimizes distance between corresponding points. Point-to-plane ICP improves performance by taking advantage of surface normal information Generalized-ICP Standard Euclidean distance to find correspondences. Kd-trees in the look up of closest points. Outliers are removed. Probabilistic model models the covariance matrices associated with the measured points. Transform a to b. \\[ d_i^{(\\mathbf{T})} \\thicksim \\mathcal{N}(\\hat{b_i}-(\\mathbf{T^*})\\hat{a_i}, C^B_i+(\\mathbf{T^*})C^A_i(\\mathbf{T^*})^T) \\\\ = \\mathcal{N}(0, C^B_i+(\\mathbf{T^*})C^A_i(\\mathbf{T^*})^T)) \\\\ \\mathbf{T} = \\text{argmax}_{\\mathbf{T}} \\prod_i p(d_i^{(\\mathbf{T})}) = \\text{argmax}_{\\mathbf{T}} \\sum_i \\log(p(d_i^{(\\mathbf{T})})) \\\\ \\mathbf{T} = \\text{argmin}_{\\mathbf{T}} \\sum_i ({d_i^{(\\mathbf{T})}}^T(C^B_i+(\\mathbf{T})C^A_i(\\mathbf{T})^T))^{-1}d_i^{(\\mathbf{T})}), \\text{(from multi-gaussian)} \\] Standard ICP can be seen as \\(C^A_i = 0, C^B_i = I\\) , while point-to-plane as \\(C^A_i = 0, C^B_i = P_i^{-1}\\) , where \\(P_i = A(A^T A)^{-1}A^T\\) and \\(A = v_i\\) is the direction of the normal. Since \\(P_i\\) is non-invertible, we need to approximate it with an invertible \\(Q_i\\) . Locally planar assumption. Plane-to-plane: Increase the symmetry of the model, constraint along its surface normal, but not exact correspondence. High covariance along its local plane, and very low covariance in the surface normal direction. \\[ \\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\\\ C_i^A = R_{\\nu_i}\\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}R_{\\nu_i}^T \\\\ C_i^B = R_{\\mu_i}\\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}R_{\\mu_i}^T \\] The incorrect correspondences form very weak and uninformative constraints. PCA (20 closest points) is used to recover surface normals from point clouds. The rotation matrices in the above equations are replaced by eigen decomposition \\(\\hat{\\Sigma} = UDU^T\\) , where \\(D\\) is replaced with \\(\\text{diag}(\\epsilon,1,1)\\) . Standard ICP was used in the pairwise matching to generate the ground truth. The testing were extracted with much higher spacing. The proposed method is less sensitive to \\(d_{max}\\) and give equal consideration to both scans (removed local minima in point-tp-plane). But how the normal in source scans to be extracted will still be a problem. Visual\u2013inertial navigation, mapping and localization: A scalable real-time causal approach Explains why extrinsic parameters are observable. It is related to identifiability given the measurements. Motion model Under constant dynamic model, or trivial dynamics (constant, slowly varying, or random walk). Observability The author proves that under that motion model the model (4)\u2013(5) and \\(T_{bi}\\) , \\(R_{bi}\\) , \\(\\gamma\\) (gravity vector) added to the state with constant dynamics, is locally observable, so long as motion is sufficiently exciting and the global reference frame is fixed (equivalently the initial conditions ( \\(R(0)\\) , \\(T(0)\\) )).","title":"2018 April Reading Reports"},{"location":"2018/2018%20Apr/#2018-april-reading-reports","text":"","title":"2018 April Reading Reports"},{"location":"2018/2018%20Apr/#camodocal","text":"","title":"CamOdoCal"},{"location":"2018/2018%20Apr/#monocular-vo","text":"","title":"Monocular VO"},{"location":"2018/2018%20Apr/#initial-estimate-of-camera-odometry-transform","text":"Unified projection model proposed by Mei is used. Both of the intrinsic of the cameras and extrinsic parameters between cameras and odometry are calculated and SURF features are used. Hand-eye problem for initialization: \\[ q^{O_{i+1}}_{O_{i}}q^{O}_{C}=q^{O}_{C}q^{C_{i+1}}_{C_{i}} (1) \\\\ (R^{O_{i+1}}_{O_{i}} - I)t^O_C=s_jR^O_Ct^{C_{i+1}}_{C_{i}}-t^{O_{i+1}}_{O_{i}} (2) \\\\ q^{O_{i+1}}_{C_{i}}=q^{O_{i+1}}_{C_{i}} \\\\ t^{O_{i+1}}_{O_iC_i}-t^{O_{i+1}}_{O_{i+1}C_{i+1}}=s_jt^{O_{i+1}}_{C_{i+1}C_{i}}-t^{O_{i+1}}_{O_{i+1}O_i} \\] Assume that \\(q^{O_{i+1}}_{O_{i}}\u200b\\) has only rotation on z axis. Then \\(q_z\u200b\\) and \\(q^{O_{i+1}}_{O_{i}}\u200b\\) are commutable, which can be solved by the planar hand eye calibration for initialization (https://github.com/hengli/camodocal/blob/master/src/calib/PlanarHandEyeCalibration.cc). For \\(q_{yx}\\) , we have two constraints, \\(x_{q_{yx}}y_{q_{yx}}=-z_{q_{yx}}w_{q_{yx}}\\) and \\(q_{yx}^Tq_{yx}=1\\) . These two equations will constraint the two least eigen vectors from SVD. \\(q_{yx} = \u03bb_1v_3 +\u03bb_2v_4\\) , in the program above, it is solved by the solveQuadraticEquation function, which solves \\(s = \\frac{\u03bb_1}{\u03bb_2}, as^2+bs+c=0\\) and \\((sv_3+v_4)^2=t^2, \u03bb_2 = 1/t, \u03bb_1 = s/t\\) . Use (2) to obtain the initial values of yaw (chose the the best hypothesis from the least square problem), 2D translations and scales.","title":"Initial Estimate of Camera - Odometry Transform"},{"location":"2018/2018%20Apr/#3d-point-triangulation","text":"to find feature correspondences and Ceres to optimize the image reprojection error (bundle adjustment) across all frames.","title":"3D Point Triangulation"},{"location":"2018/2018%20Apr/#finding-local-inter-camera-feature-point-correspondences","text":"Keep local frame history. The image pair is rectified on a common image plane which corresponds to the average rotation between the first camera\u2019s pose and the second camera\u2019s pose","title":"Finding Local Inter-Camera Feature Point Correspondences"},{"location":"2018/2018%20Apr/#loop-closures","text":"Most of the loops are last frame in one monocular VO segment for a particular camera, and the first frame in the next monocular VO segment for the same camera.","title":"Loop Closures"},{"location":"2018/2018%20Apr/#full-bundle-adjustment","text":"Optimizes all intrinsics, extrinsics, odometry poses, and 3D scene points.","title":"Full Bundle Adjustment"},{"location":"2018/2018%20Apr/#msckf-20","text":"","title":"MSCKF 2.0"},{"location":"2018/2018%20Apr/#msckf-brief-introduction","text":"The effects of features for the linearization are avoided by multiply left nullspace of \\(H_f\\) . The feature in all the images are used for calculating the final position of the feature. The feature point is not in the state vector.","title":"MSCKF Brief Introduction"},{"location":"2018/2018%20Apr/#msckf-20-msckf-ekf-slam-comparison","text":"It shows that MSCKF and EKF-SLAM has inconsistency. Global parameterization of the rotation \\(^I_GR \\approx \\hat{^I_GR}(\\mathbf{I}_3 - \\lfloor\\delta^G\\tilde{\\theta}\\rfloor_\\times)\\) , which removes the orientation error from the observable matrix. The authors also use first estimate Jacobian for not letting errors increase the rank of the observable matrix (a novel closed-form expression for the IMU error-state transition matrix and fixed linearization states). Compare to the original MSCKF, it also includes extrinsic parameters. The Monte-Carlo simulations part can be adapted for other tests. Having a linearized system model with appropriate observability properties is more important than using re-linearization to better approximate the nonlinear measurement models. The uncertainty of yaw and position are underestimated by MSCKF and FLS.","title":"MSCKF 2.0, MSCKF, EKF-SLAM Comparison"},{"location":"2018/2018%20Apr/#probabilistic-surfel-fusion-for-dense-lidar-mapping","text":"","title":"Probabilistic Surfel Fusion for Dense LiDAR Mapping"},{"location":"2018/2018%20Apr/#main-idea","text":"Dual surfel maps are used, ellipsoid surfel map (ESM) is used to do localization, while disk surfel map (DSM) is used to fuse the map. Uncertainty model is used to represent the surfel. Surfel uncertainty and fusion are the keys of the paper. For ESM, the paper doesn't discuss a lot. For DSM, the data association is done by first octree-based nearest neighbor search algorithm, then thresholds-based method to pick the correspondences. The position and normal of the surfel are updated by by Bayesian filtering, including the mean and covariance. Unstable surfels which are not observed for a certain period of time is removed when revisited. The tests are performed under synthetic and real environments. It shows that the method have less noise in the fused map.","title":"Main idea"},{"location":"2018/2018%20Apr/#continuous-3d-scan-matching-with-a-spinning-2d-laser","text":"","title":"Continuous 3D Scan-Matching with a Spinning 2D Laser"},{"location":"2018/2018%20Apr/#main-idea_1","text":"This paper uses 3D spinning lidar sensor on a skid-steer loader vehicle to produce quality map of outdoor scenes and estimate the vehicle trajectory. The shape parameters are from mean and covariance and eigenvalues. Matching is based on ICP with first compute correspondences (centroid, smallest eigenvector and largest eigenvector), then estimate the trajectories with three constraints: match constraints, smoothness constraints and initial constraints. The industrial environment test is compared to 2D laser SLAM system, while off-road environment test is compared to closed-loop results (appearance-based methods). Box plots are used to visualize the translational and rotational errors.","title":"Main idea"},{"location":"2018/2018%20Apr/#generalized-icp","text":"This can be thought of as \u2018plane-to-plane\u2019. Outperform both standard ICP and point-to-plane and to be more robust to incorrect correspondences. More expressive probabilistic models. Addition of outlier terms, measurement noise, and other probabilistic techniques to increase robustness. G-ICP aims to take into account structure.","title":"Generalized-ICP"},{"location":"2018/2018%20Apr/#icp","text":"Compute correspondences between the two scans. Compute a transformation which minimizes distance between corresponding points.","title":"ICP"},{"location":"2018/2018%20Apr/#point-to-plane","text":"ICP improves performance by taking advantage of surface normal information","title":"Point-to-plane"},{"location":"2018/2018%20Apr/#generalized-icp_1","text":"Standard Euclidean distance to find correspondences. Kd-trees in the look up of closest points. Outliers are removed. Probabilistic model models the covariance matrices associated with the measured points. Transform a to b. \\[ d_i^{(\\mathbf{T})} \\thicksim \\mathcal{N}(\\hat{b_i}-(\\mathbf{T^*})\\hat{a_i}, C^B_i+(\\mathbf{T^*})C^A_i(\\mathbf{T^*})^T) \\\\ = \\mathcal{N}(0, C^B_i+(\\mathbf{T^*})C^A_i(\\mathbf{T^*})^T)) \\\\ \\mathbf{T} = \\text{argmax}_{\\mathbf{T}} \\prod_i p(d_i^{(\\mathbf{T})}) = \\text{argmax}_{\\mathbf{T}} \\sum_i \\log(p(d_i^{(\\mathbf{T})})) \\\\ \\mathbf{T} = \\text{argmin}_{\\mathbf{T}} \\sum_i ({d_i^{(\\mathbf{T})}}^T(C^B_i+(\\mathbf{T})C^A_i(\\mathbf{T})^T))^{-1}d_i^{(\\mathbf{T})}), \\text{(from multi-gaussian)} \\] Standard ICP can be seen as \\(C^A_i = 0, C^B_i = I\\) , while point-to-plane as \\(C^A_i = 0, C^B_i = P_i^{-1}\\) , where \\(P_i = A(A^T A)^{-1}A^T\\) and \\(A = v_i\\) is the direction of the normal. Since \\(P_i\\) is non-invertible, we need to approximate it with an invertible \\(Q_i\\) . Locally planar assumption. Plane-to-plane: Increase the symmetry of the model, constraint along its surface normal, but not exact correspondence. High covariance along its local plane, and very low covariance in the surface normal direction. \\[ \\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\\\ C_i^A = R_{\\nu_i}\\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}R_{\\nu_i}^T \\\\ C_i^B = R_{\\mu_i}\\begin{bmatrix} \\epsilon & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}R_{\\mu_i}^T \\] The incorrect correspondences form very weak and uninformative constraints. PCA (20 closest points) is used to recover surface normals from point clouds. The rotation matrices in the above equations are replaced by eigen decomposition \\(\\hat{\\Sigma} = UDU^T\\) , where \\(D\\) is replaced with \\(\\text{diag}(\\epsilon,1,1)\\) . Standard ICP was used in the pairwise matching to generate the ground truth. The testing were extracted with much higher spacing. The proposed method is less sensitive to \\(d_{max}\\) and give equal consideration to both scans (removed local minima in point-tp-plane). But how the normal in source scans to be extracted will still be a problem.","title":"Generalized-ICP"},{"location":"2018/2018%20Apr/#visualinertial-navigation-mapping-and-localization-a-scalable-real-time-causal-approach","text":"Explains why extrinsic parameters are observable. It is related to identifiability given the measurements.","title":"Visual\u2013inertial navigation, mapping and localization: A scalable real-time causal approach"},{"location":"2018/2018%20Apr/#motion-model","text":"Under constant dynamic model, or trivial dynamics (constant, slowly varying, or random walk).","title":"Motion model"},{"location":"2018/2018%20Apr/#observability","text":"The author proves that under that motion model the model (4)\u2013(5) and \\(T_{bi}\\) , \\(R_{bi}\\) , \\(\\gamma\\) (gravity vector) added to the state with constant dynamics, is locally observable, so long as motion is sufficiently exciting and the global reference frame is fixed (equivalently the initial conditions ( \\(R(0)\\) , \\(T(0)\\) )).","title":"Observability"},{"location":"2018/2018%20Aug/","text":"2018 August Reading Reports An Efficient Solution to the Five-Point Relative Pose Problem The proposed five-point algorithm is as a hypothesis generator within a random sample consensus scheme (RANSAC). The paper itself introduces a method to efficiently calculate five-point algorithm, which requires 5 corresponding points. The five-point algorithm uses \\({q'}^TEq = 0\\) and the constraints of \\(E\\) , $$ \\det(E) = 0 $$ and $$ EE^TE - \\frac{1}{2}\\text{trace}(EE^T)E = 0 $$ to calculate the essential matrix \\(E\\) . Then SVD is used to obtain the \\(R\\) and \\(t\\) with the cheirality constraint imposed. LeGO-LOAM Point cloud segmentation is performed to discard points that may represent unreliable features after ground separation (not use unstable features). Two-step L-M optimization is used, which reduces the computation time. Planar features extracted from the ground are used to obtain \\([t_z; roll; pitch]\\) during the first step. In the second step, the rest of the transformation \\([t_x; t_y; yaw]\\) is obtained by matching edge features extracted from the segmented point cloud. The ability to perform loop closures is integrated to correct motion estimation drift. Ground points or the segmented points are used for feature extraction and odometry. Direct Visual SLAM using Sparse Depth for Camera-LiDAR System A sliding window-based tracking method. Strict pose marginalization for accurate pose-graph SLAM. Depth-integrated frame matching for large-scale mapping. Tracking-based visual odometry (VO) module, loop-closure module and a pose-graph optimizer. Frame management uses the ratio of projectable points and certain period of time. Then marginalization is applied for old keyframes. Point selection is implemented by uniformly sampling from lidar data from 2\u00b0 by 2\u00b0 spatial bin. In tracking, the brightness in grayscale are defined as \\[ r(\\mathbf{p}_m) = \\mathcal{I}_n(\\pi(\\mathbf{T}^n_m \\mathbf{p}_m))-\\mathcal{I}_m(\\pi(\\mathbf{p}_m)) \\\\ E_{track} = \\sum_{\\mathbf{p} \\in \\Omega}\\omega(r(\\mathbf{p}))r(\\mathbf{p})^2 \\] \\(\\omega(\\cdot)\\) is a weight function based on the t-distribution. And the coarse-to-fine scheme is used. The selected points are projected to all keyframes with covisibility. In window based optimization, the error is computed among all keyframes as \\[ E_{win} = \\sum_{\\mathcal{F}_i \\in \\mathbf{W}_{\\mathcal{F}}}\\sum_{\\mathbf{p}_k\\in \\mathbf{P}_I}\\sum_{\\mathbf{p}_k \\in \\Omega} \\omega(r(\\mathbf{p}_k))r(\\mathbf{p}_k)^2 \\] ORB features and descriptor is used to do loop closure detection. Dynamic Covariance Scaling (DCS) is applied to cope with unreliable constraints. The lidar points are sampled into 16 and 8 to simulate the sparse data. ArUCo markers and VRS-GPS (at fixed state) are used for evaluation. Future work includes illumination changes and moving objects. DEMO A visual odometry framework that can utilize sparse depth information from various sources to assist motion estimation. The proposed method is able to maximally utilize image information with and without depth coverage in motion estimation. Combining both features with and without depth in solving for motion. Frame to frame motion estimation For the feature points with unknown depth, \\(z^k_i\\) and/or \\(z^{k-1}_i\\) are eliminated. The \\(m\\) and \\(n\\) features with known and unknown depth forms a objective function w.r.t. \\(\\theta\\) and \\(\\mathbf{T}\\) with \\(2m+n\\) rows. Current \\(\\bar{\\mathbf{X}}^{k}_i\\) and the last \\(\\mathbf{X}^{k-1}_i\\) and \\(\\bar{\\mathbf{X}}^{k-1}_i\\) . New points in front of the camera from depth sensors are added to the depth map . It is downsized according to two angular coordinates to maintain a constant point density. The depth is found by projecting onto the planar patch , which is formed by three points in the angular coordinates (2D KD-tree). For the points unavailable from the depth map but tracked for longer than a certain distance in the Euclidean space. The features are triangulated based on a Bayesian probabilistic model. Bundle adjustment It takes a sequence of images and performs a batch optimization to refine the frame to frame motion estimation. One image out of every five images is the bundle adjustment input. The image sequence contains a number of eight images. It uses iSAM open source library to all selected inlier features. The information matrix contains constant first two elements and the third one is related to the depth sensor, inversely proportional to the square of the depth, or as zero if depth is known. Two outputs (frame-to-frame and bundle adjustment) are integrated together. In the implementation, Harris corners are tracked by the Kanade Lucas Tomasi (KLT) method. Gap between the two ends of a trajectory compared to the length of the trajectory. DEMO shares the same frame to frame motion estimation with V-LOAM. V-LOAM replaces the bundle adjustment with the scan matching. Scale Drift-Aware Large Scale Monocular SLAM The most interesting part is the backend optimization. It utilizes the 7 DoF similarity transform optimization. Source Tight Coupling of Laser Scanner and Inertial Measurements for a Fully Autonomous Relative Navigation Solution 2D lidar is used in this work with INS. Line extracting and matching are applied to estimate the 2D pose. Tilted lidar measurements are corrected by INS. Then the lidar range values are used as measurements for the Kalman filter involving INS measurements and the states (for correct the INS error). Navigation computation includes line matching and updates. Collar Line Segments for Fast Odometry Estimation from Velodyne Point Clouds The Collar Line Segments are designed for overcoming the sparsity and ring structure of Velodyne data. The Velodyne point cloud is transformed into a line cloud by random generation of Collar Line Segments (CLS). It is randomly generated between lines within the bins and the shortest ones are selected. Registration of the line cloud representation (iteratively). The middle points of the target lines are extracted and matched with the closest ones in the source line cloud. Corresponding points (closet points) between two lines are found to operate ICP. Linear prediction is used for the transformation. Mean values of the transformations among multiple scans are used for the final results. Source An Explicit Loop Closing Technique for 6D SLAM An explicit loop closing heuristic (ELCH) is discussed. It dissociates the last scan of a sequence of acquired scans, reassociates it to the map, built so far by scan registration, and distributes the difference in the pose error over the SLAM graph. Euclidean distance between the current and all previous poses and the intermediate scans are more than a threshold. The transformation is distributes according to the ratio of covariance sum. Then iterative method LUM can be implemented. Real-Time Loop Closure in 2D LIDAR SLAM A branch-and-bound approach and several precomputed grids per finished submap. The submap is based on 2D occupancy map (binary Bayes Filters, p95 PR). IMU with the angular velocities can help to estimate the rotational component of the pose. If finished submaps and the correct scan are close enough based on current pose estimates, a scan matcher tries to find the scan in the submap. Integral numbers of steps are adopted for loop scan matching. To solve this problem a branch and bound scan matching is introduced. It contains node selection, branching rule and computing upper bounds. Long-range GPS-denied Aerial Inertial Navigation with LIDAR Localization It uses a method with tightly coupled IMU with semi-intermittent global corrections using LIDAR measurements matched against an a priori Digital Elevation Model (DEM). It is in the category of Terrain Referenced Navigation (TRN). A region of sufficient number of line scans are accumulated. A smooth surface-like heightmap \\(\\mathbf{I}_L\\) is generated, which is compared with the DEM image \\(\\mathbf{I}_D\\) by normalized cross-correlation value. First estimate the \\(x\\) and \\(y\\) axes, then a vertical correction ( \\(z\\) ) from the elevation images (mean of the element-wise subtraction). The offset represents the as the state estimation error as measured by the lidar against the ground truth DEM. Runge-Kutta integration method RK4 is used in the IMU propagation. Error state Kalman filter is used. The filter is rolled back to the state at which the correction occurred and recompute the propagation. Joseph form of the Kalman filter measurement is used, and the covariance matrix maintains symmetry by regularly re-symmetrizing. 1-Point-RANSAC Structure from Motion for Vehicle-Mounted Cameras by Exploiting Non-holonomic Constraints A restrictive motion model is introduced, allowing to parameterize the motion with only 1 point correspondence. Only one feature correspondence for computing the epipolar geometry. Once the outliers are removed, the motion can be refined using all the inliers. It is theoretically valid under the assumption that the camera is positioned above the rear wheel axis of the vehicle. It assume non-holonomic constraints, which result in $$ \\sin \\left(\\frac{\\theta}{2} \\right) \\cdot (x'z+z'x) + \\cos \\left(\\frac{\\theta}{2} \\right) \\cdot (y'z-z'y) = 0 $$ Once the outliers are identified, the unconstrained motion estimate (6DoF) can be computed from all the remaining inliers using standard methods. The outlier removing approaches are based on RANSAC (reprojection error) and histogram voting (for max or median \\(\\theta^*\\) , within a given distance \\(t\\) or using the reprojection error). Degraded for non-perfectly planar motion. A hyperbolic mirror (KAIDAN 360 One VR) for real data. Harris detector is used. The speed of the car from CAN-bus is read to measure the absolute scale. Switch between 1-point and 5-point. When the local circular planar motion is well verified, this reflects in a narrow histogram with a very distinguishable peak. Visual\u2013Inertial Combined Odometry System for Aerial Vehicles A method that improves estimation of translation for a high-accuracy INS. A feature reparametrization technique to partially remove odometry drift (equivalent to rotating the camera to a virtual point perpendicular to the ground). Project to a virtual horizontal plane and form a function of 5 unknown parameters. Solve the equation with rotation \\(R^V_P\\) as zero, then solve non-linear function iteratively. It is adapted to a robust fitting algorithm (x, y translations). For the push-broom laser, the plane is fitted into a plane, which gives the inclination of the ground. Analysis of error propagation is provided. The upper bound of the position drift introduced by the yaw angle noise form the INS and altimeter/push-broom laser noise is proportional to the flying distance. Scan Segments Matching for Pairwise 3D Alignment Object level matching based on segmentation. Coarse alignment by matching segments across scans. Matched segments are then used to constrain point-to-point associations in regular ICP (nearest neighbor search is performed in matched segments only). A pipeline combining 3D object segmentation, segment matching across scans, and alignment. The scan segmentation separate the ground and objects in voxel grid . Center of mass is used for the segments. The procedure of Segment Associations is first bipartite assignment by Hungarian algorithm (Symmetric Shape Distance). Then geometric consistency is used to check the associations and it will filter the wrong assignments. 6 DoF point-to-point ICP is applied to the associated segments, followed by point-to-plane full-scan match. Evaluation Metric The evaluation metric simply consists of voxelising the point cloud and returning the number of occupied voxels. The lower this number, the more crisp the point cloud and in turn the more accurate the alignment. But the results are not as good as point-to-plane ICP when the displacement is small. Self-Calibration and Visual SLAM with a Multi-Camera System on a Micro Aerial Vehicle For multiple calibrated stereo cameras and IMU. A 3-point minimal and linear solution for motion estimation. Build a graph of keyframes and constraints and use the double-window optimization method. Self-Calibration For the self-calibration, first it obtains a globally consistent map for each stereo camera via stereo vSLAM. Bundle adjustment based on dot product. Then hand-eye calibration gives an initial estimation of the inter-stereo-camera transforms. Map is merged form all stereo cameras, followed by globally consistent pose estimates. Full bundle adjustment (reprojection error of 3D scene points and chessboard corner poimts) allows to recover an accurate estimation. Another hand-eye for the final IMU-involved calibration (only rotation + hand-measurement of translation). Visual SLAM The motion estimation is based on the rotation from IMU, and form generalized epipolar constraint by substituting \\(R\\) (resulting in linear form), where only minimal 3 points are needed. After RANSAC, non-linear refinement is applied to the inlier set of correspondences. When no new keyframe for some time, pose estimation instead of motion estimation is used. The optimization includes the additional residuals corresponding to the error between the vertical direction associated with the MAV's estimated pose and the vertical direction measurement from the IMU. Implementation CenSurE feature detector is used. ORB feature descriptor is extracted. \\(r_2Er_1\\) is less than a threshold, the feature match is consider as valid. Vicon is used to calculate the ground truth of the extrinsic parameters with a marked chessboard. Vicon motion capture system and loop closure error metric is used. Using Many Cameras as One The generalized camera model is simplified as that it only includes the definition of the ray that the pixel samples. Plucker Vectors are used to parameterize arbitrary lines in space. \\(q\\) and \\(q'\\) named the direction vector and moment vector. Fisher information matrix is used to give the optimal design of panoramic imaging systems constructed from multiple cameras. Plucker Vectors \\(q\\) is a vector of any length in the direction of the line. \\(q' = q\\times P\\) , for any point \\(P\\) on the line. For a normalized point in the origin camera, the Plucker vector can be written as \\(\\langle \\tilde{x}^T, 0 \\rangle\\) , for other cameras, the direction vector \\(q = R \\tilde{x}\\) the moment vector \\(q \\times T\\) . Motion Estimation The Generalized Epi-polar Constraint (GEC) can be used to estimate the motion. The pair of lines with Pluck vectors intersect iff \\(q_b\\cdot q'_a + q'_b\\cdot q_a = 0\\) . To be coincident with Heng et al.'s work, I think the equation should be $$ q_2 TRq'_1+q_2 T \\lfloor T \\rfloor_{\\times} q_1 + {q'}_2^TRq_1=0 $$ A camera system with cameras facing in opposite directions but aligned along their optic axis is perhaps the best design for two standard cameras. The \u201cline\u201d ambiguity is still present. Six cameras placed to view in opposite directions along coordinate axes show no ambiguities. Double Window Optimisation for Constant Time Visual SLAM It takes a two-level approach that combines accurate pose-point constraints in the primary region of interest with a stabilising periphery of pose-pose soft constraints. It is related to relative bundle adjustment (RBA), but RBA does not enforce metric consistency within its optimisation window. The proposed method minimise the error in both windows simultaneously . An efficient approximation of covariance is defined by constant for rotation and the parallax for translation. But is not clear for the reprojection error. The method does not have a fixed keyframe, but let the damping factor of LM takes care of the gauge freedom. Metric and appearance-based loop closure are introduced.","title":"2018 August Reading Reports"},{"location":"2018/2018%20Aug/#2018-august-reading-reports","text":"","title":"2018 August Reading Reports"},{"location":"2018/2018%20Aug/#an-efficient-solution-to-the-five-point-relative-pose-problem","text":"The proposed five-point algorithm is as a hypothesis generator within a random sample consensus scheme (RANSAC). The paper itself introduces a method to efficiently calculate five-point algorithm, which requires 5 corresponding points. The five-point algorithm uses \\({q'}^TEq = 0\\) and the constraints of \\(E\\) , $$ \\det(E) = 0 $$ and $$ EE^TE - \\frac{1}{2}\\text{trace}(EE^T)E = 0 $$ to calculate the essential matrix \\(E\\) . Then SVD is used to obtain the \\(R\\) and \\(t\\) with the cheirality constraint imposed.","title":"An Efficient Solution to the Five-Point Relative Pose Problem"},{"location":"2018/2018%20Aug/#lego-loam","text":"Point cloud segmentation is performed to discard points that may represent unreliable features after ground separation (not use unstable features). Two-step L-M optimization is used, which reduces the computation time. Planar features extracted from the ground are used to obtain \\([t_z; roll; pitch]\\) during the first step. In the second step, the rest of the transformation \\([t_x; t_y; yaw]\\) is obtained by matching edge features extracted from the segmented point cloud. The ability to perform loop closures is integrated to correct motion estimation drift. Ground points or the segmented points are used for feature extraction and odometry.","title":"LeGO-LOAM"},{"location":"2018/2018%20Aug/#direct-visual-slam-using-sparse-depth-for-camera-lidar-system","text":"A sliding window-based tracking method. Strict pose marginalization for accurate pose-graph SLAM. Depth-integrated frame matching for large-scale mapping. Tracking-based visual odometry (VO) module, loop-closure module and a pose-graph optimizer. Frame management uses the ratio of projectable points and certain period of time. Then marginalization is applied for old keyframes. Point selection is implemented by uniformly sampling from lidar data from 2\u00b0 by 2\u00b0 spatial bin. In tracking, the brightness in grayscale are defined as \\[ r(\\mathbf{p}_m) = \\mathcal{I}_n(\\pi(\\mathbf{T}^n_m \\mathbf{p}_m))-\\mathcal{I}_m(\\pi(\\mathbf{p}_m)) \\\\ E_{track} = \\sum_{\\mathbf{p} \\in \\Omega}\\omega(r(\\mathbf{p}))r(\\mathbf{p})^2 \\] \\(\\omega(\\cdot)\\) is a weight function based on the t-distribution. And the coarse-to-fine scheme is used. The selected points are projected to all keyframes with covisibility. In window based optimization, the error is computed among all keyframes as \\[ E_{win} = \\sum_{\\mathcal{F}_i \\in \\mathbf{W}_{\\mathcal{F}}}\\sum_{\\mathbf{p}_k\\in \\mathbf{P}_I}\\sum_{\\mathbf{p}_k \\in \\Omega} \\omega(r(\\mathbf{p}_k))r(\\mathbf{p}_k)^2 \\] ORB features and descriptor is used to do loop closure detection. Dynamic Covariance Scaling (DCS) is applied to cope with unreliable constraints. The lidar points are sampled into 16 and 8 to simulate the sparse data. ArUCo markers and VRS-GPS (at fixed state) are used for evaluation. Future work includes illumination changes and moving objects.","title":"Direct Visual SLAM using Sparse Depth for Camera-LiDAR System"},{"location":"2018/2018%20Aug/#demo","text":"A visual odometry framework that can utilize sparse depth information from various sources to assist motion estimation. The proposed method is able to maximally utilize image information with and without depth coverage in motion estimation. Combining both features with and without depth in solving for motion.","title":"DEMO"},{"location":"2018/2018%20Aug/#frame-to-frame-motion-estimation","text":"For the feature points with unknown depth, \\(z^k_i\\) and/or \\(z^{k-1}_i\\) are eliminated. The \\(m\\) and \\(n\\) features with known and unknown depth forms a objective function w.r.t. \\(\\theta\\) and \\(\\mathbf{T}\\) with \\(2m+n\\) rows. Current \\(\\bar{\\mathbf{X}}^{k}_i\\) and the last \\(\\mathbf{X}^{k-1}_i\\) and \\(\\bar{\\mathbf{X}}^{k-1}_i\\) . New points in front of the camera from depth sensors are added to the depth map . It is downsized according to two angular coordinates to maintain a constant point density. The depth is found by projecting onto the planar patch , which is formed by three points in the angular coordinates (2D KD-tree). For the points unavailable from the depth map but tracked for longer than a certain distance in the Euclidean space. The features are triangulated based on a Bayesian probabilistic model.","title":"Frame to frame motion estimation"},{"location":"2018/2018%20Aug/#bundle-adjustment","text":"It takes a sequence of images and performs a batch optimization to refine the frame to frame motion estimation. One image out of every five images is the bundle adjustment input. The image sequence contains a number of eight images. It uses iSAM open source library to all selected inlier features. The information matrix contains constant first two elements and the third one is related to the depth sensor, inversely proportional to the square of the depth, or as zero if depth is known. Two outputs (frame-to-frame and bundle adjustment) are integrated together. In the implementation, Harris corners are tracked by the Kanade Lucas Tomasi (KLT) method. Gap between the two ends of a trajectory compared to the length of the trajectory. DEMO shares the same frame to frame motion estimation with V-LOAM. V-LOAM replaces the bundle adjustment with the scan matching.","title":"Bundle adjustment"},{"location":"2018/2018%20Aug/#scale-drift-aware-large-scale-monocular-slam","text":"The most interesting part is the backend optimization. It utilizes the 7 DoF similarity transform optimization. Source","title":"Scale Drift-Aware Large Scale Monocular SLAM"},{"location":"2018/2018%20Aug/#tight-coupling-of-laser-scanner-and-inertial-measurements-for-a-fully-autonomous-relative-navigation-solution","text":"2D lidar is used in this work with INS. Line extracting and matching are applied to estimate the 2D pose. Tilted lidar measurements are corrected by INS. Then the lidar range values are used as measurements for the Kalman filter involving INS measurements and the states (for correct the INS error). Navigation computation includes line matching and updates.","title":"Tight Coupling of Laser Scanner and Inertial Measurements for a Fully Autonomous Relative Navigation Solution"},{"location":"2018/2018%20Aug/#collar-line-segments-for-fast-odometry-estimation-from-velodyne-point-clouds","text":"The Collar Line Segments are designed for overcoming the sparsity and ring structure of Velodyne data. The Velodyne point cloud is transformed into a line cloud by random generation of Collar Line Segments (CLS). It is randomly generated between lines within the bins and the shortest ones are selected. Registration of the line cloud representation (iteratively). The middle points of the target lines are extracted and matched with the closest ones in the source line cloud. Corresponding points (closet points) between two lines are found to operate ICP. Linear prediction is used for the transformation. Mean values of the transformations among multiple scans are used for the final results. Source","title":"Collar Line Segments for Fast Odometry Estimation from Velodyne Point Clouds"},{"location":"2018/2018%20Aug/#an-explicit-loop-closing-technique-for-6d-slam","text":"An explicit loop closing heuristic (ELCH) is discussed. It dissociates the last scan of a sequence of acquired scans, reassociates it to the map, built so far by scan registration, and distributes the difference in the pose error over the SLAM graph. Euclidean distance between the current and all previous poses and the intermediate scans are more than a threshold. The transformation is distributes according to the ratio of covariance sum. Then iterative method LUM can be implemented.","title":"An Explicit Loop Closing Technique for 6D SLAM"},{"location":"2018/2018%20Aug/#real-time-loop-closure-in-2d-lidar-slam","text":"A branch-and-bound approach and several precomputed grids per finished submap. The submap is based on 2D occupancy map (binary Bayes Filters, p95 PR). IMU with the angular velocities can help to estimate the rotational component of the pose. If finished submaps and the correct scan are close enough based on current pose estimates, a scan matcher tries to find the scan in the submap. Integral numbers of steps are adopted for loop scan matching. To solve this problem a branch and bound scan matching is introduced. It contains node selection, branching rule and computing upper bounds.","title":"Real-Time Loop Closure in 2D LIDAR SLAM"},{"location":"2018/2018%20Aug/#long-range-gps-denied-aerial-inertial-navigation-with-lidar-localization","text":"It uses a method with tightly coupled IMU with semi-intermittent global corrections using LIDAR measurements matched against an a priori Digital Elevation Model (DEM). It is in the category of Terrain Referenced Navigation (TRN). A region of sufficient number of line scans are accumulated. A smooth surface-like heightmap \\(\\mathbf{I}_L\\) is generated, which is compared with the DEM image \\(\\mathbf{I}_D\\) by normalized cross-correlation value. First estimate the \\(x\\) and \\(y\\) axes, then a vertical correction ( \\(z\\) ) from the elevation images (mean of the element-wise subtraction). The offset represents the as the state estimation error as measured by the lidar against the ground truth DEM. Runge-Kutta integration method RK4 is used in the IMU propagation. Error state Kalman filter is used. The filter is rolled back to the state at which the correction occurred and recompute the propagation. Joseph form of the Kalman filter measurement is used, and the covariance matrix maintains symmetry by regularly re-symmetrizing.","title":"Long-range GPS-denied Aerial Inertial Navigation with LIDAR Localization"},{"location":"2018/2018%20Aug/#1-point-ransac-structure-from-motion-for-vehicle-mounted-cameras-by-exploiting-non-holonomic-constraints","text":"A restrictive motion model is introduced, allowing to parameterize the motion with only 1 point correspondence. Only one feature correspondence for computing the epipolar geometry. Once the outliers are removed, the motion can be refined using all the inliers. It is theoretically valid under the assumption that the camera is positioned above the rear wheel axis of the vehicle. It assume non-holonomic constraints, which result in $$ \\sin \\left(\\frac{\\theta}{2} \\right) \\cdot (x'z+z'x) + \\cos \\left(\\frac{\\theta}{2} \\right) \\cdot (y'z-z'y) = 0 $$ Once the outliers are identified, the unconstrained motion estimate (6DoF) can be computed from all the remaining inliers using standard methods. The outlier removing approaches are based on RANSAC (reprojection error) and histogram voting (for max or median \\(\\theta^*\\) , within a given distance \\(t\\) or using the reprojection error). Degraded for non-perfectly planar motion. A hyperbolic mirror (KAIDAN 360 One VR) for real data. Harris detector is used. The speed of the car from CAN-bus is read to measure the absolute scale. Switch between 1-point and 5-point. When the local circular planar motion is well verified, this reflects in a narrow histogram with a very distinguishable peak.","title":"1-Point-RANSAC Structure from Motion for Vehicle-Mounted Cameras by Exploiting Non-holonomic Constraints"},{"location":"2018/2018%20Aug/#visualinertial-combined-odometry-system-for-aerial-vehicles","text":"A method that improves estimation of translation for a high-accuracy INS. A feature reparametrization technique to partially remove odometry drift (equivalent to rotating the camera to a virtual point perpendicular to the ground). Project to a virtual horizontal plane and form a function of 5 unknown parameters. Solve the equation with rotation \\(R^V_P\\) as zero, then solve non-linear function iteratively. It is adapted to a robust fitting algorithm (x, y translations). For the push-broom laser, the plane is fitted into a plane, which gives the inclination of the ground. Analysis of error propagation is provided. The upper bound of the position drift introduced by the yaw angle noise form the INS and altimeter/push-broom laser noise is proportional to the flying distance.","title":"Visual\u2013Inertial Combined Odometry System for Aerial Vehicles"},{"location":"2018/2018%20Aug/#scan-segments-matching-for-pairwise-3d-alignment","text":"Object level matching based on segmentation. Coarse alignment by matching segments across scans. Matched segments are then used to constrain point-to-point associations in regular ICP (nearest neighbor search is performed in matched segments only). A pipeline combining 3D object segmentation, segment matching across scans, and alignment. The scan segmentation separate the ground and objects in voxel grid . Center of mass is used for the segments. The procedure of Segment Associations is first bipartite assignment by Hungarian algorithm (Symmetric Shape Distance). Then geometric consistency is used to check the associations and it will filter the wrong assignments. 6 DoF point-to-point ICP is applied to the associated segments, followed by point-to-plane full-scan match.","title":"Scan Segments Matching for Pairwise 3D Alignment"},{"location":"2018/2018%20Aug/#evaluation-metric","text":"The evaluation metric simply consists of voxelising the point cloud and returning the number of occupied voxels. The lower this number, the more crisp the point cloud and in turn the more accurate the alignment. But the results are not as good as point-to-plane ICP when the displacement is small.","title":"Evaluation Metric"},{"location":"2018/2018%20Aug/#self-calibration-and-visual-slam-with-a-multi-camera-system-on-a-micro-aerial-vehicle","text":"For multiple calibrated stereo cameras and IMU. A 3-point minimal and linear solution for motion estimation. Build a graph of keyframes and constraints and use the double-window optimization method.","title":"Self-Calibration and Visual SLAM with a Multi-Camera System on a Micro Aerial Vehicle"},{"location":"2018/2018%20Aug/#self-calibration","text":"For the self-calibration, first it obtains a globally consistent map for each stereo camera via stereo vSLAM. Bundle adjustment based on dot product. Then hand-eye calibration gives an initial estimation of the inter-stereo-camera transforms. Map is merged form all stereo cameras, followed by globally consistent pose estimates. Full bundle adjustment (reprojection error of 3D scene points and chessboard corner poimts) allows to recover an accurate estimation. Another hand-eye for the final IMU-involved calibration (only rotation + hand-measurement of translation).","title":"Self-Calibration"},{"location":"2018/2018%20Aug/#visual-slam","text":"The motion estimation is based on the rotation from IMU, and form generalized epipolar constraint by substituting \\(R\\) (resulting in linear form), where only minimal 3 points are needed. After RANSAC, non-linear refinement is applied to the inlier set of correspondences. When no new keyframe for some time, pose estimation instead of motion estimation is used. The optimization includes the additional residuals corresponding to the error between the vertical direction associated with the MAV's estimated pose and the vertical direction measurement from the IMU.","title":"Visual SLAM"},{"location":"2018/2018%20Aug/#implementation","text":"CenSurE feature detector is used. ORB feature descriptor is extracted. \\(r_2Er_1\\) is less than a threshold, the feature match is consider as valid. Vicon is used to calculate the ground truth of the extrinsic parameters with a marked chessboard. Vicon motion capture system and loop closure error metric is used.","title":"Implementation"},{"location":"2018/2018%20Aug/#using-many-cameras-as-one","text":"The generalized camera model is simplified as that it only includes the definition of the ray that the pixel samples. Plucker Vectors are used to parameterize arbitrary lines in space. \\(q\\) and \\(q'\\) named the direction vector and moment vector. Fisher information matrix is used to give the optimal design of panoramic imaging systems constructed from multiple cameras.","title":"Using Many Cameras as One"},{"location":"2018/2018%20Aug/#plucker-vectors","text":"\\(q\\) is a vector of any length in the direction of the line. \\(q' = q\\times P\\) , for any point \\(P\\) on the line. For a normalized point in the origin camera, the Plucker vector can be written as \\(\\langle \\tilde{x}^T, 0 \\rangle\\) , for other cameras, the direction vector \\(q = R \\tilde{x}\\) the moment vector \\(q \\times T\\) .","title":"Plucker Vectors"},{"location":"2018/2018%20Aug/#motion-estimation","text":"The Generalized Epi-polar Constraint (GEC) can be used to estimate the motion. The pair of lines with Pluck vectors intersect iff \\(q_b\\cdot q'_a + q'_b\\cdot q_a = 0\\) . To be coincident with Heng et al.'s work, I think the equation should be $$ q_2 TRq'_1+q_2 T \\lfloor T \\rfloor_{\\times} q_1 + {q'}_2^TRq_1=0 $$ A camera system with cameras facing in opposite directions but aligned along their optic axis is perhaps the best design for two standard cameras. The \u201cline\u201d ambiguity is still present. Six cameras placed to view in opposite directions along coordinate axes show no ambiguities.","title":"Motion Estimation"},{"location":"2018/2018%20Aug/#double-window-optimisation-for-constant-time-visual-slam","text":"It takes a two-level approach that combines accurate pose-point constraints in the primary region of interest with a stabilising periphery of pose-pose soft constraints. It is related to relative bundle adjustment (RBA), but RBA does not enforce metric consistency within its optimisation window. The proposed method minimise the error in both windows simultaneously . An efficient approximation of covariance is defined by constant for rotation and the parallax for translation. But is not clear for the reprojection error. The method does not have a fixed keyframe, but let the damping factor of LM takes care of the gauge freedom. Metric and appearance-based loop closure are introduced.","title":"Double Window Optimisation for Constant Time Visual SLAM"},{"location":"2018/2018%20Dec/","text":"2018 December Reading Reports 3D Modeling on the Go: Interactive 3D Reconstruction of Large-Scale Scenes on Mobile Devices This paper presented and evaluated a system for large-scale 3D reconstruction of outdoor scenes that runs at interactive frame rates on modern mobile devices. It used monochrome \ufb01sheye images. The authors showed the importance of \ufb01ltering outliers in the depth maps and propose multiple \ufb01ltering steps. The trajectory is given and the depth estimation and uncertainty is calculated by a cost function (one image). Then the estimation is updated by the consecutive images, followed by several filters. The \ufb01ltered depth maps are integrated into a volume storing a TSDF using the voxel hashing . More details are shown in [1] (plane-sweeping stereo) and [2] (voxel hashing). [1] H\u00e4ne, C., Heng, L., Lee, G. H., Sizov, A., & Pollefeys, M. (2014, December). Real-time direct dense matching on fisheye images using plane-sweeping stereo. In 3D Vision (3DV), 2014 2nd International Conference on (Vol. 1, pp. 57-64). IEEE. [2] Klingensmith, M., Dryanovski, I., Srinivasa, S., & Xiao, J. (2015, July). Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields. In Robotics: science and systems (Vol. 4). Real-Time Direct Dense Matching on Fisheye Images Using Plane-Sweeping Stereo This paper introduced a method to use fisheye images for plane-sweeping stereo. It contains camera projection model (MEI or FOV), homography (with ZNCC matching scores), cost aggregation and depth extraction and sub-pixel interpolation. It is slightly slow but have larger coverage. No pretty sure about sub-pixel interpolation mentioned in the paper. Real-Time Plane-Sweeping Stereo with Multiple Sweeping Directions It presented a real-time multi-view stereo algorithm based on plane-sweeping which correctly handles slanted surfaces. Identifying sweeping directions (by uniform sampling 2D points), plane selection (range), incorporating plane priors (SfM), and selecting depths from multiple sweeps (containing the matching cost, surface normal smoothness, and depth smoothness, three-label graph cut ). ElasticFusion: Dense SLAM Without A Pose Graph (i) use photometric and geometric frame-to-model predictive tracking in a fused surfel-based dense map; (ii) perform dense model-to-model local surface loop closures with a non-rigid space deformation and (iii) utilise a predicted surface appearance-based place recognition method to resolve global surface loop closures and hence capture globally consistent dense surfel-based maps without a pose graph . This paper uses RGB-D camera to build consistent map. It contains four parts, fused predicted tracking (geometric pose estimate + (added, full color) photometric pose estimation), deformation graph as [1] (added a pin item, time related), local loop closure and global loop closure [2] (fern encoded frame). [1] R. W. Sumner, J. Schmid, and M. Pauly. Embedded deformation for shape manipulation. In Proceedings of SIGGRAPH, 2007. [2] RealTime RGB-D Camera Relocalization via Randomized Ferns for Keyframe Encoding. IEEE Transactions on Visualization and Computer Graphics, 21(5):571\u2013583, 2015.","title":"2018 December Reading Reports"},{"location":"2018/2018%20Dec/#2018-december-reading-reports","text":"","title":"2018 December Reading Reports"},{"location":"2018/2018%20Dec/#3d-modeling-on-the-go-interactive-3d-reconstruction-of-large-scale-scenes-on-mobile-devices","text":"This paper presented and evaluated a system for large-scale 3D reconstruction of outdoor scenes that runs at interactive frame rates on modern mobile devices. It used monochrome \ufb01sheye images. The authors showed the importance of \ufb01ltering outliers in the depth maps and propose multiple \ufb01ltering steps. The trajectory is given and the depth estimation and uncertainty is calculated by a cost function (one image). Then the estimation is updated by the consecutive images, followed by several filters. The \ufb01ltered depth maps are integrated into a volume storing a TSDF using the voxel hashing . More details are shown in [1] (plane-sweeping stereo) and [2] (voxel hashing). [1] H\u00e4ne, C., Heng, L., Lee, G. H., Sizov, A., & Pollefeys, M. (2014, December). Real-time direct dense matching on fisheye images using plane-sweeping stereo. In 3D Vision (3DV), 2014 2nd International Conference on (Vol. 1, pp. 57-64). IEEE. [2] Klingensmith, M., Dryanovski, I., Srinivasa, S., & Xiao, J. (2015, July). Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields. In Robotics: science and systems (Vol. 4).","title":"3D Modeling on the Go: Interactive 3D Reconstruction of Large-Scale Scenes on Mobile Devices"},{"location":"2018/2018%20Dec/#real-time-direct-dense-matching-on-fisheye-images-using-plane-sweeping-stereo","text":"This paper introduced a method to use fisheye images for plane-sweeping stereo. It contains camera projection model (MEI or FOV), homography (with ZNCC matching scores), cost aggregation and depth extraction and sub-pixel interpolation. It is slightly slow but have larger coverage. No pretty sure about sub-pixel interpolation mentioned in the paper.","title":"Real-Time Direct Dense Matching on Fisheye Images Using Plane-Sweeping Stereo"},{"location":"2018/2018%20Dec/#real-time-plane-sweeping-stereo-with-multiple-sweeping-directions","text":"It presented a real-time multi-view stereo algorithm based on plane-sweeping which correctly handles slanted surfaces. Identifying sweeping directions (by uniform sampling 2D points), plane selection (range), incorporating plane priors (SfM), and selecting depths from multiple sweeps (containing the matching cost, surface normal smoothness, and depth smoothness, three-label graph cut ).","title":"Real-Time Plane-Sweeping Stereo with Multiple Sweeping Directions"},{"location":"2018/2018%20Dec/#elasticfusion-dense-slam-without-a-pose-graph","text":"(i) use photometric and geometric frame-to-model predictive tracking in a fused surfel-based dense map; (ii) perform dense model-to-model local surface loop closures with a non-rigid space deformation and (iii) utilise a predicted surface appearance-based place recognition method to resolve global surface loop closures and hence capture globally consistent dense surfel-based maps without a pose graph . This paper uses RGB-D camera to build consistent map. It contains four parts, fused predicted tracking (geometric pose estimate + (added, full color) photometric pose estimation), deformation graph as [1] (added a pin item, time related), local loop closure and global loop closure [2] (fern encoded frame). [1] R. W. Sumner, J. Schmid, and M. Pauly. Embedded deformation for shape manipulation. In Proceedings of SIGGRAPH, 2007. [2] RealTime RGB-D Camera Relocalization via Randomized Ferns for Keyframe Encoding. IEEE Transactions on Visualization and Computer Graphics, 21(5):571\u2013583, 2015.","title":"ElasticFusion: Dense SLAM Without A Pose Graph"},{"location":"2018/2018%20Feb/","text":"2018 February Reading Reports FAB-MAP The mean idea of FAB-MAP is to use a probabilistic framework for appearance-based place recognition. The original paper used 128D SURF descriptors extracted from images. And then cluster them to 11k words as vocabulary. Then compute Chow Liu tree to approximate the probability of \\(p(Z_k|L_i, Z^{k-1})\\) instead of a Naive Bayesian assumption. Hidden events \\(e_i\\) are introduced to provide ability to incorporate multiple sensors and factor the distribution \\(p(Z|L_j)\\) into two parts. It deals with the case that a new place could appear, and the corresponding probability under such case. As mentioned by the authors, Monte Carlo approximation, or a sampling method, is applied to get a better performance. Location Prior should be given. Smoothing is applied for rejecting a single similar image pair. The assumptions in this methods are: Sets of observations are conditionally independent given position: \\(p(Z_k|L_i,Z_{k-1}) = p(Z_k|L_i)\\) . Detector behavior is independent of position: \\(p(z_j|e_j,L_i) = p(z_j|e_j)\\) . Location models are generated independently by the environment. Approximation of \\(p(e_j|Z_k) \\approx p(e_j|z_{j_k})\\) . Some comments: The method needs off-line training, which requires some conditions, e.g. no overlapping on the training data. The words are fixed in the origin work, which are obtained from descriptors in training data. Some hyper-parameters, like priors could be hard to determine. @article { cummins2008fab, title= { FAB-MAP: Probabilistic localization and mapping in the space of appearance } , author= { Cummins, Mark and Newman, Paul } , journal= { The International Journal of Robotics Research } , volume= { 27 } , number= { 6 } , pages= { 647--665 } , year= { 2008 } , publisher= { SAGE Publications Sage UK: London, England } } PL-SLAM Visual SLAM with point and line features. The paper used minimum four parameters to describe line features and proposed some method like merge lines, calculate Jacobian matrix for fast computation, and modified ORB-SLAM2 to fit with the proposed method. @article { zuo2017robust, title= { Robust Visual SLAM with Point and Line Features } , author= { Zuo, Xingxing and Xie, Xiaojia and Liu, Yong and Huang, Guoquan } , journal= { arXiv preprint arXiv:1711.08654 } , year= { 2017 } } LocNet First, extract some features based on ring data, which is possible to be rotation invariant , and kind of translation invariant (from consecutive points). Then a siamese network is used to compare the distance of two frames. Contrastive loss function is proposed to train the data here. The prior global map is utilized by a Kd-tree for fast matching. The result from matching is fused by odometry information, also a resamping for rotation estimation (wrong observation with odometry information will decrease the probability). Finally, the previous results are use as initial value for ICP algorithm. In trainning, use tw consecutive frames as the same place. Distance greater than p will be taken as negative samples. The problem of the method can be as following: 1. It cannot work on 3D environment with height changes. 2. Can only work on a place with map, i.e. localization work only. If the robots move to some places, it will not update the map. 3. Some parameter related performance. 4. The comparision of segmatch is based on the same sequence, which is a point to be attacked easily. 5. Odometry is necessary in this method currently. @article { yin2017locnet, title= { LocNet: Global localization in 3D point clouds for mobile robots } , author= { Yin, Huan and Wang, Yue and Tang, Li and Ding, Xiaqing and Xiong, Rong } , journal= { arXiv preprint arXiv:1712.02165 } , year= { 2017 } } BOW The main idea of bag of words methods is extract some features from images. Then cluster them into several clusters as words. In the cited paper, it first use kmeans++ to get the words. A vocabulary tree is used to fast search. TF-IDF then is applied for calculating the weights of the words in a image. The final vectors to be compared consist of the weights of the words. It uses inverse and direct index, which helps to find similar images and find the corresponding features faster. @article { galvez2012bags, title= { Bags of binary words for fast place recognition in image sequences } , author= { G { \\' a } lvez-L { \\' o } pez, Dorian and Tardos, Juan D } , journal= { IEEE Transactions on Robotics } , volume= { 28 } , number= { 5 } , pages= { 1188--1197 } , year= { 2012 } , publisher= { IEEE } } IMLS-SLAM A new scan-to-model framework using an implicit ( Implicit Moving Least Squares (IMLS) surface representation ) surface representation of the map. First, we compute a local de-skewed point cloud from one rotation of the 3D LiDAR. Second, we select specific samples from that point cloud to minimize the distance to the model cloud in the third part. Part 1, the de-skewed point cloud is from previous odometry and registered into the end of the point cloud. Then voxel growing is applied for ground points removal. Then small clusters are removed and ground is added back. Part 2, choose \\(9s\\) samples from the results from results of part 1. The choosing of samples is based on point features, which are aimed at different translation or rotation parameters. Part 3, A scan-to-model matching using implicit moving least square representation. Calculate the distance between implicit surface and points by \\(I^{P_k}(x)\\) . Since mininizing \\(\\sum_{x_j \\in \\tilde{S_k}}|I^{P_k}(x)(Rx_j+t)|^2\\) cannot be approximate by a linear least-square problem. A projection of points on IMLS is calculated, and the points are \\(Y_k: y_j = x_j-I^{P_k}(x)\\vec{n_j}\\) . Then minimize error \\(\\sum_{x_j \\in \\tilde{S_k}}|\\vec{n_j}(Rx_j+t-y_j)|^2\\) , which stands for the differences between \\(x_j\\) and \\(y_j\\) . After one iteration, new projection and transformation is calculated. Comments: Not real-time. Some procedures in the pipeline seems not very useful, like the removal of dynamic objects Many we can test on the mapping job for JD, which is based on the merged local map.","title":"2018 February Reading Reports"},{"location":"2018/2018%20Feb/#2018-february-reading-reports","text":"","title":"2018 February Reading Reports"},{"location":"2018/2018%20Feb/#fab-map","text":"The mean idea of FAB-MAP is to use a probabilistic framework for appearance-based place recognition. The original paper used 128D SURF descriptors extracted from images. And then cluster them to 11k words as vocabulary. Then compute Chow Liu tree to approximate the probability of \\(p(Z_k|L_i, Z^{k-1})\\) instead of a Naive Bayesian assumption. Hidden events \\(e_i\\) are introduced to provide ability to incorporate multiple sensors and factor the distribution \\(p(Z|L_j)\\) into two parts. It deals with the case that a new place could appear, and the corresponding probability under such case. As mentioned by the authors, Monte Carlo approximation, or a sampling method, is applied to get a better performance. Location Prior should be given. Smoothing is applied for rejecting a single similar image pair. The assumptions in this methods are: Sets of observations are conditionally independent given position: \\(p(Z_k|L_i,Z_{k-1}) = p(Z_k|L_i)\\) . Detector behavior is independent of position: \\(p(z_j|e_j,L_i) = p(z_j|e_j)\\) . Location models are generated independently by the environment. Approximation of \\(p(e_j|Z_k) \\approx p(e_j|z_{j_k})\\) . Some comments: The method needs off-line training, which requires some conditions, e.g. no overlapping on the training data. The words are fixed in the origin work, which are obtained from descriptors in training data. Some hyper-parameters, like priors could be hard to determine. @article { cummins2008fab, title= { FAB-MAP: Probabilistic localization and mapping in the space of appearance } , author= { Cummins, Mark and Newman, Paul } , journal= { The International Journal of Robotics Research } , volume= { 27 } , number= { 6 } , pages= { 647--665 } , year= { 2008 } , publisher= { SAGE Publications Sage UK: London, England } }","title":"FAB-MAP"},{"location":"2018/2018%20Feb/#pl-slam","text":"Visual SLAM with point and line features. The paper used minimum four parameters to describe line features and proposed some method like merge lines, calculate Jacobian matrix for fast computation, and modified ORB-SLAM2 to fit with the proposed method. @article { zuo2017robust, title= { Robust Visual SLAM with Point and Line Features } , author= { Zuo, Xingxing and Xie, Xiaojia and Liu, Yong and Huang, Guoquan } , journal= { arXiv preprint arXiv:1711.08654 } , year= { 2017 } }","title":"PL-SLAM"},{"location":"2018/2018%20Feb/#locnet","text":"First, extract some features based on ring data, which is possible to be rotation invariant , and kind of translation invariant (from consecutive points). Then a siamese network is used to compare the distance of two frames. Contrastive loss function is proposed to train the data here. The prior global map is utilized by a Kd-tree for fast matching. The result from matching is fused by odometry information, also a resamping for rotation estimation (wrong observation with odometry information will decrease the probability). Finally, the previous results are use as initial value for ICP algorithm. In trainning, use tw consecutive frames as the same place. Distance greater than p will be taken as negative samples. The problem of the method can be as following: 1. It cannot work on 3D environment with height changes. 2. Can only work on a place with map, i.e. localization work only. If the robots move to some places, it will not update the map. 3. Some parameter related performance. 4. The comparision of segmatch is based on the same sequence, which is a point to be attacked easily. 5. Odometry is necessary in this method currently. @article { yin2017locnet, title= { LocNet: Global localization in 3D point clouds for mobile robots } , author= { Yin, Huan and Wang, Yue and Tang, Li and Ding, Xiaqing and Xiong, Rong } , journal= { arXiv preprint arXiv:1712.02165 } , year= { 2017 } }","title":"LocNet"},{"location":"2018/2018%20Feb/#bow","text":"The main idea of bag of words methods is extract some features from images. Then cluster them into several clusters as words. In the cited paper, it first use kmeans++ to get the words. A vocabulary tree is used to fast search. TF-IDF then is applied for calculating the weights of the words in a image. The final vectors to be compared consist of the weights of the words. It uses inverse and direct index, which helps to find similar images and find the corresponding features faster. @article { galvez2012bags, title= { Bags of binary words for fast place recognition in image sequences } , author= { G { \\' a } lvez-L { \\' o } pez, Dorian and Tardos, Juan D } , journal= { IEEE Transactions on Robotics } , volume= { 28 } , number= { 5 } , pages= { 1188--1197 } , year= { 2012 } , publisher= { IEEE } }","title":"BOW"},{"location":"2018/2018%20Feb/#imls-slam","text":"A new scan-to-model framework using an implicit ( Implicit Moving Least Squares (IMLS) surface representation ) surface representation of the map. First, we compute a local de-skewed point cloud from one rotation of the 3D LiDAR. Second, we select specific samples from that point cloud to minimize the distance to the model cloud in the third part. Part 1, the de-skewed point cloud is from previous odometry and registered into the end of the point cloud. Then voxel growing is applied for ground points removal. Then small clusters are removed and ground is added back. Part 2, choose \\(9s\\) samples from the results from results of part 1. The choosing of samples is based on point features, which are aimed at different translation or rotation parameters. Part 3, A scan-to-model matching using implicit moving least square representation. Calculate the distance between implicit surface and points by \\(I^{P_k}(x)\\) . Since mininizing \\(\\sum_{x_j \\in \\tilde{S_k}}|I^{P_k}(x)(Rx_j+t)|^2\\) cannot be approximate by a linear least-square problem. A projection of points on IMLS is calculated, and the points are \\(Y_k: y_j = x_j-I^{P_k}(x)\\vec{n_j}\\) . Then minimize error \\(\\sum_{x_j \\in \\tilde{S_k}}|\\vec{n_j}(Rx_j+t-y_j)|^2\\) , which stands for the differences between \\(x_j\\) and \\(y_j\\) . After one iteration, new projection and transformation is calculated. Comments: Not real-time. Some procedures in the pipeline seems not very useful, like the removal of dynamic objects Many we can test on the mapping job for JD, which is based on the merged local map.","title":"IMLS-SLAM"},{"location":"2018/2018%20Jul/","text":"2018 July Reading Reports Linear SLAM This paper provides a new map joining framework which only requires solving linear least squares problems and performing nonlinear coordinate transformations. There is no assumption on the structure of the covariance matrices of the local maps. There is no need of an initial guess and no need of iterations. It has been shown that it could solve different versions of SLAM problems (feature-based SLAM, pose graph SLAM and decoupling SLAM). The consistency analysis of Linear SLAM is performed. The theoretical analysis on the computational complexity shows the efficiency. It is also pointed out that one has the freedom to choose the coordinate frame of a local map or the global map. Joining Two Pose-feature Maps using Linear Least Squares The main idea is to convert the submaps to the intermediate coordinate to make the objective function linear. Sequential Map Joining, Divide and Conquer Map joining and Computatinoal Complexity The improvements are from no iteration and no initial values, because of the linear form of the objective function. It fuses two maps at a time instead of fusing all together. Results The Consistency Analysis is done by NEES, not analytically proved. The Linear SLAM (LSLAM) can be faster using the Divide and Conquer method, but the estimation and the uncertainty are not as good as the full non-linear method, which is used as the upper bound for the results in the Table 2 and 4. The 3D Pose-feature Map Joining's or feature-only's results are on the simulation with only one test. The LSLAM implementation is slower than g2o. It is explained as\"g2o code is highly optimized and is one of the fastest implementations of Nonlinear LS SLAM\". One thing uncertain is that the accurate under noisy measurements or even outliers , which may cause linear solution unstable. For our related projects Might be useful to merge multiple Lidar maps. But current implementation doesn't keep features in the map. And Lidar features are different from the ones in image. One way to describe feature can be surfel as [1]. But the stability of the surfel features requires further investigation. Using LSLAM can obtain suboptimal results efficiently, but not as good as state-of-the-art non-linear method. [1] Behley J, Stachniss C. Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments[J]. LIMO Feature extraction and preprocessing (Block A,B) The feature tracking methodology is viso2. And semantic image is used for dynamic object removing. Scale estimation (Block S) The depth corresponding to detected feature points is extracted from LIDAR. One shot depth estimation approach is applied. LIDAR points are projected onto the image plane. For each feature point \\(f\\) , region around the point \\(F\\) , which is segmented as \\(F_{seg}\\) (using histogram by depth) and fitted to a plane \\(p\\) (triangle \\(F_{\\Delta}\\) with maximum area). Ground plane will have a different fitting algorithm. Intersect \\(p\\) with the line of sight corresponding to $f $ to get its depth. Perform a test for the estimated depth. Frame to frame odometry (Block C) PnP and fundamental matrix added with Cauchy function are used to form the cost function. Backend (Block D) Keyframes, landmarks are selected by the methods introduced (no Block E). Keyframes are chosen when motion (rotation) is rapid, and rejected when the mean optical flow is smaller than a fix threshold. The remaining frames are chosen in time intervals of 0.3 s. Connectivity of the keyframes by counting landmarks that connect the current keyframe with the newest keyframe is used to determine the length of the optimization window. The window is bounded on both sides. Landmarks are categorized in near, middle and far bins. Near with the largest optical flow, random (the idea is to use unseen landmarks) for the middle bin. Longest tracking length for the far points. Then a semantic information is used to determine the weight of landmarks (0.9 for vegetation). Landmark depth is inserted by measured depth, and constrained by the oldest motion. Robustification and trimmed-least-square-like approach are applied on the problem formulation. Inverse Depth Parameterization Doubling the map state vector size, but it benefits the intialization and distant features. Low parallax features (XYZ feature parameterization) are not well represented by the Gaussian distributions implicit. The single Gaussian representation that the inverse depth parametrization, including undelayed initialization large \u201cinfinite\u201d depths, which can be useful for the rotation estimation. It is important to represent the uncertainty in depth of seemingly infinite features. 6-D state vector \\([x_i, y_i, z_i, \\theta_i, \\phi_i, \\rho_i]^T\\) is used for each of the feature points. It proves and shows that the inverse depth parameterization can have better linearity in both low or high parallax, if the feature is continuously observed. It also convert inverse depth into XYZ the linearity index is less enough. V-LOAM The method is suitable when the sensors moves at a high speed and is subject to significant lighting changes. It is divided into two sections. One is the visual odometry section, which contains depth map registration block. Another one is Lidar odometry. The sensor pose outputs are integration of the transforms from both sections. The visual odometry solve two types of equations about known or unknown depth to obtain the 6-DOF motion. Then the depthmap is maintained. Associated points form the plane is used to get the projection of a ray on the plane, while the unassociated points are triangulated. Then the results from visual odometry is refined by the lidar odometry method, which contains sweep to sweep registration and sweep to map registration. The drift of visual odometry is modeled as linear drift. The remaining part is similar with LOAM. There are many tests on the methods, while the method itself is not tight-coupled. SOFT-SLAM SOFT-SLAM enables a complete separation of the odometry part and mapping part. It does not share the map between SOFT (stereo odometry algorithm relying on feature tracking) odometry and mapping threads as PTAM, ORB-SLAM2. It yields a constant runtime with global consistency. An exponential filter merges the odometry and mapping parts. Odometry Thread If IMU is available, gyroscopic measurements and one-point RANSAC optimization are applied. Otherwise, a three-point algorithm is applied. For KITTI (five-point Nister algorithm, five-point RANSAC on the left stereo image), the rotation from a single camera can improve localization precision. The sum of absolute differences (SAD) is used. Circular matching, both left and right images of two consecutive frames corresponding to steps \\(k \u2212 1\\) and \\(k\\) . \\(\\mathcal{I}^L_{k-1} \\rightarrow \\mathcal{I}^R_{k-1} \\rightarrow \\mathcal{I}^R_k \\rightarrow \\mathcal{I}^L_k \\rightarrow \\mathcal{I}^L_{k-1}\\) . Normalized cross correlation (NCC) as an additional check. Properties includes (i) unique identifier, (ii) age, (iii) current position in image space, (iv) feature strength, (v) class, and (vi) initial descriptor. Spatial and temporal variety policy (careful selection) is realized. For no IMU fusion, five-point RANSAC (closed-form) for rotation and is not updated with translation. The translation is first from the one-point RANSAC, and optimized in the Gaussian-Newton algorithm (weighted points). With IMU, it uses Kalman filter to estimate the rotation and bias. Use one-point RANSAC to find the inlier features. Finally, it optimize the rotation and translation in the optimization. A exponential filtering is applied to smooth the outputs. Mapping Thread Put feature management into the frame queue. The keyframe is set according to the distances from the last stored keyframe. Then loop closure is checked, following with a sparse set of poses only optimization. The pose and covariance of the keyframe are proportional to the distance between the previous and the new keyframe. Temporary keyframe will be discard after the graph optimization, while full map keyframe will be insert to the graph. Loop Closing Loop closing is from the candidate keyframes with the radius and angle. Histograms of each keyframe consisting of four classes are measured by SAD. For the candidates: Circular matching and NCC. Three-point RANSAC. Optimization when the number of inliers is larger than the threshold. Check if the final linking distance is too large. Levenberg-Marquard algorithm in g2o to optimize the pose graph. The uncertainties are assumed as proportional to the magnitude of the relative motion (rotation and translation). ORB-SLAM2 Stereo keypoints are defined by three coordinates \\(\\mathbf{x}_s = (u_L,v_L,u_R)\\) . Then for both stereo or RGB-D cameras, \\(u_R=u_L-\\frac{f_x b}{d}\\) is applied. Monocular keypoints are defined by two coordinates \\(\\mathbf{x}_m=(u_L,v_L)\\) . As the same as ORB-SLAM, the system performs BA to optimize the camera pose in the tracking thread ( motion-only BA ), to optimize a local window of keyframes and points in the local mapping thread ( local BA ), and after a loop closure to optimize all keyframes and points ( full BA ). Levenberg\u2013Marquardt method is applied. Different projection functions \\(\\pi_m\\) and \\(\\pi_s\\) are used for these two kinds of keypoints. Loop closing will abort the full BA optimization. When the loop is closed, the full BA will be optimized again. Nonupdated keyframes and points will be propagated through the spanning tree. The keyframe insertion is the same as ORB-SLAM. In localization mode, the tracking leverages visual odometry matches (matched with the previous frame, will drift) and matches to map point (drift-free). Multi-threading system has the nondeterministic nature. Two metrics are used to evaluate the results, the absolute translation root-mean-square error (RMSE) and the average relative translation and rotation errors.","title":"2018 July Reading Reports"},{"location":"2018/2018%20Jul/#2018-july-reading-reports","text":"","title":"2018 July Reading Reports"},{"location":"2018/2018%20Jul/#linear-slam","text":"This paper provides a new map joining framework which only requires solving linear least squares problems and performing nonlinear coordinate transformations. There is no assumption on the structure of the covariance matrices of the local maps. There is no need of an initial guess and no need of iterations. It has been shown that it could solve different versions of SLAM problems (feature-based SLAM, pose graph SLAM and decoupling SLAM). The consistency analysis of Linear SLAM is performed. The theoretical analysis on the computational complexity shows the efficiency. It is also pointed out that one has the freedom to choose the coordinate frame of a local map or the global map.","title":"Linear SLAM"},{"location":"2018/2018%20Jul/#joining-two-pose-feature-maps-using-linear-least-squares","text":"The main idea is to convert the submaps to the intermediate coordinate to make the objective function linear.","title":"Joining Two Pose-feature Maps using Linear Least Squares"},{"location":"2018/2018%20Jul/#sequential-map-joining-divide-and-conquer-map-joining-and-computatinoal-complexity","text":"The improvements are from no iteration and no initial values, because of the linear form of the objective function. It fuses two maps at a time instead of fusing all together.","title":"Sequential Map Joining, Divide and Conquer Map joining and Computatinoal Complexity"},{"location":"2018/2018%20Jul/#results","text":"The Consistency Analysis is done by NEES, not analytically proved. The Linear SLAM (LSLAM) can be faster using the Divide and Conquer method, but the estimation and the uncertainty are not as good as the full non-linear method, which is used as the upper bound for the results in the Table 2 and 4. The 3D Pose-feature Map Joining's or feature-only's results are on the simulation with only one test. The LSLAM implementation is slower than g2o. It is explained as\"g2o code is highly optimized and is one of the fastest implementations of Nonlinear LS SLAM\". One thing uncertain is that the accurate under noisy measurements or even outliers , which may cause linear solution unstable.","title":"Results"},{"location":"2018/2018%20Jul/#for-our-related-projects","text":"Might be useful to merge multiple Lidar maps. But current implementation doesn't keep features in the map. And Lidar features are different from the ones in image. One way to describe feature can be surfel as [1]. But the stability of the surfel features requires further investigation. Using LSLAM can obtain suboptimal results efficiently, but not as good as state-of-the-art non-linear method. [1] Behley J, Stachniss C. Efficient Surfel-Based SLAM using 3D Laser Range Data in Urban Environments[J].","title":"For our related projects"},{"location":"2018/2018%20Jul/#limo","text":"","title":"LIMO"},{"location":"2018/2018%20Jul/#feature-extraction-and-preprocessing","text":"(Block A,B) The feature tracking methodology is viso2. And semantic image is used for dynamic object removing.","title":"Feature extraction and preprocessing"},{"location":"2018/2018%20Jul/#scale-estimation","text":"(Block S) The depth corresponding to detected feature points is extracted from LIDAR. One shot depth estimation approach is applied. LIDAR points are projected onto the image plane. For each feature point \\(f\\) , region around the point \\(F\\) , which is segmented as \\(F_{seg}\\) (using histogram by depth) and fitted to a plane \\(p\\) (triangle \\(F_{\\Delta}\\) with maximum area). Ground plane will have a different fitting algorithm. Intersect \\(p\\) with the line of sight corresponding to $f $ to get its depth. Perform a test for the estimated depth.","title":"Scale estimation"},{"location":"2018/2018%20Jul/#frame-to-frame-odometry","text":"(Block C) PnP and fundamental matrix added with Cauchy function are used to form the cost function.","title":"Frame to frame odometry"},{"location":"2018/2018%20Jul/#backend","text":"(Block D) Keyframes, landmarks are selected by the methods introduced (no Block E). Keyframes are chosen when motion (rotation) is rapid, and rejected when the mean optical flow is smaller than a fix threshold. The remaining frames are chosen in time intervals of 0.3 s. Connectivity of the keyframes by counting landmarks that connect the current keyframe with the newest keyframe is used to determine the length of the optimization window. The window is bounded on both sides. Landmarks are categorized in near, middle and far bins. Near with the largest optical flow, random (the idea is to use unseen landmarks) for the middle bin. Longest tracking length for the far points. Then a semantic information is used to determine the weight of landmarks (0.9 for vegetation). Landmark depth is inserted by measured depth, and constrained by the oldest motion. Robustification and trimmed-least-square-like approach are applied on the problem formulation.","title":"Backend"},{"location":"2018/2018%20Jul/#inverse-depth-parameterization","text":"Doubling the map state vector size, but it benefits the intialization and distant features. Low parallax features (XYZ feature parameterization) are not well represented by the Gaussian distributions implicit. The single Gaussian representation that the inverse depth parametrization, including undelayed initialization large \u201cinfinite\u201d depths, which can be useful for the rotation estimation. It is important to represent the uncertainty in depth of seemingly infinite features. 6-D state vector \\([x_i, y_i, z_i, \\theta_i, \\phi_i, \\rho_i]^T\\) is used for each of the feature points. It proves and shows that the inverse depth parameterization can have better linearity in both low or high parallax, if the feature is continuously observed. It also convert inverse depth into XYZ the linearity index is less enough.","title":"Inverse Depth Parameterization"},{"location":"2018/2018%20Jul/#v-loam","text":"The method is suitable when the sensors moves at a high speed and is subject to significant lighting changes. It is divided into two sections. One is the visual odometry section, which contains depth map registration block. Another one is Lidar odometry. The sensor pose outputs are integration of the transforms from both sections. The visual odometry solve two types of equations about known or unknown depth to obtain the 6-DOF motion. Then the depthmap is maintained. Associated points form the plane is used to get the projection of a ray on the plane, while the unassociated points are triangulated. Then the results from visual odometry is refined by the lidar odometry method, which contains sweep to sweep registration and sweep to map registration. The drift of visual odometry is modeled as linear drift. The remaining part is similar with LOAM. There are many tests on the methods, while the method itself is not tight-coupled.","title":"V-LOAM"},{"location":"2018/2018%20Jul/#soft-slam","text":"SOFT-SLAM enables a complete separation of the odometry part and mapping part. It does not share the map between SOFT (stereo odometry algorithm relying on feature tracking) odometry and mapping threads as PTAM, ORB-SLAM2. It yields a constant runtime with global consistency. An exponential filter merges the odometry and mapping parts.","title":"SOFT-SLAM"},{"location":"2018/2018%20Jul/#odometry-thread","text":"If IMU is available, gyroscopic measurements and one-point RANSAC optimization are applied. Otherwise, a three-point algorithm is applied. For KITTI (five-point Nister algorithm, five-point RANSAC on the left stereo image), the rotation from a single camera can improve localization precision. The sum of absolute differences (SAD) is used. Circular matching, both left and right images of two consecutive frames corresponding to steps \\(k \u2212 1\\) and \\(k\\) . \\(\\mathcal{I}^L_{k-1} \\rightarrow \\mathcal{I}^R_{k-1} \\rightarrow \\mathcal{I}^R_k \\rightarrow \\mathcal{I}^L_k \\rightarrow \\mathcal{I}^L_{k-1}\\) . Normalized cross correlation (NCC) as an additional check. Properties includes (i) unique identifier, (ii) age, (iii) current position in image space, (iv) feature strength, (v) class, and (vi) initial descriptor. Spatial and temporal variety policy (careful selection) is realized. For no IMU fusion, five-point RANSAC (closed-form) for rotation and is not updated with translation. The translation is first from the one-point RANSAC, and optimized in the Gaussian-Newton algorithm (weighted points). With IMU, it uses Kalman filter to estimate the rotation and bias. Use one-point RANSAC to find the inlier features. Finally, it optimize the rotation and translation in the optimization. A exponential filtering is applied to smooth the outputs.","title":"Odometry Thread"},{"location":"2018/2018%20Jul/#mapping-thread","text":"Put feature management into the frame queue. The keyframe is set according to the distances from the last stored keyframe. Then loop closure is checked, following with a sparse set of poses only optimization. The pose and covariance of the keyframe are proportional to the distance between the previous and the new keyframe. Temporary keyframe will be discard after the graph optimization, while full map keyframe will be insert to the graph.","title":"Mapping Thread"},{"location":"2018/2018%20Jul/#loop-closing","text":"Loop closing is from the candidate keyframes with the radius and angle. Histograms of each keyframe consisting of four classes are measured by SAD. For the candidates: Circular matching and NCC. Three-point RANSAC. Optimization when the number of inliers is larger than the threshold. Check if the final linking distance is too large. Levenberg-Marquard algorithm in g2o to optimize the pose graph. The uncertainties are assumed as proportional to the magnitude of the relative motion (rotation and translation).","title":"Loop Closing"},{"location":"2018/2018%20Jul/#orb-slam2","text":"Stereo keypoints are defined by three coordinates \\(\\mathbf{x}_s = (u_L,v_L,u_R)\\) . Then for both stereo or RGB-D cameras, \\(u_R=u_L-\\frac{f_x b}{d}\\) is applied. Monocular keypoints are defined by two coordinates \\(\\mathbf{x}_m=(u_L,v_L)\\) . As the same as ORB-SLAM, the system performs BA to optimize the camera pose in the tracking thread ( motion-only BA ), to optimize a local window of keyframes and points in the local mapping thread ( local BA ), and after a loop closure to optimize all keyframes and points ( full BA ). Levenberg\u2013Marquardt method is applied. Different projection functions \\(\\pi_m\\) and \\(\\pi_s\\) are used for these two kinds of keypoints. Loop closing will abort the full BA optimization. When the loop is closed, the full BA will be optimized again. Nonupdated keyframes and points will be propagated through the spanning tree. The keyframe insertion is the same as ORB-SLAM. In localization mode, the tracking leverages visual odometry matches (matched with the previous frame, will drift) and matches to map point (drift-free). Multi-threading system has the nondeterministic nature. Two metrics are used to evaluate the results, the absolute translation root-mean-square error (RMSE) and the average relative translation and rotation errors.","title":"ORB-SLAM2"},{"location":"2018/2018%20Jun/","text":"2018 June Reading Reports SVO It eliminates the need of costly feature extraction and robust matching techniques and operates directly on pixel intensities which results in subpixel precision at high frame-rates. It combines a semi-direct VO that combines the success-factors of feature-based methods (tracking many features, parallel tracking and mapping, keyframe selection). Compared to dense direct method, only pixels characterized by strong gradient is used in this paper. The method still needs feature-correspondence, but the feature extraction and matching part is from direct motion estimation. Small patches rather than few (tens) large planar patches. The algorithm continues using only point-features after initialization. Probabilistic mapping method. A Bayesian filter estimate the depth at feature locations, only inserted when the depth-filter has converged. Two threads are applied in the program, one for estimating the camera motion, and a second one for mapping. Motion estimation Minimize the photometric error. Then refined through alignment of the corresponding feature-patches. Finally motion estimation concludes by refining the pose and the structure through minimizing the reprojection error in the previous feature-alignment step. Inverse compositional approach results in a significant speedup. Refined by being aligned w.r.t the map. Inverse compositional Lucas-Kanade algorithm is used. First motion-only BA, then structure only BA. Mapping A large uncertainty in depth when new keyframe is selected. For every subsequent observation, using the one with the highest correlation with the reference patch. The use of depth-filters help to reduce the features the system needed. Evaluation The paper aligned the first 10 frames with the ground-truth using [31]. [31] S. Umeyama, \u201cLeast-Squares Estimation of Transformation Parameters Between Two Point Patterns,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 13, no. 4, 1991. Least-squares estimation of transformation parameters between two point patterns \\[ \\mathbf{\\mu}_x = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i \\\\ \\mathbf{\\mu}_y = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{y}_i \\\\ \\sigma_x^2 = \\frac{1}{n}\\sum_{i=1}^n ||\\mathbf{x}_i - \\mathbf{\\mu}_x ||^2 \\\\ \\sigma_y^2 = \\frac{1}{n}\\sum_{i=1}^n ||\\mathbf{y}_i - \\mathbf{\\mu}_y ||^2 \\\\ \\Sigma_{xy} = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{y}_i - \\mathbf{\\mu}_y )(\\mathbf{x}_i - \\mathbf{\\mu}_x)^T \\\\ \\Sigma_{xy} = UDV^T \\] where \\(D=\\text{diag}(d_i), d_1 \\geq d_2 \\geq \\cdots d_m \\geq 0\\) . \\[ R = USV^T \\\\ t = \\mathbf{\\mu}_y - cR\\mathbf{\\mu}_x \\\\ c = \\frac{1}{\\sigma_x^2}\\text{tr}(DS) \\\\ S =\\begin{cases} I & \\text{if det}(U)\\text{det}(V) = 1 \\\\ \\text{diag}(1, 1, \\cdots, 1, -1) & \\text{if det}(U)\\text{det}(V) = -1 \\end{cases} \\\\ e^2(R,t,c) = \\frac{1}{n}\\sum_{i=1}^n||\\mathbf{y}_i - (cR\\mathbf{x}_i + t)||^2 \\\\ \\min e^2(R,t,c) = \\epsilon^2(R,t,c) \\\\ = \\sigma_y^2 + c^2{\\sigma_x^2} - 2c\\text{tr}(DS) = \\sigma_y^2 - \\frac{\\text{tr}(DS)^2}{\\sigma_x^2} \\] Complementary Perception for Handheld SLAM It uses continuous-time trajectory estimation. The algorithm has association of depth uncertainty to each visual feature and a novel means of estimating feature depth and uncertainty. A comparison against state of the art techniques on six datasets. Generating Visual Constraints (combined covariance) are applied for 3D-3D, 3D-2D and 2D-2D constraints. It constraints with and without depth measurements. ICE-BA The acceleration of local BA and the relative marginalization for the global consistency. Since in local BA most points can be observed by most frames in the sliding window, common incremental BA will not be efficient anymore. The Sub-Track based IBA helps to solve the problem. The main idea is to split the origin long feature track into several short overlapping Sub-Tracks. Incremental PCG (I-PCG) is used with different initial values to improve the accuracy, due to better convergence. The method maintains the consistency between marginalization prior and global BA with the relative marginalization . It formulates the prior relative to the reference keyframe coordinate system instead of the global coordinate system. The relative representation is only used in marginalization. C-KLAM It utilizes both proprioceptive [e.g., inertial measurement unit (IMU)] and exteroceptive (e.g., camera) measurements from non-keyframes to generate pose constraints between the keyframes in a consistent manner . C-KLAM incorporates information from marginalized frames and landmarks without destroying the sparsity of the information matrix. Marginalization is cubic for non-key poses and linear for non-key landmarks. The keyframes and the associated landmark-map are maintained over the entire robot trajectory. The main idea is to approximate the MAP estimator by ignoring the data association when doing marginalization. The features are duplicated and marginalization the virtual features and the features in keyframes, which are not marginalized out are still kept. SuMa Efficient mapping of three-dimensional laser range data results in globally consistent maps. A map-based criterion for loop closure even in situations with small overlap between scans. Surfel there is defined by a position, a normal and a radius, a creation and a update timestamp. Preprocessing, Map Representation, Odometry Estimation, and Map Update The stability of the map can be adopted in our projects. There is still no global loop closure search in this work. Projective data association is used. Vertex map are projected from sphere , while normals are calculated from cross products. Map representation is updated by binary Bayes Filter . For the odometry, it determines the compatibility of the current laser scan and nearby poses using point-to-plane ICP based on the vertex and normal maps . The radius of surfel is used to update the map if the data surfel. It increases or decreases the stability if the data surfel is compatible or not. Loop Closure Detection The detection is based on the residuals of that composed map \\(E_{map}\\) and the residual in respect to the active map \\(E_{odom}\\) . Verification A virtual view of the map is rendered with the poses and the map is checked with the current measurements for consistency.","title":"2018 June Reading Reports"},{"location":"2018/2018%20Jun/#2018-june-reading-reports","text":"","title":"2018 June Reading Reports"},{"location":"2018/2018%20Jun/#svo","text":"It eliminates the need of costly feature extraction and robust matching techniques and operates directly on pixel intensities which results in subpixel precision at high frame-rates. It combines a semi-direct VO that combines the success-factors of feature-based methods (tracking many features, parallel tracking and mapping, keyframe selection). Compared to dense direct method, only pixels characterized by strong gradient is used in this paper. The method still needs feature-correspondence, but the feature extraction and matching part is from direct motion estimation. Small patches rather than few (tens) large planar patches. The algorithm continues using only point-features after initialization. Probabilistic mapping method. A Bayesian filter estimate the depth at feature locations, only inserted when the depth-filter has converged. Two threads are applied in the program, one for estimating the camera motion, and a second one for mapping.","title":"SVO"},{"location":"2018/2018%20Jun/#motion-estimation","text":"Minimize the photometric error. Then refined through alignment of the corresponding feature-patches. Finally motion estimation concludes by refining the pose and the structure through minimizing the reprojection error in the previous feature-alignment step. Inverse compositional approach results in a significant speedup. Refined by being aligned w.r.t the map. Inverse compositional Lucas-Kanade algorithm is used. First motion-only BA, then structure only BA.","title":"Motion estimation"},{"location":"2018/2018%20Jun/#mapping","text":"A large uncertainty in depth when new keyframe is selected. For every subsequent observation, using the one with the highest correlation with the reference patch. The use of depth-filters help to reduce the features the system needed.","title":"Mapping"},{"location":"2018/2018%20Jun/#evaluation","text":"The paper aligned the first 10 frames with the ground-truth using [31]. [31] S. Umeyama, \u201cLeast-Squares Estimation of Transformation Parameters Between Two Point Patterns,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 13, no. 4, 1991.","title":"Evaluation"},{"location":"2018/2018%20Jun/#least-squares-estimation-of-transformation-parameters-between-two-point-patterns","text":"\\[ \\mathbf{\\mu}_x = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{x}_i \\\\ \\mathbf{\\mu}_y = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{y}_i \\\\ \\sigma_x^2 = \\frac{1}{n}\\sum_{i=1}^n ||\\mathbf{x}_i - \\mathbf{\\mu}_x ||^2 \\\\ \\sigma_y^2 = \\frac{1}{n}\\sum_{i=1}^n ||\\mathbf{y}_i - \\mathbf{\\mu}_y ||^2 \\\\ \\Sigma_{xy} = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{y}_i - \\mathbf{\\mu}_y )(\\mathbf{x}_i - \\mathbf{\\mu}_x)^T \\\\ \\Sigma_{xy} = UDV^T \\] where \\(D=\\text{diag}(d_i), d_1 \\geq d_2 \\geq \\cdots d_m \\geq 0\\) . \\[ R = USV^T \\\\ t = \\mathbf{\\mu}_y - cR\\mathbf{\\mu}_x \\\\ c = \\frac{1}{\\sigma_x^2}\\text{tr}(DS) \\\\ S =\\begin{cases} I & \\text{if det}(U)\\text{det}(V) = 1 \\\\ \\text{diag}(1, 1, \\cdots, 1, -1) & \\text{if det}(U)\\text{det}(V) = -1 \\end{cases} \\\\ e^2(R,t,c) = \\frac{1}{n}\\sum_{i=1}^n||\\mathbf{y}_i - (cR\\mathbf{x}_i + t)||^2 \\\\ \\min e^2(R,t,c) = \\epsilon^2(R,t,c) \\\\ = \\sigma_y^2 + c^2{\\sigma_x^2} - 2c\\text{tr}(DS) = \\sigma_y^2 - \\frac{\\text{tr}(DS)^2}{\\sigma_x^2} \\]","title":"Least-squares estimation of transformation parameters between two point patterns"},{"location":"2018/2018%20Jun/#complementary-perception-for-handheld-slam","text":"It uses continuous-time trajectory estimation. The algorithm has association of depth uncertainty to each visual feature and a novel means of estimating feature depth and uncertainty. A comparison against state of the art techniques on six datasets. Generating Visual Constraints (combined covariance) are applied for 3D-3D, 3D-2D and 2D-2D constraints. It constraints with and without depth measurements.","title":"Complementary Perception for Handheld SLAM"},{"location":"2018/2018%20Jun/#ice-ba","text":"The acceleration of local BA and the relative marginalization for the global consistency. Since in local BA most points can be observed by most frames in the sliding window, common incremental BA will not be efficient anymore. The Sub-Track based IBA helps to solve the problem. The main idea is to split the origin long feature track into several short overlapping Sub-Tracks. Incremental PCG (I-PCG) is used with different initial values to improve the accuracy, due to better convergence. The method maintains the consistency between marginalization prior and global BA with the relative marginalization . It formulates the prior relative to the reference keyframe coordinate system instead of the global coordinate system. The relative representation is only used in marginalization.","title":"ICE-BA"},{"location":"2018/2018%20Jun/#c-klam","text":"It utilizes both proprioceptive [e.g., inertial measurement unit (IMU)] and exteroceptive (e.g., camera) measurements from non-keyframes to generate pose constraints between the keyframes in a consistent manner . C-KLAM incorporates information from marginalized frames and landmarks without destroying the sparsity of the information matrix. Marginalization is cubic for non-key poses and linear for non-key landmarks. The keyframes and the associated landmark-map are maintained over the entire robot trajectory. The main idea is to approximate the MAP estimator by ignoring the data association when doing marginalization. The features are duplicated and marginalization the virtual features and the features in keyframes, which are not marginalized out are still kept.","title":"C-KLAM"},{"location":"2018/2018%20Jun/#suma","text":"Efficient mapping of three-dimensional laser range data results in globally consistent maps. A map-based criterion for loop closure even in situations with small overlap between scans. Surfel there is defined by a position, a normal and a radius, a creation and a update timestamp.","title":"SuMa"},{"location":"2018/2018%20Jun/#preprocessing-map-representation-odometry-estimation-and-map-update","text":"The stability of the map can be adopted in our projects. There is still no global loop closure search in this work. Projective data association is used. Vertex map are projected from sphere , while normals are calculated from cross products. Map representation is updated by binary Bayes Filter . For the odometry, it determines the compatibility of the current laser scan and nearby poses using point-to-plane ICP based on the vertex and normal maps . The radius of surfel is used to update the map if the data surfel. It increases or decreases the stability if the data surfel is compatible or not.","title":"Preprocessing, Map Representation, Odometry Estimation, and Map Update"},{"location":"2018/2018%20Jun/#loop-closure","text":"","title":"Loop Closure"},{"location":"2018/2018%20Jun/#detection","text":"The detection is based on the residuals of that composed map \\(E_{map}\\) and the residual in respect to the active map \\(E_{odom}\\) .","title":"Detection"},{"location":"2018/2018%20Jun/#verification","text":"A virtual view of the map is rendered with the poses and the map is checked with the current measurements for consistency.","title":"Verification"},{"location":"2018/2018%20Mar/","text":"2018 March Reading Reports iBoW-LCD Incremental Bags of Binary Words with inverted index and concept of islands etc. The ideas in LCD part can be adopted in further application. Different from previous offline bag of words, this method using online incremental BoW for loop closure detection. The words in the paper is binary, thus an algorithm based on Muja's hierarchical structure of indexing and matching binary features is adopted and be modified into incremental fashion. This structure has the ability to add or delete the words online. The tests in the paper use ORB descriptors. Incremental Segment-Based Localization in 3D Point Clouds The most different parts of this paper and Segmatch are the incremental methods for the dynamic voxel grid method, the calculation of the normals, the curvatures, the region growing segmentation, and a faster geometric validation method based on maximum pairwise consistent set (MPCS). NICP First, it introduces projection and unprojection of a 3D point cloud onto a range image. The surface normal is calculated by integral images. And for reducing the noise of plane, the ellipsoid is changed as the same as what is mentioned in GICP. Then points are projected into index image. For the current points, it only computed only; while for the reference points, the index image is calculated with actual estimate T. Then, some correspondences are found as candidates, and some candidates are discarded by distance, curvature and normal angle. To determine the transformation, it uses 6 length vectors and its corresponding information matrix, the local parameterization of the perturbation is used to do calculation iteratively. KineticFusion Dense Surface Four parts, Surface measurement, surface reconstruction update, surface prediction, sensor pose estimation. For the surface measurement, noise of points are reduced in depth map and back-project the filtered one into the sensor's frame. The normal of each pixel is calculated by two of the neighbor points. Then multi-scale pyramid depth map (vertex) and normal map are calculated. Volumetric truncated signed distance function (TSDF) is used to present the plane in the method, for the points above the surface \\(\\mu\\) is used to truncate the value; the points behind \\(\\mu\\) will be null. Raw depth data and weights are applied for fusing multiple measurements. \\(\\textbf{p}\\) is orthogonal to the zero level set, the normal on the plane is calculated by numerical derivative of the SDF. The range of the sensor is limited, thus it keeps the calculation as constant. A simple approximation is used for the higher quality intersections. For the final pose estimation, model prediction is used instead of just one frame. Fast projective data association algorithm to obtain correspondences. Some outliers are filtered based on some features, e.g. the mask, the distance between model and scan, and the normal orientation difference. Then a ICP like method based on the point-plane metric is used for the final results, with the help of pyramids from coarse to fine framework. Finally, a stability and validity check, i.e null space of the normal system , and magnitude of incremental transform parameters makes only good tracking to update the model. DVO The name is from NICP as dense visual odometry. Assuming a static scene, and the twist is constant, and the temporal derivative of the image is constant. Energy minimization or maximize photoconsistency on the image warps, which is like the unprojection into the image frame (only the 3D points part of the image). The final error function can be consider as intensity differences between two images, and results in a least square problem with 1x6 constraints and a 6x6 normal equation. A coarse to fine refinement is applied also. NDT It is designed for the matching in mine tunnel. First, subdivide the space occupied by the model into regularly sized cells (squares in 2D or cubes in 3D). Then for the cell containing more than some minimum number of points, the mean vector and covariance matrix are calculated. And use \\(N(\\mathbf{q},\\mathbf{C)}\\) to model the normal distribution. An error function is built based on the normal distribution with transformation \\(\\textbf{p}\\) . Then minimizing the error function by Newton's algorithm. For the 3D case, the main different is the Hessian and gradient for the transformation. It is derived in the paper. Then some practical implementations of 3D-NDT are discussed, e.g. the sampling method, the cell size and the discretization methods. ICP (iterative closest point) algorithm (Besl&McKay, 1992; Chen&Medioni, 1992) Integral Images Surface normal from organized point cloud data using integral images. It is suitable for organized points. Smoothing upon different depth values, depth changes are utilized for the final smoothing area map. Two methods are proposed using the smoothed integral images. One is the smoothed depth changes (by cross product on two vectors based on the adjacent smoothed integral images) and the other is based on optimization (covariance matrices). This method is fast and robust for organized point cloud, but it could smooths over edges. ORB-SLAM Orb-slam contains four major parts: tracking, mapping, relocalization, and loop closing . It is viewpoint and illumination invariant, and uses covisibility graph to make the tracking and mapping only related to the local covisible area. Essential graph is build from a spanning tree. Relocalization and initialization are discussed. Survival of the fittest approach is used to map point and keyframe selection, which improves tracking robustness and discards redundant keyframes. LM algorithm in g2o is used to carry out all optimizations. For each map point \\(p_i\\) , it stores its 3-D position \\(X_{w,i}\\) in the world coordinate system, the viewing direction, a representive ORB descriptor and maximum, minimum distances at which the point can be observed. For each keyframe \\(K_i\\) is stores the camera pose \\(T_{iw}\\) , from the world to the camera coordinate system, the camera intrinsics and all the ORB features extracted in the frame (associated or not with a map point, undistorted). The covisibility graph is a graph whose edges connect every two keyframes sharing observations of the same map points (at least 15). An essential graph is a strong network of cameras containing subset of edges from the covisibility graph with high covisibility ( \\(\\theta_{min} = 100\\) ), and the loop closure edges. Initialization Two methods are used in parallel. Extract ORB features with the finest scale. The one is the homography for the planar or low parallax cases; the other one is the fundamental matrix for the nonplanar or enough parallax cases. Finally a full BA is used to refine the initial reconstruction. Tracking For every frame from the camera, motion-only BA is applied. First ORB extraction using different parameters according to different pixels in the datasets. Then initial pose is estimated from previous frame using a constant velocity motion model (guided search). If this model is violated (not enough matches), a searching around map points positions in the last frame is applied (wider search). If the tracking is lost, global relocalization will work. PnP is used to find a camera pose (RANSAC). Tracking local map requires a local map containing the keyframes \\(K_1\\) with the same map points in current frame ( \\(K_{ref} \\in K_1\\) , which share the most map points) and \\(K_2\\) the neighbors with \\(K_1\\) . Discard the points lay out of the bounds; discard the points with too large distance between viewing directions and viewing rays; discard the points out of the scale invariance region; compute the scale and match unmatched features with the predicted scale. Keyframes: 20 frames (global relocalization and local mapping is idle or after the last keyframe insertion), 50 points, 90% points. Local Mapping For every new keyframe \\(K_i\\) . Insert new keyframe: update the covisibility graph, add a new node and edges, update the spanning tree, and the bags of words representation. Local points culling, trackable points are retained (25% visible, at least three keyframes). New map point creation, triangulating ORB from connected keyframes. Projected in the rest of connected keyframes. Local bundle adjustment (local BA) , currently processed keyframe, all the keyframes connected to it in the covisibility graph and all the map points seen by those keyframes. Local keyframe culling, detect redundant keyframes and delete them (90% in at least other three keyframes). Loop Cloosing The last keyframe \\(K_i\\) processed by the local mapping, and tries to detect and close loops. A normalizing score is used (discard the other keyframes with score lower than the lowest score of covisibility graph). It needs consecutively three loop candidates (keyframes). Similarity transformation, geometrical validation, 3-D-to-3-D correspondences for each loop candidate, RANSAC to find a similarity transformation. To fuse duplicated map points and insert new edges in the covisibility graph that will attach the loop closure. Pose graph optimization over the Essential Graph. Each map point is transformed according to the correction of one of the keyframes that observes it. Visual-Inertial Monocular SLAM With Map Reuse Close loops in real-time and localize the sensor reusing the map in already mapped areas. (Others like odometry, marginalize past states or need a map, which is built offline) A reliable visual-inertial initialization \u2014 provides fixed states for optimization. Zero-drift localization in the same workspace. Expect to achievebetter accuracy and robustness by using stereo or RGB-D cameras. Weakness of the proposed IMU initialization is that it relies on the initialization of the monocular SLAM. Slow initialization comparing toVINS system (investigate the use of the gyroscope). Some Ideas LOAM algorithm extracts lines (edges) and planes from single scan. Since single scan can provide such features within local information. At the same time, the provided line or plane features can help to calculate the error of lines or planes in the following scans. But these features may not be good for jobs related pure localization, since the features are just points with some limited information, like curvature or planarity. It is the problem about this method. @inproceedings { zhang2014loam, title= { LOAM: Lidar Odometry and Mapping in Real-time. } , author= { Zhang, Ji and Singh, Sanjiv } , booktitle= { Robotics: Science and Systems } , volume= { 2 } , year= { 2014 } } TODO If one wants to do place recognition, one thing is to find some common features that could appear in multiple places. @article { dube2016segmatch, title= { SegMatch: Segment based loop-closure for 3D point clouds } , author= { Dub { \\' e } , Renaud and Dugas, Daniel and Stumm, Elena and Nieto, Juan and Siegwart, Roland and Cadena, Cesar } , journal= { arXiv preprint arXiv:1609.07720 } , year= { 2016 } }","title":"2018 March Reading Reports"},{"location":"2018/2018%20Mar/#2018-march-reading-reports","text":"","title":"2018 March Reading Reports"},{"location":"2018/2018%20Mar/#ibow-lcd","text":"Incremental Bags of Binary Words with inverted index and concept of islands etc. The ideas in LCD part can be adopted in further application. Different from previous offline bag of words, this method using online incremental BoW for loop closure detection. The words in the paper is binary, thus an algorithm based on Muja's hierarchical structure of indexing and matching binary features is adopted and be modified into incremental fashion. This structure has the ability to add or delete the words online. The tests in the paper use ORB descriptors.","title":"iBoW-LCD"},{"location":"2018/2018%20Mar/#incremental-segment-based-localization-in-3d-point-clouds","text":"The most different parts of this paper and Segmatch are the incremental methods for the dynamic voxel grid method, the calculation of the normals, the curvatures, the region growing segmentation, and a faster geometric validation method based on maximum pairwise consistent set (MPCS).","title":"Incremental Segment-Based Localization in 3D Point Clouds"},{"location":"2018/2018%20Mar/#nicp","text":"First, it introduces projection and unprojection of a 3D point cloud onto a range image. The surface normal is calculated by integral images. And for reducing the noise of plane, the ellipsoid is changed as the same as what is mentioned in GICP. Then points are projected into index image. For the current points, it only computed only; while for the reference points, the index image is calculated with actual estimate T. Then, some correspondences are found as candidates, and some candidates are discarded by distance, curvature and normal angle. To determine the transformation, it uses 6 length vectors and its corresponding information matrix, the local parameterization of the perturbation is used to do calculation iteratively.","title":"NICP"},{"location":"2018/2018%20Mar/#kineticfusion","text":"Dense Surface Four parts, Surface measurement, surface reconstruction update, surface prediction, sensor pose estimation. For the surface measurement, noise of points are reduced in depth map and back-project the filtered one into the sensor's frame. The normal of each pixel is calculated by two of the neighbor points. Then multi-scale pyramid depth map (vertex) and normal map are calculated. Volumetric truncated signed distance function (TSDF) is used to present the plane in the method, for the points above the surface \\(\\mu\\) is used to truncate the value; the points behind \\(\\mu\\) will be null. Raw depth data and weights are applied for fusing multiple measurements. \\(\\textbf{p}\\) is orthogonal to the zero level set, the normal on the plane is calculated by numerical derivative of the SDF. The range of the sensor is limited, thus it keeps the calculation as constant. A simple approximation is used for the higher quality intersections. For the final pose estimation, model prediction is used instead of just one frame. Fast projective data association algorithm to obtain correspondences. Some outliers are filtered based on some features, e.g. the mask, the distance between model and scan, and the normal orientation difference. Then a ICP like method based on the point-plane metric is used for the final results, with the help of pyramids from coarse to fine framework. Finally, a stability and validity check, i.e null space of the normal system , and magnitude of incremental transform parameters makes only good tracking to update the model.","title":"KineticFusion"},{"location":"2018/2018%20Mar/#dvo","text":"The name is from NICP as dense visual odometry. Assuming a static scene, and the twist is constant, and the temporal derivative of the image is constant. Energy minimization or maximize photoconsistency on the image warps, which is like the unprojection into the image frame (only the 3D points part of the image). The final error function can be consider as intensity differences between two images, and results in a least square problem with 1x6 constraints and a 6x6 normal equation. A coarse to fine refinement is applied also.","title":"DVO"},{"location":"2018/2018%20Mar/#ndt","text":"It is designed for the matching in mine tunnel. First, subdivide the space occupied by the model into regularly sized cells (squares in 2D or cubes in 3D). Then for the cell containing more than some minimum number of points, the mean vector and covariance matrix are calculated. And use \\(N(\\mathbf{q},\\mathbf{C)}\\) to model the normal distribution. An error function is built based on the normal distribution with transformation \\(\\textbf{p}\\) . Then minimizing the error function by Newton's algorithm. For the 3D case, the main different is the Hessian and gradient for the transformation. It is derived in the paper. Then some practical implementations of 3D-NDT are discussed, e.g. the sampling method, the cell size and the discretization methods. ICP (iterative closest point) algorithm (Besl&McKay, 1992; Chen&Medioni, 1992)","title":"NDT"},{"location":"2018/2018%20Mar/#integral-images","text":"Surface normal from organized point cloud data using integral images. It is suitable for organized points. Smoothing upon different depth values, depth changes are utilized for the final smoothing area map. Two methods are proposed using the smoothed integral images. One is the smoothed depth changes (by cross product on two vectors based on the adjacent smoothed integral images) and the other is based on optimization (covariance matrices). This method is fast and robust for organized point cloud, but it could smooths over edges.","title":"Integral Images"},{"location":"2018/2018%20Mar/#orb-slam","text":"Orb-slam contains four major parts: tracking, mapping, relocalization, and loop closing . It is viewpoint and illumination invariant, and uses covisibility graph to make the tracking and mapping only related to the local covisible area. Essential graph is build from a spanning tree. Relocalization and initialization are discussed. Survival of the fittest approach is used to map point and keyframe selection, which improves tracking robustness and discards redundant keyframes. LM algorithm in g2o is used to carry out all optimizations. For each map point \\(p_i\\) , it stores its 3-D position \\(X_{w,i}\\) in the world coordinate system, the viewing direction, a representive ORB descriptor and maximum, minimum distances at which the point can be observed. For each keyframe \\(K_i\\) is stores the camera pose \\(T_{iw}\\) , from the world to the camera coordinate system, the camera intrinsics and all the ORB features extracted in the frame (associated or not with a map point, undistorted). The covisibility graph is a graph whose edges connect every two keyframes sharing observations of the same map points (at least 15). An essential graph is a strong network of cameras containing subset of edges from the covisibility graph with high covisibility ( \\(\\theta_{min} = 100\\) ), and the loop closure edges.","title":"ORB-SLAM"},{"location":"2018/2018%20Mar/#initialization","text":"Two methods are used in parallel. Extract ORB features with the finest scale. The one is the homography for the planar or low parallax cases; the other one is the fundamental matrix for the nonplanar or enough parallax cases. Finally a full BA is used to refine the initial reconstruction.","title":"Initialization"},{"location":"2018/2018%20Mar/#tracking","text":"For every frame from the camera, motion-only BA is applied. First ORB extraction using different parameters according to different pixels in the datasets. Then initial pose is estimated from previous frame using a constant velocity motion model (guided search). If this model is violated (not enough matches), a searching around map points positions in the last frame is applied (wider search). If the tracking is lost, global relocalization will work. PnP is used to find a camera pose (RANSAC). Tracking local map requires a local map containing the keyframes \\(K_1\\) with the same map points in current frame ( \\(K_{ref} \\in K_1\\) , which share the most map points) and \\(K_2\\) the neighbors with \\(K_1\\) . Discard the points lay out of the bounds; discard the points with too large distance between viewing directions and viewing rays; discard the points out of the scale invariance region; compute the scale and match unmatched features with the predicted scale. Keyframes: 20 frames (global relocalization and local mapping is idle or after the last keyframe insertion), 50 points, 90% points.","title":"Tracking"},{"location":"2018/2018%20Mar/#local-mapping","text":"For every new keyframe \\(K_i\\) . Insert new keyframe: update the covisibility graph, add a new node and edges, update the spanning tree, and the bags of words representation. Local points culling, trackable points are retained (25% visible, at least three keyframes). New map point creation, triangulating ORB from connected keyframes. Projected in the rest of connected keyframes. Local bundle adjustment (local BA) , currently processed keyframe, all the keyframes connected to it in the covisibility graph and all the map points seen by those keyframes. Local keyframe culling, detect redundant keyframes and delete them (90% in at least other three keyframes).","title":"Local Mapping"},{"location":"2018/2018%20Mar/#loop-cloosing","text":"The last keyframe \\(K_i\\) processed by the local mapping, and tries to detect and close loops. A normalizing score is used (discard the other keyframes with score lower than the lowest score of covisibility graph). It needs consecutively three loop candidates (keyframes). Similarity transformation, geometrical validation, 3-D-to-3-D correspondences for each loop candidate, RANSAC to find a similarity transformation. To fuse duplicated map points and insert new edges in the covisibility graph that will attach the loop closure. Pose graph optimization over the Essential Graph. Each map point is transformed according to the correction of one of the keyframes that observes it.","title":"Loop Cloosing"},{"location":"2018/2018%20Mar/#visual-inertial-monocular-slam-with-map-reuse","text":"Close loops in real-time and localize the sensor reusing the map in already mapped areas. (Others like odometry, marginalize past states or need a map, which is built offline) A reliable visual-inertial initialization \u2014 provides fixed states for optimization. Zero-drift localization in the same workspace. Expect to achievebetter accuracy and robustness by using stereo or RGB-D cameras. Weakness of the proposed IMU initialization is that it relies on the initialization of the monocular SLAM. Slow initialization comparing toVINS system (investigate the use of the gyroscope).","title":"Visual-Inertial Monocular SLAM With Map Reuse"},{"location":"2018/2018%20Mar/#some-ideas","text":"LOAM algorithm extracts lines (edges) and planes from single scan. Since single scan can provide such features within local information. At the same time, the provided line or plane features can help to calculate the error of lines or planes in the following scans. But these features may not be good for jobs related pure localization, since the features are just points with some limited information, like curvature or planarity. It is the problem about this method. @inproceedings { zhang2014loam, title= { LOAM: Lidar Odometry and Mapping in Real-time. } , author= { Zhang, Ji and Singh, Sanjiv } , booktitle= { Robotics: Science and Systems } , volume= { 2 } , year= { 2014 } }","title":"Some Ideas"},{"location":"2018/2018%20Mar/#todo","text":"If one wants to do place recognition, one thing is to find some common features that could appear in multiple places. @article { dube2016segmatch, title= { SegMatch: Segment based loop-closure for 3D point clouds } , author= { Dub { \\' e } , Renaud and Dugas, Daniel and Stumm, Elena and Nieto, Juan and Siegwart, Roland and Cadena, Cesar } , journal= { arXiv preprint arXiv:1609.07720 } , year= { 2016 } }","title":"TODO"},{"location":"2018/2018%20May/","text":"2018 May Reading Reports Motion Tracking with Fixed-lag Smoothing: Algorithm and Consistency Analysis Non-linear estimation as a MAP estimator. The term \\(c_m\\) has been permanently approximated by the second-order Taylor series approximation of \\(c_m \\approx c_m(\\hat{x}_r(k), \\hat{x}_m(k))\\) . Discard after the approximation, then we can have constant-time and constant-memory. It analyze the null space of \\(A\\) to find out the change of rank(A). Prior-Linearization (PL) fixed-lag smoothing Direct Sparse Odometry Direct: no feature extraction, Sparse: not full image It is a sparse and direct approach, involving the model parameters and geometry parameters (inverse depth values). All of the parameters are jointly optimized.E $$ E_{photo} + E_{prior} $$ The parameters the algorithm estimates: (1) the point\u2019s inverse depth \\(d_{\\mathbf{p}}\\) , (2) the camera intrinsics \\(\\mathbf{c}\\) , (3) the poses of the involved frames \\(\\mathbf{T}_i\\) , \\(\\mathbf{T}_j\\) , and (4) their brightness transfer function parameters \\(a_i\\) , \\(b_i\\) , \\(a_j\\) , \\(b_j\\) . Marginalization is applied using \"First Estimate Jacobians\". The front-end is also using direct method, including frame management and point management. Parameter study indicates that, (1) simply using more data does not increase tracking accuracy (although it makes the 3D models denser), (2) using all points instead of only corners does provide a real gain in accuracy and robustness, and (3) incorporating photometric calibration does increase performance, in particular compared to the basic \u201cbrightness constancy\u201d assumption. Indirect method is robust to geometric noise, while direct approach is more robust to photometric noise. SE(2)-Constrained SE(3) Poses Keyframe poses and map point positions are optimized with four types of edges: 1) image feature-based constraints; 2) odometry-based constraints on camera poses; 3) SE(2) planar motion constraints on camera poses; and 4) co-visible map points-based constraints.","title":"2018 May Reading Reports"},{"location":"2018/2018%20May/#2018-may-reading-reports","text":"","title":"2018 May Reading Reports"},{"location":"2018/2018%20May/#motion-tracking-with-fixed-lag-smoothing-algorithm-and-consistency-analysis","text":"Non-linear estimation as a MAP estimator. The term \\(c_m\\) has been permanently approximated by the second-order Taylor series approximation of \\(c_m \\approx c_m(\\hat{x}_r(k), \\hat{x}_m(k))\\) . Discard after the approximation, then we can have constant-time and constant-memory. It analyze the null space of \\(A\\) to find out the change of rank(A). Prior-Linearization (PL) fixed-lag smoothing","title":"Motion Tracking with Fixed-lag Smoothing: Algorithm and Consistency Analysis"},{"location":"2018/2018%20May/#direct-sparse-odometry","text":"Direct: no feature extraction, Sparse: not full image It is a sparse and direct approach, involving the model parameters and geometry parameters (inverse depth values). All of the parameters are jointly optimized.E $$ E_{photo} + E_{prior} $$ The parameters the algorithm estimates: (1) the point\u2019s inverse depth \\(d_{\\mathbf{p}}\\) , (2) the camera intrinsics \\(\\mathbf{c}\\) , (3) the poses of the involved frames \\(\\mathbf{T}_i\\) , \\(\\mathbf{T}_j\\) , and (4) their brightness transfer function parameters \\(a_i\\) , \\(b_i\\) , \\(a_j\\) , \\(b_j\\) . Marginalization is applied using \"First Estimate Jacobians\". The front-end is also using direct method, including frame management and point management. Parameter study indicates that, (1) simply using more data does not increase tracking accuracy (although it makes the 3D models denser), (2) using all points instead of only corners does provide a real gain in accuracy and robustness, and (3) incorporating photometric calibration does increase performance, in particular compared to the basic \u201cbrightness constancy\u201d assumption. Indirect method is robust to geometric noise, while direct approach is more robust to photometric noise.","title":"Direct Sparse Odometry"},{"location":"2018/2018%20May/#se2-constrained-se3-poses","text":"Keyframe poses and map point positions are optimized with four types of edges: 1) image feature-based constraints; 2) odometry-based constraints on camera poses; 3) SE(2) planar motion constraints on camera poses; and 4) co-visible map points-based constraints.","title":"SE(2)-Constrained SE(3) Poses"},{"location":"2018/2018%20Nov/","text":"2018 November Reading Reports On Degeneracy of Optimization-based State Estimation Problems The problem is only solved in well-conditioned directions, and the best guess is used in degenerate directions. The method checks the rows in \\(||Ax - b||\\) . Degeneracy of a linearized system is defined by \\((\\mathbf{c}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{c} + 1)^{-1}\\) , which can be solved by $$ \\text{arg}\\min_\\mathbf{c}\\frac{\\mathbf{c} T\\mathbf{A} T\\mathbf{A}\\mathbf{c}}{\\mathbf{c}^T\\mathbf{c}} $$ with the minimum eigenvalue of \\(\\mathbf{A}^T\\mathbf{A}\\) . Solution remapping is used to handle degeneracy \\(x_f \\leftarrow \\mathbf{V}_f^{-1} \\mathbf{V}_u \\Delta x_{u}\\) , or \\(x_f \\leftarrow \\mathbf{V}^T_u \\mathbf{V}_f^{-T} \\Delta x_{u}\\) (in my implementation \\(\\mathbf{V}^T\\) is the matrix of eigenvectors). Single View Point Omnidirectional Camera Calibration from Planar Grids A method to calibrate sphere projection model is proposed (MEI). The model can used for many kinds of camera with catadioptric lenses. The camera and lens are modeled together. It projects the points in \\(\\mathbb{R}^3\\) to a unit sphere and then projects from another point (can be different from the origin point) to an image plane. Colourising Point Clouds Using Independent Cameras This letter first synchronizes the mapping device data and camera data using optical \ufb02ow information (yaw rate, cross-correlation). Visible analysis based on exponential kernel function is introduced, followed by memory-efficient coloring (mean of weighted Gaussian distribution). Result dataset 3D LIDAR\u2013camera intrinsic and extrinsic calibration: Identi\ufb01ability and analytical least-squares-based initialization This paper introduces a method use calibration plane. It assumes the normal direction is known, and solve a problem in the form of \\[\\alpha_i(\\rho_{ijk}+ \\rho_{oi}) {}^C\\bar{n}^T_j {}^C_{Li}C\\bar{p}_{ijk} + {}^C\\bar{n}^T_j {}^C_{Li}t - d_{ijk}\\] where \\(\\alpha_i,\\rho_{oi},{}^C_{Li}C,{}^C_{Li}t\\) are the parameters to be estimated, others are the measurements containing errors (the plane coefficients are provide by stereo system). At the beginning, it is solve in a two-step linear estimation, first \\(\\rho_{oi},{}^C_{Li}C\\) (polynomial system solver is used, the multiplication matrix, monomial, Gr\u00f6bner basis, ideal etc.), then \\(\\alpha_i,{}^C_{Li}t\\) . Then a non-linear optimization iteratively uses a minimal parameterization of the unknowns with some of the initial values in the linear solver part. Identi\ufb01ability conditions are also discussed in the paper. It shows that three planes with linearly independent normal vectors are observed, we can determine all of the unknowns. Intrinsic and extrinsic parameters are estimated. The evaluation can be referred. REMODE: Probabilistic, Monocular Dense Reconstruction in Real Time A probabilistic depth map containing Gamma and Gaussian. A fast smoothing method smoothes the noisy depth. Link Video-based, real-time multi-view stereo The probabilistic model with Gamma and Gaussian is proposed as the same as the one in REMODE. It is in the form of $$ q(Z;\\pi|a_n;b_n;\\mu_n;\\sigma_n) : = Beta(\\pi|a_n,b_n)N(Z|\\mu_n;\\sigma^2_n) $$ It is updated with the new measurement with mean and second moment approximation (ref: supplementary material and PRML 10.1.1). Then pruning of seeds is used to keep seeds as constant. Voxblox It uses Truncated Signed Distance Fields (TSDFs) to build Euclidean Signed Distance Fields (ESDFs). For the TSDF, it uses a behind-surface drop-off and grouped raycasting. For building ESDF, it uses the distance stored in the TSDF map (wavefronts, waves that propagate from a start voxel to its neighbors, priority queue, quasi-Euclidean Distance ). The error of ESDF is discussed to be used to inflate the model. Real-Time Camera Tracking and 3D Reconstruction Using Signed Distance Functions Similar with KinectFusion, the main difference is on the camera pose tracking, which uses the gradient from SDF instead of ICP. Integrating Deep Semantic Segmentation Into 3-D Point Cloud Registration Integrate semantic information to the point cloud registration (GICP and NDT). Camera and LIDAR Fusion for Mapping of Actively Illuminated Subterranean Voids It uses Markov Random Field (MRF) [1] among illuminated HDR image and lidar points. The polar estimates ( \\(\\theta_{nl}\\) , from corrected illumination intensity \\(E_{unbiased}\\) and the the albedo values \\(\\rho_{est}\\) ) are combined with azimuth estimates ( \\(\\phi\\) ) from the range image and converted to gradients for integration in the MRF. In my opinion, it combines the surface normal and light direction to get the gradient direction, which can be used in the MRF. It focuses on a continuously rotating planar LIDAR scanner and an (iluminated HDR) 8 megapixel DSLR camera from a single view point (or odometry/ICP with evenly spaced intervals roughly 3 meters apart). The results are displayed using a hole-filling method. [1] Diebel, J., & Thrun, S. (2006). An application of markov random fields to range sensing. In Advances in neural information processing systems (pp. 291-298). An Application of Markov Random Fields to Range Sensing This paper applies graphical models to the problem of fusing low-res depth images with high-res camera images. It increases the resolution of our depth sensor by an order of magnitude while improves local accuracy . Two layers depth measurement potential and depth smoothness prior are used to form \\(p(y | x, z)\\) . The weights are based on smoothing occur across edges in the image. The gradient (CG) algorithms are used to iteratively solve the mode of the posterior from bilinear interpolation of \\(z\u200b\\) (laser range measurement).","title":"2018 November Reading Reports"},{"location":"2018/2018%20Nov/#2018-november-reading-reports","text":"","title":"2018 November Reading Reports"},{"location":"2018/2018%20Nov/#on-degeneracy-of-optimization-based-state-estimation-problems","text":"The problem is only solved in well-conditioned directions, and the best guess is used in degenerate directions. The method checks the rows in \\(||Ax - b||\\) . Degeneracy of a linearized system is defined by \\((\\mathbf{c}^T\\mathbf{A}^T\\mathbf{A}\\mathbf{c} + 1)^{-1}\\) , which can be solved by $$ \\text{arg}\\min_\\mathbf{c}\\frac{\\mathbf{c} T\\mathbf{A} T\\mathbf{A}\\mathbf{c}}{\\mathbf{c}^T\\mathbf{c}} $$ with the minimum eigenvalue of \\(\\mathbf{A}^T\\mathbf{A}\\) . Solution remapping is used to handle degeneracy \\(x_f \\leftarrow \\mathbf{V}_f^{-1} \\mathbf{V}_u \\Delta x_{u}\\) , or \\(x_f \\leftarrow \\mathbf{V}^T_u \\mathbf{V}_f^{-T} \\Delta x_{u}\\) (in my implementation \\(\\mathbf{V}^T\\) is the matrix of eigenvectors).","title":"On Degeneracy of Optimization-based State Estimation Problems"},{"location":"2018/2018%20Nov/#single-view-point-omnidirectional-camera-calibration-from-planar-grids","text":"A method to calibrate sphere projection model is proposed (MEI). The model can used for many kinds of camera with catadioptric lenses. The camera and lens are modeled together. It projects the points in \\(\\mathbb{R}^3\\) to a unit sphere and then projects from another point (can be different from the origin point) to an image plane.","title":"Single View Point Omnidirectional Camera Calibration from Planar Grids"},{"location":"2018/2018%20Nov/#colourising-point-clouds-using-independent-cameras","text":"This letter first synchronizes the mapping device data and camera data using optical \ufb02ow information (yaw rate, cross-correlation). Visible analysis based on exponential kernel function is introduced, followed by memory-efficient coloring (mean of weighted Gaussian distribution). Result dataset","title":"Colourising Point Clouds Using Independent Cameras"},{"location":"2018/2018%20Nov/#3d-lidarcamera-intrinsic-and-extrinsic-calibration-identifiability-and-analytical-least-squares-based-initialization","text":"This paper introduces a method use calibration plane. It assumes the normal direction is known, and solve a problem in the form of \\[\\alpha_i(\\rho_{ijk}+ \\rho_{oi}) {}^C\\bar{n}^T_j {}^C_{Li}C\\bar{p}_{ijk} + {}^C\\bar{n}^T_j {}^C_{Li}t - d_{ijk}\\] where \\(\\alpha_i,\\rho_{oi},{}^C_{Li}C,{}^C_{Li}t\\) are the parameters to be estimated, others are the measurements containing errors (the plane coefficients are provide by stereo system). At the beginning, it is solve in a two-step linear estimation, first \\(\\rho_{oi},{}^C_{Li}C\\) (polynomial system solver is used, the multiplication matrix, monomial, Gr\u00f6bner basis, ideal etc.), then \\(\\alpha_i,{}^C_{Li}t\\) . Then a non-linear optimization iteratively uses a minimal parameterization of the unknowns with some of the initial values in the linear solver part. Identi\ufb01ability conditions are also discussed in the paper. It shows that three planes with linearly independent normal vectors are observed, we can determine all of the unknowns. Intrinsic and extrinsic parameters are estimated. The evaluation can be referred.","title":"3D LIDAR\u2013camera intrinsic and extrinsic calibration: Identi\ufb01ability and analytical least-squares-based initialization"},{"location":"2018/2018%20Nov/#remode-probabilistic-monocular-dense-reconstruction-in-real-time","text":"A probabilistic depth map containing Gamma and Gaussian. A fast smoothing method smoothes the noisy depth. Link","title":"REMODE: Probabilistic, Monocular Dense Reconstruction in Real Time"},{"location":"2018/2018%20Nov/#video-based-real-time-multi-view-stereo","text":"The probabilistic model with Gamma and Gaussian is proposed as the same as the one in REMODE. It is in the form of $$ q(Z;\\pi|a_n;b_n;\\mu_n;\\sigma_n) : = Beta(\\pi|a_n,b_n)N(Z|\\mu_n;\\sigma^2_n) $$ It is updated with the new measurement with mean and second moment approximation (ref: supplementary material and PRML 10.1.1). Then pruning of seeds is used to keep seeds as constant.","title":"Video-based, real-time multi-view stereo"},{"location":"2018/2018%20Nov/#voxblox","text":"It uses Truncated Signed Distance Fields (TSDFs) to build Euclidean Signed Distance Fields (ESDFs). For the TSDF, it uses a behind-surface drop-off and grouped raycasting. For building ESDF, it uses the distance stored in the TSDF map (wavefronts, waves that propagate from a start voxel to its neighbors, priority queue, quasi-Euclidean Distance ). The error of ESDF is discussed to be used to inflate the model.","title":"Voxblox"},{"location":"2018/2018%20Nov/#real-time-camera-tracking-and-3d-reconstruction-using-signed-distance-functions","text":"Similar with KinectFusion, the main difference is on the camera pose tracking, which uses the gradient from SDF instead of ICP.","title":"Real-Time Camera Tracking and 3D Reconstruction Using Signed Distance Functions"},{"location":"2018/2018%20Nov/#integrating-deep-semantic-segmentation-into-3-d-point-cloud-registration","text":"Integrate semantic information to the point cloud registration (GICP and NDT).","title":"Integrating Deep Semantic Segmentation Into 3-D Point Cloud Registration"},{"location":"2018/2018%20Nov/#camera-and-lidar-fusion-for-mapping-of-actively-illuminated-subterranean-voids","text":"It uses Markov Random Field (MRF) [1] among illuminated HDR image and lidar points. The polar estimates ( \\(\\theta_{nl}\\) , from corrected illumination intensity \\(E_{unbiased}\\) and the the albedo values \\(\\rho_{est}\\) ) are combined with azimuth estimates ( \\(\\phi\\) ) from the range image and converted to gradients for integration in the MRF. In my opinion, it combines the surface normal and light direction to get the gradient direction, which can be used in the MRF. It focuses on a continuously rotating planar LIDAR scanner and an (iluminated HDR) 8 megapixel DSLR camera from a single view point (or odometry/ICP with evenly spaced intervals roughly 3 meters apart). The results are displayed using a hole-filling method. [1] Diebel, J., & Thrun, S. (2006). An application of markov random fields to range sensing. In Advances in neural information processing systems (pp. 291-298).","title":"Camera and LIDAR Fusion for Mapping of Actively Illuminated Subterranean Voids"},{"location":"2018/2018%20Nov/#an-application-of-markov-random-fields-to-range-sensing","text":"This paper applies graphical models to the problem of fusing low-res depth images with high-res camera images. It increases the resolution of our depth sensor by an order of magnitude while improves local accuracy . Two layers depth measurement potential and depth smoothness prior are used to form \\(p(y | x, z)\\) . The weights are based on smoothing occur across edges in the image. The gradient (CG) algorithms are used to iteratively solve the mode of the posterior from bilinear interpolation of \\(z\u200b\\) (laser range measurement).","title":"An Application of Markov Random Fields to Range Sensing"},{"location":"2018/2018%20Oct/","text":"2018 October Reading Reports Colored Point Cloud Registration Revisited The approach establishes correspondences in the physical three-dimensional space, but defines a joint optimization objective that integrates both geometric and photometric terms. It locks the alignment both along the tangent plane and (photometric term) the normal direction (geometric term). RGB-D Image Alignment Two objectives are introduced first, the photometric objective \\(E_I\\) and geometric objective \\(E_G\\) . \\[ E_I(\\mathbf{T}) = \\sum_{\\mathbf{x}} (I_i(\\mathbf{x}')-I_j(\\mathbf{x}))^2 \\\\ \\mathbf{x}' = \\mathbf{g}_{uv}(\\mathbf{s}(\\mathbf{h}(\\mathbf{x},D_j(\\mathbf{x})), \\mathbf{T})) \\\\ E_D(\\mathbf{T}) = \\sum_{\\mathbf{x}}(D_i(\\mathbf{x'})-\\mathbf{g}_d(\\mathbf{s}(\\mathbf{h}(\\mathbf{x},D_j(\\mathbf{x})),\\mathbf{T})))^2 \\] Parametrization For each colored point \\(\\mathbf{p} \\in \\mathbf{P}\\) , the virtual orthogonal camera can provide two approximations of functions: $$ C_{\\mathbf{p}}(\\mathbf{u}) \\approx C(\\mathbf{p}) + \\mathbf{d_p}^\\top\\mathbf{u} \\ G_{\\mathbf{p}}\\approx(\\mathbf{o_p}-\\mathbf{p})^\\top\\mathbf{n_p} $$ where \\(\\mathbf{d_p}\\) is solved by least square fitting (constrained by the projection of the neighbor points and the normal direction), and the latter one is a constant function. The color parametrization requires to be pre-computed . Objective The final objective is \\(E(\\mathbf{T}) = (1-\\sigma)E_C(\\mathbf{T})+\\sigma E_G(\\mathbf{T})\\) , where \\(E_C\\) includes \\(C(\\mathbf{q})\\) and \\(C_{\\mathbf{p}}(\\mathbf{q}')\\) , \\(\\mathbf{q}'\\) on the tangent plane of \\(\\mathbf{p}\\) , and \\(E_G\\) includes the transformed \\(\\mathbf{q}\\) , \\(\\mathbf{p}\\) and \\(\\mathbf{n_p}\\) (like point-to-plane distance, let \\(\\mathbf{q}\\) to be on the same plane where \\(\\mathbf{p}\\) is). The optimization is coarse-to-fine (point cloud pyramid) with Gauss-Newton method. StructVIO It includes line representation , which integrates Atlanta world representation and structural features, structural line parameterization to recognize the local Manhattan world and vertical lines to help estimate orientation, several improvements on the estimator and line tracking , e.g. a novel marginalization method for long feature tracks and a line tracking method by sampling multiple points and delayed EKF update. LSD line detector is used. The main pipeline is under EKF VIO (MSCKF). The main idea is to track the line and estimate the line parameters in the local frames. Residual are formed by the line residual \\(r_k\u200b\\) , which is related to the dot product of homogeneous coordinates of points and the predicted lines, and line prior (like EKF). Information Sparsification in Visual-Inertial Odometry Instead of marginalization, it uses sparsification by minimizing the KL divergence. Sparse Iterative Closest Point Link and Github It formulates the registration optimization using sparsity inducing norms, which can deal with outliers and incomplete data. It formulates the local alignment problem as recovering a rigid transformation that maximizes the number of zero distances between correspondences ( \\(\\mathcal{l}_p\\) norms, where \\(p \\in [0, 1]\\) ). Alternating Direction Method of Multipliers (ADMM) is applied. Group sparsity vanish residual vector's \\(x, y, z\\) simultaneously. This paper doesn't use the reweight approximation for the \\(\\mathcal{l}_p\\) norm, while it introduces a new set of variables \\(Z\\) and augmented Lagrangian function, which can be solved by ADMM. The ADMM decomposes the problem into three simple steps. ADMM is faster when using point-to-plane distance while not using point-to-point distance. RaD-VIO Not an interesting one. It uses planar assumption and homography constraints to estimate unscaled translation and use IMU orientation directly. The EKF is used to combine the range finder data to estimate the scale. CNN for IMU Assisted Odometry Estimation using Velodyne LiDAR Not an interesting one. It uses three channels' images from LiDAR as inputs and trains the model separately for translations and rotations.","title":"2018 October Reading Reports"},{"location":"2018/2018%20Oct/#2018-october-reading-reports","text":"","title":"2018 October Reading Reports"},{"location":"2018/2018%20Oct/#colored-point-cloud-registration-revisited","text":"The approach establishes correspondences in the physical three-dimensional space, but defines a joint optimization objective that integrates both geometric and photometric terms. It locks the alignment both along the tangent plane and (photometric term) the normal direction (geometric term).","title":"Colored Point Cloud Registration Revisited"},{"location":"2018/2018%20Oct/#rgb-d-image-alignment","text":"Two objectives are introduced first, the photometric objective \\(E_I\\) and geometric objective \\(E_G\\) . \\[ E_I(\\mathbf{T}) = \\sum_{\\mathbf{x}} (I_i(\\mathbf{x}')-I_j(\\mathbf{x}))^2 \\\\ \\mathbf{x}' = \\mathbf{g}_{uv}(\\mathbf{s}(\\mathbf{h}(\\mathbf{x},D_j(\\mathbf{x})), \\mathbf{T})) \\\\ E_D(\\mathbf{T}) = \\sum_{\\mathbf{x}}(D_i(\\mathbf{x'})-\\mathbf{g}_d(\\mathbf{s}(\\mathbf{h}(\\mathbf{x},D_j(\\mathbf{x})),\\mathbf{T})))^2 \\]","title":"RGB-D Image Alignment"},{"location":"2018/2018%20Oct/#parametrization","text":"For each colored point \\(\\mathbf{p} \\in \\mathbf{P}\\) , the virtual orthogonal camera can provide two approximations of functions: $$ C_{\\mathbf{p}}(\\mathbf{u}) \\approx C(\\mathbf{p}) + \\mathbf{d_p}^\\top\\mathbf{u} \\ G_{\\mathbf{p}}\\approx(\\mathbf{o_p}-\\mathbf{p})^\\top\\mathbf{n_p} $$ where \\(\\mathbf{d_p}\\) is solved by least square fitting (constrained by the projection of the neighbor points and the normal direction), and the latter one is a constant function. The color parametrization requires to be pre-computed .","title":"Parametrization"},{"location":"2018/2018%20Oct/#objective","text":"The final objective is \\(E(\\mathbf{T}) = (1-\\sigma)E_C(\\mathbf{T})+\\sigma E_G(\\mathbf{T})\\) , where \\(E_C\\) includes \\(C(\\mathbf{q})\\) and \\(C_{\\mathbf{p}}(\\mathbf{q}')\\) , \\(\\mathbf{q}'\\) on the tangent plane of \\(\\mathbf{p}\\) , and \\(E_G\\) includes the transformed \\(\\mathbf{q}\\) , \\(\\mathbf{p}\\) and \\(\\mathbf{n_p}\\) (like point-to-plane distance, let \\(\\mathbf{q}\\) to be on the same plane where \\(\\mathbf{p}\\) is). The optimization is coarse-to-fine (point cloud pyramid) with Gauss-Newton method.","title":"Objective"},{"location":"2018/2018%20Oct/#structvio","text":"It includes line representation , which integrates Atlanta world representation and structural features, structural line parameterization to recognize the local Manhattan world and vertical lines to help estimate orientation, several improvements on the estimator and line tracking , e.g. a novel marginalization method for long feature tracks and a line tracking method by sampling multiple points and delayed EKF update. LSD line detector is used. The main pipeline is under EKF VIO (MSCKF). The main idea is to track the line and estimate the line parameters in the local frames. Residual are formed by the line residual \\(r_k\u200b\\) , which is related to the dot product of homogeneous coordinates of points and the predicted lines, and line prior (like EKF).","title":"StructVIO"},{"location":"2018/2018%20Oct/#information-sparsification-in-visual-inertial-odometry","text":"Instead of marginalization, it uses sparsification by minimizing the KL divergence.","title":"Information Sparsification in Visual-Inertial Odometry"},{"location":"2018/2018%20Oct/#sparse-iterative-closest-point","text":"Link and Github It formulates the registration optimization using sparsity inducing norms, which can deal with outliers and incomplete data. It formulates the local alignment problem as recovering a rigid transformation that maximizes the number of zero distances between correspondences ( \\(\\mathcal{l}_p\\) norms, where \\(p \\in [0, 1]\\) ). Alternating Direction Method of Multipliers (ADMM) is applied. Group sparsity vanish residual vector's \\(x, y, z\\) simultaneously. This paper doesn't use the reweight approximation for the \\(\\mathcal{l}_p\\) norm, while it introduces a new set of variables \\(Z\\) and augmented Lagrangian function, which can be solved by ADMM. The ADMM decomposes the problem into three simple steps. ADMM is faster when using point-to-plane distance while not using point-to-point distance.","title":"Sparse Iterative Closest Point"},{"location":"2018/2018%20Oct/#rad-vio","text":"Not an interesting one. It uses planar assumption and homography constraints to estimate unscaled translation and use IMU orientation directly. The EKF is used to combine the range finder data to estimate the scale.","title":"RaD-VIO"},{"location":"2018/2018%20Oct/#cnn-for-imu-assisted-odometry-estimation-using-velodyne-lidar","text":"Not an interesting one. It uses three channels' images from LiDAR as inputs and trains the model separately for translations and rotations.","title":"CNN for IMU Assisted Odometry Estimation using Velodyne LiDAR"},{"location":"2018/2018%20Sep/","text":"2018 September Reading Reports CodeSLAM It introduced a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code. For mapping, photometric error and geometric error are related not only the pixel position and the pose of the camera, but also the codes , which will be optimized. For tracking or localization, it uses geometric cost only and with coarse to fine optimization. The SLAM system is inspired by PTAM. The overall results are not as good as VI method, but performs respectably for a vision only system. Learning Compact Geometric Feature Local features in point cloud is learned in this work. First, it parameterizes the points within the sphere by spherical histogram (binned in \\(N = R \\times E \\times A\\) ), which results in the feature \\(\\mathbb{R}^N\\) . Triple embedding loss is used to train the model mapping the high-dimensional spherical histogram's space to a very low-dimensional Euclidean space. The training triples are generated by two concentric spheres, which are used to determine the positive and difficult negative samples of points (in training, negative can also be from an entire model). Synthesizing depth images are used to automatically generate enough training data. Correspondences are found by kd-tree. It can serve as drop-in replacements for existing descriptors. The transformation is given in some of the evaluations. Pipeline of fast global registration (FGR, original with FPFH) is adapted with the proposed method to register the points. Robust optimization can prune false positives. Fast Global Registration It does not require initialization and can align noisy partially overlapping surfaces at a computational cost that is more than an order of magnitude lower than the global alignment + refinement framework. It can also jointly align multiple partially overlapping surfaces directly by a single optimization of a global objective. Related Work Typically workflow consists of two stages: global alignment, followed by local refinement. The global alignment includes the methods operate on candidate correspondences (RANSAC, pose clustering, etc., samples), the branch-and-bound framework (Go-ICP). The local refinement is usually done by ICP and its variants. Objective A scaled Geman-McClure estimator: $$ \\rho(x) = \\frac{\\mu x^2}{\\mu + x^2} $$ can be solved by Black-Rangarajan duality. It can be optimized by alternating between \\(\\mathbf{T}\\) and \\(\\mathbb{L}\\) (iteration needed, but very efficient). Reciprocity test and tuple test are used to improve the inlier ratio. Multi-way registration is similar with the original version, added backbone odometry terms to stabilize the optimization. Correspondences are not changed in the optimization. Robust Reconstruction of Indoor Scenes It presents a dedicated formulation for dense surface reconstruction that identifies outliers by directly optimizing for surface alignment, using an objective that efficiently incorporates dense correspondence constraints. Fragment is constructed by visual odometry, geometric registration for each pair is run for find loops, robust optimization is used to handle with the outliers, and final model is refined using ICP and volumetric integration. Geometric registration determine the loops, and robust optimization with line process is used to solve the objective function. Proximity of correspondence pairs is used to simplify the objective. And finally becomes a quadratic form, which can be solved by g2o . BRE (???) is used to evaluate the results (Amazon Mechanical Turk). Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving Ego-motion and car motion are estimated in the work. It mainly uses non-linear optimization (BA) for both of the estimation. Learning-based semantic labelling (car detection), 2D-3D car representation (classification in 2*8 = 16 viewpoints), feature tracking (ORB for temporal matching, the center distance and shape similarity of the 2D boxes) and car kinematics model are used. Further alignment with stereo point-cloud and the estimated points is carried out. LIPS: LiDAR-Inertial 3D Plane SLAM Closest Point Plane Representation It is based on plane representation on \\(\\mathbf{n}, d\\) . The optimization includes the plane representations and the states. In real tests, RANSAC plane extraction is needed, the small-scale tests show the proof-of-concept only. Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera Three kinds of loss are designed to make sparse point-cloud into dense point-cloud, depth loss, photometric loss and smooth loss.","title":"2018 September Reading Reports"},{"location":"2018/2018%20Sep/#2018-september-reading-reports","text":"","title":"2018 September Reading Reports"},{"location":"2018/2018%20Sep/#codeslam","text":"It introduced a new compact but dense representation of scene geometry which is conditioned on the intensity data from a single image and generated from a code. For mapping, photometric error and geometric error are related not only the pixel position and the pose of the camera, but also the codes , which will be optimized. For tracking or localization, it uses geometric cost only and with coarse to fine optimization. The SLAM system is inspired by PTAM. The overall results are not as good as VI method, but performs respectably for a vision only system.","title":"CodeSLAM"},{"location":"2018/2018%20Sep/#learning-compact-geometric-feature","text":"Local features in point cloud is learned in this work. First, it parameterizes the points within the sphere by spherical histogram (binned in \\(N = R \\times E \\times A\\) ), which results in the feature \\(\\mathbb{R}^N\\) . Triple embedding loss is used to train the model mapping the high-dimensional spherical histogram's space to a very low-dimensional Euclidean space. The training triples are generated by two concentric spheres, which are used to determine the positive and difficult negative samples of points (in training, negative can also be from an entire model). Synthesizing depth images are used to automatically generate enough training data. Correspondences are found by kd-tree. It can serve as drop-in replacements for existing descriptors. The transformation is given in some of the evaluations. Pipeline of fast global registration (FGR, original with FPFH) is adapted with the proposed method to register the points. Robust optimization can prune false positives.","title":"Learning Compact Geometric Feature"},{"location":"2018/2018%20Sep/#fast-global-registration","text":"It does not require initialization and can align noisy partially overlapping surfaces at a computational cost that is more than an order of magnitude lower than the global alignment + refinement framework. It can also jointly align multiple partially overlapping surfaces directly by a single optimization of a global objective.","title":"Fast Global Registration"},{"location":"2018/2018%20Sep/#related-work","text":"Typically workflow consists of two stages: global alignment, followed by local refinement. The global alignment includes the methods operate on candidate correspondences (RANSAC, pose clustering, etc., samples), the branch-and-bound framework (Go-ICP). The local refinement is usually done by ICP and its variants.","title":"Related Work"},{"location":"2018/2018%20Sep/#objective","text":"A scaled Geman-McClure estimator: $$ \\rho(x) = \\frac{\\mu x^2}{\\mu + x^2} $$ can be solved by Black-Rangarajan duality. It can be optimized by alternating between \\(\\mathbf{T}\\) and \\(\\mathbb{L}\\) (iteration needed, but very efficient). Reciprocity test and tuple test are used to improve the inlier ratio. Multi-way registration is similar with the original version, added backbone odometry terms to stabilize the optimization. Correspondences are not changed in the optimization.","title":"Objective"},{"location":"2018/2018%20Sep/#robust-reconstruction-of-indoor-scenes","text":"It presents a dedicated formulation for dense surface reconstruction that identifies outliers by directly optimizing for surface alignment, using an objective that efficiently incorporates dense correspondence constraints. Fragment is constructed by visual odometry, geometric registration for each pair is run for find loops, robust optimization is used to handle with the outliers, and final model is refined using ICP and volumetric integration. Geometric registration determine the loops, and robust optimization with line process is used to solve the objective function. Proximity of correspondence pairs is used to simplify the objective. And finally becomes a quadratic form, which can be solved by g2o . BRE (???) is used to evaluate the results (Amazon Mechanical Turk).","title":"Robust Reconstruction of Indoor Scenes"},{"location":"2018/2018%20Sep/#stereo-vision-based-semantic-3d-object-and-ego-motion-tracking-for-autonomous-driving","text":"Ego-motion and car motion are estimated in the work. It mainly uses non-linear optimization (BA) for both of the estimation. Learning-based semantic labelling (car detection), 2D-3D car representation (classification in 2*8 = 16 viewpoints), feature tracking (ORB for temporal matching, the center distance and shape similarity of the 2D boxes) and car kinematics model are used. Further alignment with stereo point-cloud and the estimated points is carried out.","title":"Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for Autonomous Driving"},{"location":"2018/2018%20Sep/#lips-lidar-inertial-3d-plane-slam","text":"","title":"LIPS: LiDAR-Inertial 3D Plane SLAM"},{"location":"2018/2018%20Sep/#closest-point-plane-representation","text":"It is based on plane representation on \\(\\mathbf{n}, d\\) . The optimization includes the plane representations and the states. In real tests, RANSAC plane extraction is needed, the small-scale tests show the proof-of-concept only.","title":"Closest Point Plane Representation"},{"location":"2018/2018%20Sep/#self-supervised-sparse-to-dense-self-supervised-depth-completion-from-lidar-and-monocular-camera","text":"Three kinds of loss are designed to make sparse point-cloud into dense point-cloud, depth loss, photometric loss and smooth loss.","title":"Self-Supervised Sparse-to-Dense: Self-Supervised Depth Completion from LiDAR and Monocular Camera"},{"location":"2019/2019%20Apr/","text":"2019 April Reading Reports Canny-VO: Visual Odometry With RGB-D Cameras Based on Geometric 3-D\u20132-D Edge Alignment This paper solves three problems in edge-based VO: Discontinuity of the objective function. The potential biases in model-to-data paradigm. The robustness of the method, related to the reweighting formulation. To solve the first problem, it uses ICP-based estimation, but with different nearest neighbor finding method. It applies Approximate Nearest Neighbor Fields (ANNF) to make a lookup table in the current image. Similar to point-to-plane ICP, point-to-tangent residuals are used, which are formed by warped original (model) gradient vector . The second problem is solved by introducing Oriented Nearest Neighbor Fields (ONNF), which divides the orientation into eight distance fields. The orientations of the points in model are calculated with their poses and compared with relevant orientation bins. Then the gradient directions are approximated by the center of the corresponding orientations. To estimate robustly, the authors uses probabilistic sensor model as [6] and point culling by discarding the points with large geometric residuals than the median of the overall residuals. [6] C. Kerl, J. Sturm, and D. Cremers, \u201cRobust odometry estimation for RGBD cameras,\u201d in Proc. IEEE Int. Conf. Robot. Autom., 2013, pp. 3748\u20133754. Fast and Accurate Point Cloud Registration using Trees of Gaussian Mixtures This paper mainly uses the structure of trees of Gaussian mixture model to make registration as a expectation maximization problem. The E step is the adaptive tree search, which corresponds to the data association problem. It efficiently associates the points to the suitable tree level in the tree of GMM model. The M step is the proposed Mahalanobis estimation given the \\(N \\times J\\) set of point-cluster correspondences \\(\\mathcal{C} = \\{ c_{ij} \\}\\) , which is rewritten by PCA decomposition in to a mixed MLE criterion. Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation This paper combines the accurate intermediate geometric representations of traditional monocular SLAM with self-supervised depth estimation to deliver a better formulation for joint depth and motion estimation. The loss function consists of photometric loss (with percentile mask), depth smooth loss, geometric loss (by pairwise matching to form epipolar constraints, a set of feature matches \\(\\{(p_i , q_i)\\}\\) in the homogeneous image coordinates can be reliably obtained), pose loss (for comparison only, not in the proposed method). Learning Less is More - 6D Camera Localization via 3D Surface Regression This is an improved version for DSAC [1]. It can be trained from RGB images and ground truth poses only, without using the 3D scene model. It contains four parts in DSAC: 1. Scene Coordinate Regression , 2. Pose Hypothesis Sampling , 3. Hypothesis Selection , 4. Hypothesis Refinement . In this paper, 1. is replaced by fully convolutional network (FCN) instead of sampling. 3. is replaced by Soft Inlier Counting and Controlling Entropy . In 4., the training procedure is replaced by Scene Coordinate Initialization (if no model, then heuristic is used), Optimization of Reprojection Error and End-to-End Optimization (Gaussian-Netwon, and approximate refinement gradients). [1] Brachmann, Eric, et al. \"DSAC-differentiable RANSAC for camera localization.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017. LO-Net: Deep Real-time Lidar Odometry This paper is a fusion of geometry-based and learning-based method. It contains Lidar data encoding. Geometric consistency constraint based on local normal. Lidar odometry regression with ground truth pose supervision. Mask prediction fused with geometric consistency constraint loss. The mapping part uses the mask and is a geometry-based method.","title":"2019 April Reading Reports"},{"location":"2019/2019%20Apr/#2019-april-reading-reports","text":"","title":"2019 April Reading Reports"},{"location":"2019/2019%20Apr/#canny-vo-visual-odometry-with-rgb-d-cameras-based-on-geometric-3-d2-d-edge-alignment","text":"This paper solves three problems in edge-based VO: Discontinuity of the objective function. The potential biases in model-to-data paradigm. The robustness of the method, related to the reweighting formulation. To solve the first problem, it uses ICP-based estimation, but with different nearest neighbor finding method. It applies Approximate Nearest Neighbor Fields (ANNF) to make a lookup table in the current image. Similar to point-to-plane ICP, point-to-tangent residuals are used, which are formed by warped original (model) gradient vector . The second problem is solved by introducing Oriented Nearest Neighbor Fields (ONNF), which divides the orientation into eight distance fields. The orientations of the points in model are calculated with their poses and compared with relevant orientation bins. Then the gradient directions are approximated by the center of the corresponding orientations. To estimate robustly, the authors uses probabilistic sensor model as [6] and point culling by discarding the points with large geometric residuals than the median of the overall residuals. [6] C. Kerl, J. Sturm, and D. Cremers, \u201cRobust odometry estimation for RGBD cameras,\u201d in Proc. IEEE Int. Conf. Robot. Autom., 2013, pp. 3748\u20133754.","title":"Canny-VO: Visual Odometry With RGB-D Cameras Based on Geometric 3-D\u20132-D Edge Alignment"},{"location":"2019/2019%20Apr/#fast-and-accurate-point-cloud-registration-using-trees-of-gaussian-mixtures","text":"This paper mainly uses the structure of trees of Gaussian mixture model to make registration as a expectation maximization problem. The E step is the adaptive tree search, which corresponds to the data association problem. It efficiently associates the points to the suitable tree level in the tree of GMM model. The M step is the proposed Mahalanobis estimation given the \\(N \\times J\\) set of point-cluster correspondences \\(\\mathcal{C} = \\{ c_{ij} \\}\\) , which is rewritten by PCA decomposition in to a mixed MLE criterion.","title":"Fast and Accurate Point Cloud Registration using Trees of Gaussian Mixtures"},{"location":"2019/2019%20Apr/#beyond-photometric-loss-for-self-supervised-ego-motion-estimation","text":"This paper combines the accurate intermediate geometric representations of traditional monocular SLAM with self-supervised depth estimation to deliver a better formulation for joint depth and motion estimation. The loss function consists of photometric loss (with percentile mask), depth smooth loss, geometric loss (by pairwise matching to form epipolar constraints, a set of feature matches \\(\\{(p_i , q_i)\\}\\) in the homogeneous image coordinates can be reliably obtained), pose loss (for comparison only, not in the proposed method).","title":"Beyond Photometric Loss for Self-Supervised Ego-Motion Estimation"},{"location":"2019/2019%20Apr/#learning-less-is-more-6d-camera-localization-via-3d-surface-regression","text":"This is an improved version for DSAC [1]. It can be trained from RGB images and ground truth poses only, without using the 3D scene model. It contains four parts in DSAC: 1. Scene Coordinate Regression , 2. Pose Hypothesis Sampling , 3. Hypothesis Selection , 4. Hypothesis Refinement . In this paper, 1. is replaced by fully convolutional network (FCN) instead of sampling. 3. is replaced by Soft Inlier Counting and Controlling Entropy . In 4., the training procedure is replaced by Scene Coordinate Initialization (if no model, then heuristic is used), Optimization of Reprojection Error and End-to-End Optimization (Gaussian-Netwon, and approximate refinement gradients). [1] Brachmann, Eric, et al. \"DSAC-differentiable RANSAC for camera localization.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017.","title":"Learning Less is More - 6D Camera Localization via 3D Surface Regression"},{"location":"2019/2019%20Apr/#lo-net-deep-real-time-lidar-odometry","text":"This paper is a fusion of geometry-based and learning-based method. It contains Lidar data encoding. Geometric consistency constraint based on local normal. Lidar odometry regression with ground truth pose supervision. Mask prediction fused with geometric consistency constraint loss. The mapping part uses the mask and is a geometry-based method.","title":"LO-Net: Deep Real-time Lidar Odometry"},{"location":"2019/2019%20Aug/","text":"2019 August Reading Reports Learning Local Feature Descriptor with Motion Attribute For Vision-based Localization This paper designs an FCN to estimate the motion attribute of each local feature point to distinguish static points from moving or unstable points (labels from segmentation datasets, e.g., Cityscapes). It then enhances the FCN to calculate the descriptors of hand-crafted (FAST) / learning-based (SuperPoint) local features, taking the output of HardNet as the supervision signal. Finally, it integrates the proposed local feature processing pipeline into a vision-based localization method. MD-Net has three modules: the backbone network \\(\\mathcal{N}_\\text{B}\\) to extract shared features from the input image, the motion attribute branch \\(\\mathcal{N}_\\text{M}\\) to estimate motion attribute, the description branch \\(\\mathcal{N}_\\text{D}\\) to extract distinctive descriptor. Visual-LiDAR Odometry Aided by Reduced IMU This paper proposes a odmetry method using reduced IMU (two horizontal accelerometers and one vertical gyro) and visual-lidar odometry. The backbone of the method is RISS (Reduced Inertial Sensor System). This method replace the velocity from odometry by the estimation from a lidar-visual odometry, where stereo VO provide an initial value for lidar registration. It is a KF based method. Direct Sparse Mapping The propsoed direct VSLAM framework enables to build a persistent map and reuse existing map information from old keyframes directly in the photometric bundle adjustment. Local map covisibility window combines \\(N_t\\) temporally connect keyframes, like in DSO 1 , and \\(N_c\\) covisbile keyframes maximizing the depleted areas of points. The t-distribution with Median Absolute Deviation (MAD), similar to the one in 2 , is used to filter put outliers. Too many outliers make the frame be discarded directly. Flexible Trinocular: Non-rigid Multi-Camera-IMU Dense Reconstruction for UAV Navigation and Mapping This paper introduces a trinocular non-rigid multi-camera-IMU dense reconstruction method. The inertial-photometric alignment uses landmark position of a feature in the center camera and projects the point to the wing cameras. Then follow SVO's framework, it uses Gauss-Newton solver with pyramid layers to optimize the transforms among cameras. The motion constraints are obtained from the wing model. The dense reconstruction is calculated by stereo block-matching (BM) for the rectified images. The transforms of the baselines are calculated by side camera with April tags and Kalibr . These transforms of the wing model are formulated as a Gaussian priors. Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery This paper an approach to incrementally build volumetric object-centric maps during online scanning with a localized RGB-D camera. It contains a per-frame segmentation scheme (unsupervised geometric approach and instance-aware semantic predictions), a data association step and a map integration strategy by 3 . Engel, J., Koltun, V., & Cremers, D. (2017). Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3), 611-625. \u21a9 Kerl, C., Sturm, J., & Cremers, D. (2013, May). Robust odometry estimation for RGB-D cameras. In 2013 IEEE International Conference on Robotics and Automation (pp. 3748-3754). IEEE. \u21a9 Furrer, F., Novkovic, T., Fehr, M., Gawel, A., Grinvald, M., Sattler, T., ... & Nieto, J. (2018, October). Incremental object database: Building 3d models from multiple partial observations. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 6835-6842). IEEE. Chicago \u21a9","title":"2019 August Reading Reports"},{"location":"2019/2019%20Aug/#2019-august-reading-reports","text":"","title":"2019 August Reading Reports"},{"location":"2019/2019%20Aug/#learning-local-feature-descriptor-with-motion-attribute-for-vision-based-localization","text":"This paper designs an FCN to estimate the motion attribute of each local feature point to distinguish static points from moving or unstable points (labels from segmentation datasets, e.g., Cityscapes). It then enhances the FCN to calculate the descriptors of hand-crafted (FAST) / learning-based (SuperPoint) local features, taking the output of HardNet as the supervision signal. Finally, it integrates the proposed local feature processing pipeline into a vision-based localization method. MD-Net has three modules: the backbone network \\(\\mathcal{N}_\\text{B}\\) to extract shared features from the input image, the motion attribute branch \\(\\mathcal{N}_\\text{M}\\) to estimate motion attribute, the description branch \\(\\mathcal{N}_\\text{D}\\) to extract distinctive descriptor.","title":"Learning Local Feature Descriptor with Motion Attribute For Vision-based Localization"},{"location":"2019/2019%20Aug/#visual-lidar-odometry-aided-by-reduced-imu","text":"This paper proposes a odmetry method using reduced IMU (two horizontal accelerometers and one vertical gyro) and visual-lidar odometry. The backbone of the method is RISS (Reduced Inertial Sensor System). This method replace the velocity from odometry by the estimation from a lidar-visual odometry, where stereo VO provide an initial value for lidar registration. It is a KF based method.","title":"Visual-LiDAR Odometry Aided by Reduced IMU"},{"location":"2019/2019%20Aug/#direct-sparse-mapping","text":"The propsoed direct VSLAM framework enables to build a persistent map and reuse existing map information from old keyframes directly in the photometric bundle adjustment. Local map covisibility window combines \\(N_t\\) temporally connect keyframes, like in DSO 1 , and \\(N_c\\) covisbile keyframes maximizing the depleted areas of points. The t-distribution with Median Absolute Deviation (MAD), similar to the one in 2 , is used to filter put outliers. Too many outliers make the frame be discarded directly.","title":"Direct Sparse Mapping"},{"location":"2019/2019%20Aug/#flexible-trinocular-non-rigid-multi-camera-imu-dense-reconstruction-for-uav-navigation-and-mapping","text":"This paper introduces a trinocular non-rigid multi-camera-IMU dense reconstruction method. The inertial-photometric alignment uses landmark position of a feature in the center camera and projects the point to the wing cameras. Then follow SVO's framework, it uses Gauss-Newton solver with pyramid layers to optimize the transforms among cameras. The motion constraints are obtained from the wing model. The dense reconstruction is calculated by stereo block-matching (BM) for the rectified images. The transforms of the baselines are calculated by side camera with April tags and Kalibr . These transforms of the wing model are formulated as a Gaussian priors.","title":"Flexible Trinocular: Non-rigid Multi-Camera-IMU Dense Reconstruction for UAV Navigation and Mapping"},{"location":"2019/2019%20Aug/#volumetric-instance-aware-semantic-mapping-and-3d-object-discovery","text":"This paper an approach to incrementally build volumetric object-centric maps during online scanning with a localized RGB-D camera. It contains a per-frame segmentation scheme (unsupervised geometric approach and instance-aware semantic predictions), a data association step and a map integration strategy by 3 . Engel, J., Koltun, V., & Cremers, D. (2017). Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3), 611-625. \u21a9 Kerl, C., Sturm, J., & Cremers, D. (2013, May). Robust odometry estimation for RGB-D cameras. In 2013 IEEE International Conference on Robotics and Automation (pp. 3748-3754). IEEE. \u21a9 Furrer, F., Novkovic, T., Fehr, M., Gawel, A., Grinvald, M., Sattler, T., ... & Nieto, J. (2018, October). Incremental object database: Building 3d models from multiple partial observations. In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 6835-6842). IEEE. Chicago \u21a9","title":"Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery"},{"location":"2019/2019%20Dec/","text":"2019 December Reading Reports Associating Uncertainty With Three-Dimensional Poses for Use in Estimation Problems The authors introduce approaches to associate uncertainty with \\(4 \\times 4\\) transformation matrices. It demonstrates 1) propagating uncertainty through a compound pose change, 2) fusing multiple measurements of a pose (e.g., for use in pose-graph relaxation), and 3) propagating uncertainty on poses (and landmarks) through a nonlinear camera model. The accompanying script is available.","title":"2019 Dec"},{"location":"2019/2019%20Dec/#2019-december-reading-reports","text":"","title":"2019 December Reading Reports"},{"location":"2019/2019%20Dec/#associating-uncertainty-with-three-dimensional-poses-for-use-in-estimation-problems","text":"The authors introduce approaches to associate uncertainty with \\(4 \\times 4\\) transformation matrices. It demonstrates 1) propagating uncertainty through a compound pose change, 2) fusing multiple measurements of a pose (e.g., for use in pose-graph relaxation), and 3) propagating uncertainty on poses (and landmarks) through a nonlinear camera model. The accompanying script is available.","title":"Associating Uncertainty With Three-Dimensional Poses for Use in Estimation Problems"},{"location":"2019/2019%20Feb/","text":"2019 February Reading Reports 3D Reconstruction using a Sparse Laser Scanner and a Single Camera for Outdoor Autonomous Vehicle This paper introduces a method for creating dense Lidar depth from sparse inputs and image. It uses Point histogram to remove outliers, Gaussian processes to make Lidar depth denser and finally modified MRF to optimize the final potential cost function (similar to reference [5], J. Diebel and S. Thrun). Visual Localization within LIDAR Maps for Automated Urban Driving This paper uses monocular camera to localize in 3D Lidar map with reflectivities . It first builds an accurate 3D Lidar map by offline pose-graph with information from 3D Lidar, wheel odometer, and GPS (artificial height prior for z axis of GPS). Then dense ground-plane mesh is built by the poses and extracted point clouds. The projective image registration generates \\(dom(x) \\times dom(y) \\times dom(\\theta)\\) predicted views and use NMI evaluation metric get the optimal alignment and relative offset \\(\\delta x_i\\) to the initial pose prior \\(x_k\\) . It first generate \\(n_x \\times n_y\\) synthetic views, then compare each against \\(n_\\theta\\) (warped) measurements. This reduces the OpenGL pipeline overhead. Then, all the combinations go through a local search to get the optimal value by validating the normalized mutual information . Finally, the 3-DOF localization is used in a EKF localization framework (IMU prediction, localization measurement). Automatic camera and range sensor calibration using a single shot This paper mainly calibrate camera-to-camera and camera-to-range extrinsic parameters using a single shot with multiple checkerboards. For camera-to-camera, it robustly locates checkerboard corners, refines for sub-pixel accuracy, recovers structures, determines correspondences and finally optimizes the non-linear cost function. For camera-to-range, it segments the regions corresponding to a checkerboard, followed by linear global registration and non-linear fine registration. Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments Normalized information distance (NID) between a live camera and an image generated from prior map is used for localization and calibration. This paper is related to [1], [2] and [3]. In [1] and [2] the NID cost is optimized by B-spline representation of the histogram and its derivative. In [3], the cost function is of mutual information, where kernel density estimate (KDE, Gaussian blur in the codes) is used. [1] Pascoe, G., Maddern, W. P., & Newman, P. (2015, September). Robust Direct Visual Localisation using Normalised Information Distance. In BMVC (pp. 70-1). [2] Stewart, A. D. (2014). Localisation using the appearance of prior structure (Doctoral dissertation, University of Oxford). [3] Pandey, G., McBride, J. R., Savarese, S., & Eustice, R. M. (2015). Automatic extrinsic calibration of vision and lidar by maximizing mutual information. Journal of Field Robotics , 32 (5), 696-722.","title":"2019 February Reading Reports"},{"location":"2019/2019%20Feb/#2019-february-reading-reports","text":"","title":"2019 February Reading Reports"},{"location":"2019/2019%20Feb/#3d-reconstruction-using-a-sparse-laser-scanner-and-a-single-camera-for-outdoor-autonomous-vehicle","text":"This paper introduces a method for creating dense Lidar depth from sparse inputs and image. It uses Point histogram to remove outliers, Gaussian processes to make Lidar depth denser and finally modified MRF to optimize the final potential cost function (similar to reference [5], J. Diebel and S. Thrun).","title":"3D Reconstruction using a Sparse Laser Scanner and a Single Camera for Outdoor Autonomous Vehicle"},{"location":"2019/2019%20Feb/#visual-localization-within-lidar-maps-for-automated-urban-driving","text":"This paper uses monocular camera to localize in 3D Lidar map with reflectivities . It first builds an accurate 3D Lidar map by offline pose-graph with information from 3D Lidar, wheel odometer, and GPS (artificial height prior for z axis of GPS). Then dense ground-plane mesh is built by the poses and extracted point clouds. The projective image registration generates \\(dom(x) \\times dom(y) \\times dom(\\theta)\\) predicted views and use NMI evaluation metric get the optimal alignment and relative offset \\(\\delta x_i\\) to the initial pose prior \\(x_k\\) . It first generate \\(n_x \\times n_y\\) synthetic views, then compare each against \\(n_\\theta\\) (warped) measurements. This reduces the OpenGL pipeline overhead. Then, all the combinations go through a local search to get the optimal value by validating the normalized mutual information . Finally, the 3-DOF localization is used in a EKF localization framework (IMU prediction, localization measurement).","title":"Visual Localization within LIDAR Maps for Automated Urban Driving"},{"location":"2019/2019%20Feb/#automatic-camera-and-range-sensor-calibration-using-a-single-shot","text":"This paper mainly calibrate camera-to-camera and camera-to-range extrinsic parameters using a single shot with multiple checkerboards. For camera-to-camera, it robustly locates checkerboard corners, refines for sub-pixel accuracy, recovers structures, determines correspondences and finally optimizes the non-linear cost function. For camera-to-range, it segments the regions corresponding to a checkerboard, followed by linear global registration and non-linear fine registration.","title":"Automatic camera and range sensor calibration using a single shot"},{"location":"2019/2019%20Feb/#direct-visual-localisation-and-calibration-for-road-vehicles-in-changing-city-environments","text":"Normalized information distance (NID) between a live camera and an image generated from prior map is used for localization and calibration. This paper is related to [1], [2] and [3]. In [1] and [2] the NID cost is optimized by B-spline representation of the histogram and its derivative. In [3], the cost function is of mutual information, where kernel density estimate (KDE, Gaussian blur in the codes) is used. [1] Pascoe, G., Maddern, W. P., & Newman, P. (2015, September). Robust Direct Visual Localisation using Normalised Information Distance. In BMVC (pp. 70-1). [2] Stewart, A. D. (2014). Localisation using the appearance of prior structure (Doctoral dissertation, University of Oxford). [3] Pandey, G., McBride, J. R., Savarese, S., & Eustice, R. M. (2015). Automatic extrinsic calibration of vision and lidar by maximizing mutual information. Journal of Field Robotics , 32 (5), 696-722.","title":"Direct Visual Localisation and Calibration for Road Vehicles in Changing City Environments"},{"location":"2019/2019%20Jan/","text":"2019 January Reading Reports Robust Photogeometric Localization over Time for Map-Centric Loop Closure First, with the tightly coupled geometric and photometric constraints, the method robustly estimates 6 Degree of Freedom (DoF) alignment even when the initial guess for registration is completely incorrect or unknown. Second, instead of making a loop closure decision based on only a single 6 DoF alignment estimation, the proposed method observes the stability of the alignment over time and provides a reliability metric to reject or accept a loop closure hypothesis without a global trajectory optimization. For an initial seeding of the loop closure, a bag-of-word based place voting method is utilized.","title":"2019 January Reading Reports"},{"location":"2019/2019%20Jan/#2019-january-reading-reports","text":"","title":"2019 January Reading Reports"},{"location":"2019/2019%20Jan/#robust-photogeometric-localization-over-time-for-map-centric-loop-closure","text":"First, with the tightly coupled geometric and photometric constraints, the method robustly estimates 6 Degree of Freedom (DoF) alignment even when the initial guess for registration is completely incorrect or unknown. Second, instead of making a loop closure decision based on only a single 6 DoF alignment estimation, the proposed method observes the stability of the alignment over time and provides a reliability metric to reject or accept a loop closure hypothesis without a global trajectory optimization. For an initial seeding of the loop closure, a bag-of-word based place voting method is utilized.","title":"Robust Photogeometric Localization over Time for Map-Centric Loop Closure"},{"location":"2019/2019%20Jul/","text":"2019 June Reading Reports Sharing Heterogeneous Spatial Knowledge: Map Fusion between Asynchronous Monocular Vision and Lidar or Other Prior Inputs This paper extracts vertical planes from both vision and prior/lidar data, and use them as the anchoring information to fuse the heterogeneous maps. A novel consensus score in image space is proposed to perform RANSAC (closest point in the plane, some kind make it independent of the absolute map scale). The matching can be solved by topology graph matching, or using manual inputs if the number of planes is small. The optimization fuses re-projection error and planar information as a constrained optimization problem. The author solves it by converting it to an unconstrained optimization problem by adding a penalty function. Visual Navigation Using Heterogeneous Landmarks and Unsupervised Geometric Constraints This paper introduces a heterogeneous landmark-based (points, line segments, lines, planes, and vanishing points) visual navigation approach for a monocular mobile robot. The inner geometric constraints are managed by a novel multilayer feature graph (MFG). Key frame selection, 2D multilayer feature graph construction and matching, camera pose estimation, 3D multiplayer feature graph update constitute the main estimation system and matching. Finally, the system contains local bundle adjustment with geometric constraints and MFG pruning. A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3D Reconstructions This paper uses lidar and stereo camera to achieve accurate dense 3D reconstructions. It joints the bundle adjustment and cloud registration problem ( stationary ) in a probabilistic framework. Image observation is based on disparity image by Semi-Global Matching (SURF), which is used to obtain feature point with depth value. Lidar observations are from Binary Shape Context (BSC) descriptors. Then, they are used to do roughly register point clouds. Joint optimization is formed by the observations, poses and extrinsic parameters. Image observations and landmarks form reprojection and depth error; lidar observations form point-to-plane distance error. Two-fold fusion of LiDAR and camera data for each frame or station: 1. stereo depth is removed by comparing with the projected lidar depth, 2. lidar depth is filled the holes in the stereo depth (locally flat regions). The authors suggest to meet two conditions [1], 1) 3 stations, 2 motion pairs, 2)rotation axes of camera axes not colinear for different motion pairs, to guarantee a unique solution of extrinsic parameters. [1] Tsai, R. Y., & Lenz, R. K. (1989). A new technique for fully autonomous and efficient 3D robotics hand/eye calibration. IEEE Transactions on robotics and automation, 5(3), 345-358. A robust pose graph approach for city scale LiDAR mapping This paper introduces a mapping method using a 3D lidar and an INS. An enhanced graph structure considering initialization bias in each assignment is used. Scan-matching factors are examined twice: 1. a classifier considering extracted features from both registration and heterogeneous sensors, 2. in a hierarchical optimization procedure. To fileter the dynamic objects, a multi-hypothesis extended Kalman filter (MH-EKF) [1] is applied. The structure of the factor graph. Registration first by UKF integrate lidar (NDT for 15 recently tracked frames), GPS and IMU. A odometry covariance related to points and poses are updated to determined the criterion for establishing a new submap. A global pose factors are considered for submap poses. Submap matching factors are used, which are from multiple registration algorithms and considered in the pose evaluation module for rejecting misaligned pairs. Strategy of removing false submap matching factors. Two-stage strategy: a) classifying scan matching by a random forest classifier, which takes INS pose and registration pose and points and time span features. b) optimization strategy by a robust approach rather than traditional graph optimization frameworks, which can optimize a pose graph contain false-positive factors. A practical way to remove dynamic: MV3D [2] for detecting dynamic objects and MH-EKF. The object will be removed extending along the laser direction to a fitted ground for hole filling. Such filtering is performed twice (forward and backward) for a better recall. [1] Vaquero, V., del Pino, I., Moreno-Noguer, F., Sola, J., Sanfeliu, A., & Andrade-Cetto, J. (2017, September). Deconvolutional networks for point-cloud vehicle detection and tracking in driving scenarios. In 2017 European Conference on Mobile Robots (ECMR) (pp. 1-7). IEEE. [2] Chen, X., Ma, H., Wan, J., Li, B., & Xia, T. (2017). Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1907-1915).","title":"2019 June Reading Reports"},{"location":"2019/2019%20Jul/#2019-june-reading-reports","text":"","title":"2019 June Reading Reports"},{"location":"2019/2019%20Jul/#sharing-heterogeneous-spatial-knowledge-map-fusion-between-asynchronous-monocular-vision-and-lidar-or-other-prior-inputs","text":"This paper extracts vertical planes from both vision and prior/lidar data, and use them as the anchoring information to fuse the heterogeneous maps. A novel consensus score in image space is proposed to perform RANSAC (closest point in the plane, some kind make it independent of the absolute map scale). The matching can be solved by topology graph matching, or using manual inputs if the number of planes is small. The optimization fuses re-projection error and planar information as a constrained optimization problem. The author solves it by converting it to an unconstrained optimization problem by adding a penalty function.","title":"Sharing Heterogeneous Spatial Knowledge: Map Fusion between Asynchronous Monocular Vision and Lidar or Other Prior Inputs"},{"location":"2019/2019%20Jul/#visual-navigation-using-heterogeneous-landmarks-and-unsupervised-geometric-constraints","text":"This paper introduces a heterogeneous landmark-based (points, line segments, lines, planes, and vanishing points) visual navigation approach for a monocular mobile robot. The inner geometric constraints are managed by a novel multilayer feature graph (MFG). Key frame selection, 2D multilayer feature graph construction and matching, camera pose estimation, 3D multiplayer feature graph update constitute the main estimation system and matching. Finally, the system contains local bundle adjustment with geometric constraints and MFG pruning.","title":"Visual Navigation Using Heterogeneous Landmarks and Unsupervised Geometric Constraints"},{"location":"2019/2019%20Jul/#a-joint-optimization-approach-of-lidar-camera-fusion-for-accurate-dense-3d-reconstructions","text":"This paper uses lidar and stereo camera to achieve accurate dense 3D reconstructions. It joints the bundle adjustment and cloud registration problem ( stationary ) in a probabilistic framework. Image observation is based on disparity image by Semi-Global Matching (SURF), which is used to obtain feature point with depth value. Lidar observations are from Binary Shape Context (BSC) descriptors. Then, they are used to do roughly register point clouds. Joint optimization is formed by the observations, poses and extrinsic parameters. Image observations and landmarks form reprojection and depth error; lidar observations form point-to-plane distance error. Two-fold fusion of LiDAR and camera data for each frame or station: 1. stereo depth is removed by comparing with the projected lidar depth, 2. lidar depth is filled the holes in the stereo depth (locally flat regions). The authors suggest to meet two conditions [1], 1) 3 stations, 2 motion pairs, 2)rotation axes of camera axes not colinear for different motion pairs, to guarantee a unique solution of extrinsic parameters. [1] Tsai, R. Y., & Lenz, R. K. (1989). A new technique for fully autonomous and efficient 3D robotics hand/eye calibration. IEEE Transactions on robotics and automation, 5(3), 345-358.","title":"A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3D Reconstructions"},{"location":"2019/2019%20Jul/#a-robust-pose-graph-approach-for-city-scale-lidar-mapping","text":"This paper introduces a mapping method using a 3D lidar and an INS. An enhanced graph structure considering initialization bias in each assignment is used. Scan-matching factors are examined twice: 1. a classifier considering extracted features from both registration and heterogeneous sensors, 2. in a hierarchical optimization procedure. To fileter the dynamic objects, a multi-hypothesis extended Kalman filter (MH-EKF) [1] is applied. The structure of the factor graph. Registration first by UKF integrate lidar (NDT for 15 recently tracked frames), GPS and IMU. A odometry covariance related to points and poses are updated to determined the criterion for establishing a new submap. A global pose factors are considered for submap poses. Submap matching factors are used, which are from multiple registration algorithms and considered in the pose evaluation module for rejecting misaligned pairs. Strategy of removing false submap matching factors. Two-stage strategy: a) classifying scan matching by a random forest classifier, which takes INS pose and registration pose and points and time span features. b) optimization strategy by a robust approach rather than traditional graph optimization frameworks, which can optimize a pose graph contain false-positive factors. A practical way to remove dynamic: MV3D [2] for detecting dynamic objects and MH-EKF. The object will be removed extending along the laser direction to a fitted ground for hole filling. Such filtering is performed twice (forward and backward) for a better recall. [1] Vaquero, V., del Pino, I., Moreno-Noguer, F., Sola, J., Sanfeliu, A., & Andrade-Cetto, J. (2017, September). Deconvolutional networks for point-cloud vehicle detection and tracking in driving scenarios. In 2017 European Conference on Mobile Robots (ECMR) (pp. 1-7). IEEE. [2] Chen, X., Ma, H., Wan, J., Li, B., & Xia, T. (2017). Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1907-1915).","title":"A robust pose graph approach for city scale LiDAR mapping"},{"location":"2019/2019%20Jun/","text":"2019 July Reading Reports Visual-Inertial Localization with Prior LiDAR Map Constraints This paper introduces a tightly-coupled state estimation algorithm for visual-inertial localization with prior LiDAR map constraints. This feature tracking and semi-dense visual map is based on stereo. The semi-dense visual map is refined by compatibility test. Then the global pose registration by NDT point-to-distribution based method is used as a global camera pose to correct the odometry drift in the MSCKF based VIO system.","title":"2019 July Reading Reports"},{"location":"2019/2019%20Jun/#2019-july-reading-reports","text":"","title":"2019 July Reading Reports"},{"location":"2019/2019%20Jun/#visual-inertial-localization-with-prior-lidar-map-constraints","text":"This paper introduces a tightly-coupled state estimation algorithm for visual-inertial localization with prior LiDAR map constraints. This feature tracking and semi-dense visual map is based on stereo. The semi-dense visual map is refined by compatibility test. Then the global pose registration by NDT point-to-distribution based method is used as a global camera pose to correct the odometry drift in the MSCKF based VIO system.","title":"Visual-Inertial Localization with Prior LiDAR Map Constraints"},{"location":"2019/2019%20Mar/","text":"2019 March Reading Reports Real-time monocular dense mapping on aerial robots using visual-inertial fusion This paper is a VIO version (replacing the pose estimation) of Chisel [1]. The motion stereo by plane-sweeping is introduced in the paper to use the temporally accumulated matching cost. An idea similar to SGM is used to remove the outliers by dynamic programming. Then an uncertainty-aware TSDF is further introduced. [1] M. Klingensmith, I. Dryanovski, S. Srinivasa, and J. Xiao, \u201cChisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields,\u201d in Robotics: Science and Systems XI , 2015. Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields It uses many of little details to build a based signed distance field using CPU. They Chisel away data that has nonpositive stored SDF. A hashed chunks of voxels is used. Within the chuck, voxels are stored in a monolithic monolithic block. NID-SLAM: Robust Monocular SLAM using Normalised Information Distance Normalized information distance is used to do tracking (with photometric tracking first to get the depth). Multi-resolution \\(n\\) -channel histogram images are used for a coarse-to-fine image pyramid approach. Finally, it fuses into a pose graph optimization using FAB-MAP to provide loop closure candidates. Direct Sparse Visual-Inertial Odometry Using Dynamic Marginalization This paper fuses DSO with IMU to achieve an optimization-based odometry. The scale is not initialized before the optimization. The dynamic marginalization in parallel optimize \\(G_{ba}=G_{metric} \\cup G_{visual} \\cup M_{curr}\u200b\\) and \\(G_{metric} \\cup G_{visual} \\cup M_{half}\u200b\\) . \\(M_{visual}\\) only scale independent information \\(M_{curr}\\) all information since the time of setting linearization point \\(M_{half}\u200b\\) recent states These three will be updated with \\(s_{curr}\\) and upper, lower bounds related to \\(s_{middle}\\) . An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM This paper introduces a method to combine MSCKF with map feature points to reduce drift efficiently. It can have linear computational complexity in terms of map size instead of traditional quadratic complexity. Schmidt-EKF sets \\(\\mathbf{K}_{S_k} = 0\\) to reduce the update for the matured map points. The results are compatible with hybrid MSCKF/SLAM method, which is slower if too many features are included.","title":"2019 March Reading Reports"},{"location":"2019/2019%20Mar/#2019-march-reading-reports","text":"","title":"2019 March Reading Reports"},{"location":"2019/2019%20Mar/#real-time-monocular-dense-mapping-on-aerial-robots-using-visual-inertial-fusion","text":"This paper is a VIO version (replacing the pose estimation) of Chisel [1]. The motion stereo by plane-sweeping is introduced in the paper to use the temporally accumulated matching cost. An idea similar to SGM is used to remove the outliers by dynamic programming. Then an uncertainty-aware TSDF is further introduced. [1] M. Klingensmith, I. Dryanovski, S. Srinivasa, and J. Xiao, \u201cChisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields,\u201d in Robotics: Science and Systems XI , 2015.","title":"Real-time monocular dense mapping on aerial robots using visual-inertial fusion"},{"location":"2019/2019%20Mar/#chisel-real-time-large-scale-3d-reconstruction-onboard-a-mobile-device-using-spatially-hashed-signed-distance-fields","text":"It uses many of little details to build a based signed distance field using CPU. They Chisel away data that has nonpositive stored SDF. A hashed chunks of voxels is used. Within the chuck, voxels are stored in a monolithic monolithic block.","title":"Chisel: Real Time Large Scale 3D Reconstruction Onboard a Mobile Device using Spatially Hashed Signed Distance Fields"},{"location":"2019/2019%20Mar/#nid-slam-robust-monocular-slam-using-normalised-information-distance","text":"Normalized information distance is used to do tracking (with photometric tracking first to get the depth). Multi-resolution \\(n\\) -channel histogram images are used for a coarse-to-fine image pyramid approach. Finally, it fuses into a pose graph optimization using FAB-MAP to provide loop closure candidates.","title":"NID-SLAM: Robust Monocular SLAM using Normalised Information Distance"},{"location":"2019/2019%20Mar/#direct-sparse-visual-inertial-odometry-using-dynamic-marginalization","text":"This paper fuses DSO with IMU to achieve an optimization-based odometry. The scale is not initialized before the optimization. The dynamic marginalization in parallel optimize \\(G_{ba}=G_{metric} \\cup G_{visual} \\cup M_{curr}\u200b\\) and \\(G_{metric} \\cup G_{visual} \\cup M_{half}\u200b\\) . \\(M_{visual}\\) only scale independent information \\(M_{curr}\\) all information since the time of setting linearization point \\(M_{half}\u200b\\) recent states These three will be updated with \\(s_{curr}\\) and upper, lower bounds related to \\(s_{middle}\\) .","title":"Direct Sparse Visual-Inertial Odometry Using Dynamic Marginalization"},{"location":"2019/2019%20Mar/#an-efficient-schmidt-ekf-for-3d-visual-inertial-slam","text":"This paper introduces a method to combine MSCKF with map feature points to reduce drift efficiently. It can have linear computational complexity in terms of map size instead of traditional quadratic complexity. Schmidt-EKF sets \\(\\mathbf{K}_{S_k} = 0\\) to reduce the update for the matured map points. The results are compatible with hybrid MSCKF/SLAM method, which is slower if too many features are included.","title":"An Efficient Schmidt-EKF for 3D Visual-Inertial SLAM"},{"location":"2019/2019%20May/","text":"2019 May Reading Reports Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities It converts 2D Delaunay triangluation of keypoints into 3D as mesh. Then, it jointly optimizes VIO states with the 3D plane regularities in the pose graph. Simultaneous Localization and Mapping with Infinite Plane It presented a novel minimal representation for planar features as quaternion in \\(S^3\\) and use the exponential map to update planes. A relative formulation allows for faster convergence. Robust Planar Odometry Based on Symmetric Range Flow and Multiscan Alignment This paper uses a symmetric representation of geometric consistency between scans. It is similar to range flow constraint. The motion of the scan observations (related to the residual) is considered as a function of the rigid motion of the scanner. The geometric residual \\(\\rho(\\mathbf{\\xi}, \\theta)\\) is defined as the evaluation of the range flow constraint for a given motion \\(\\mathbf{\\xi}\\) at a given angle \\(\\theta\\) : \\[ \\rho(\\mathbf{\\xi}, \\theta) = R_2(\\theta) - R_1(\\theta) + (\\bar{x}\\sin\\theta - \\bar{y}\\cos\\theta - \\bar{R}_\\theta)\\xi_\\omega \\\\ + (\\cos\\theta+\\frac{\\bar{R}_\\theta\\sin\\theta}{\\bar{r}})\\xi_x + (\\sin\\theta - \\frac{\\bar{R}_\\theta\\cos\\theta}{\\bar{r}})\\xi_y \\] The incoming scan with either the previous on or the last selected keyscan are jointly aligned. The last keyscan is is warped to previous one, making the cost function related to one motion parameter. It uses preweighted set of residuals (related to the nonlinearities and discontinuities, first and second order derivative). Some robust function are applied for denying outlier and moving objects. Some thresholds (related to the keyscans, the working regions) are from simulation with different sensors (like with different FOVs). VITAMIN-E: VIsual Tracking And MappINg with Extremely Dense Feature Points This paper proposes an indirect monocular SLAM algorithm with extremely dense feature points. It first tracks extremely dense feature points, which are from local extrema of curvature points. The definition of curvature is \\[ \\kappa = f^2_yf_{xx}-2f_xf_yf_{xy}+f^2_xf_{yy}. \\] The tracking is then informed by the dominant flow estimation (the affine transformation is fit from subsampled images). Then curvature extrema points are corrected by maximizing an evaluation function w.r.t to curvature and regularization terms. Then bundle adjustment for dense tracking is formed by a variant of Schur complement. A subspace Gauss-Newton method fixing all variables except the variables to be optimized (an extension of Gauss-Seidel). Dense reconstruction is used by project the 3D points onto an image and apple Delaunay triangulation to generate triangular meshes. Then NLTGV minimization is used to remove noise on the meshes. The meshes are finally integrated in TSDF by OpenChisel. Visual-Inertial Mapping with Non-Linear Factor Recovery This paper introduces a novel two-layered visual-inertial mapping approach, which integrates keypoint-based bundle adjustment with inertial and short-term visual tracking through non-linear factor recovery. The VIO system uses sparse optical flow estimation (FAST corner detector, locally-scaled sum of squared differences (LSSD), pyramidal), estimating \\(\\mathbf{T}\\in\\text{SE}(2)\\) . Then it uses unit vectors in 3D to calculate reporjection error. The remaining part of VIO contains IMU error and optimization and partial marginalization. The visual-inertial mapping (loop closures) prevents the drifts in the previous VIO system. The optimization includes the bundle adjustment (camera poses and landmarks constraints) and non-linear factor recovery (NFR) items. The non-linear factor recovery is similar to the information sparsi\ufb01cation. The recovered measurements from marginalization priors consist of relative pose \\(\\mathbf{z}_\\text{rel}\\) , roll-pitch \\(\\mathbf{z}_\\text{rp}\\) , position \\(\\mathbf{z}_\\text{pos}\\) and yaw \\(\\mathbf{z}_\\text{yaw}\\) residuals to constrain the poses \\(\\mathbf{T}\\) (the first two are used). The approximation of NFR benefits from the full-rank invertable Jacobian \\(\\mathbf{J}_\\text{r}\\) from the recovering information matrices. Dense Surface Reconstruction from Monocular Vision and LiDAR This paper proposes anew pipeline combining LiDAR and camera in a MVS reconstruction framework to create a dense point cloud of indoor environments. It first densifies the MVS points by (inaccurate) lidar points, which improves the depth estimation in textureless areas. An extension of the Delaunay tetrahedra and graph-cut framework described in [9] to include both LiDAR and camera measurements in surface mesh extraction. In detail, all \\(\\boldsymbol{P}_c\\) , and parts of \\(\\boldsymbol{P}_d\\) , which are not in any depth map \\(\\boldsymbol{D}_p\\) , are inserted. The edge value is changed to \\[ E(\\mathbf{T}) = E_{visibility} + \\lambda_{quality}E_{quality} + \\lambda_{lidar}E_{lidar}. \\] The first two terms are the same as [10], except the visibility from lidar points. The third lidar smoothing term is determined by the composition of points (both from camera or lidar, or mixed). [9] Labatut, P., Pons, J. P., & Keriven, R. (2007, October). Efficient multi-view reconstruction of large-scale scenes using interest points, delaunay triangulation and graph cuts. In 2007 IEEE 11th international conference on computer vision (pp. 1-8). IEEE. [10] Labatut, P., Pons, J. P., & Keriven, R. (2009, December). Robust and efficient surface reconstruction from range data. In Computer graphics forum (Vol. 28, No. 8, pp. 2275-2290). Oxford, UK: Blackwell Publishing Ltd.","title":"2019 May Reading Reports"},{"location":"2019/2019%20May/#2019-may-reading-reports","text":"","title":"2019 May Reading Reports"},{"location":"2019/2019%20May/#incremental-visual-inertial-3d-mesh-generation-with-structural-regularities","text":"It converts 2D Delaunay triangluation of keypoints into 3D as mesh. Then, it jointly optimizes VIO states with the 3D plane regularities in the pose graph.","title":"Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities"},{"location":"2019/2019%20May/#simultaneous-localization-and-mapping-with-infinite-plane","text":"It presented a novel minimal representation for planar features as quaternion in \\(S^3\\) and use the exponential map to update planes. A relative formulation allows for faster convergence.","title":"Simultaneous Localization and Mapping with Infinite Plane"},{"location":"2019/2019%20May/#robust-planar-odometry-based-on-symmetric-range-flow-and-multiscan-alignment","text":"This paper uses a symmetric representation of geometric consistency between scans. It is similar to range flow constraint. The motion of the scan observations (related to the residual) is considered as a function of the rigid motion of the scanner. The geometric residual \\(\\rho(\\mathbf{\\xi}, \\theta)\\) is defined as the evaluation of the range flow constraint for a given motion \\(\\mathbf{\\xi}\\) at a given angle \\(\\theta\\) : \\[ \\rho(\\mathbf{\\xi}, \\theta) = R_2(\\theta) - R_1(\\theta) + (\\bar{x}\\sin\\theta - \\bar{y}\\cos\\theta - \\bar{R}_\\theta)\\xi_\\omega \\\\ + (\\cos\\theta+\\frac{\\bar{R}_\\theta\\sin\\theta}{\\bar{r}})\\xi_x + (\\sin\\theta - \\frac{\\bar{R}_\\theta\\cos\\theta}{\\bar{r}})\\xi_y \\] The incoming scan with either the previous on or the last selected keyscan are jointly aligned. The last keyscan is is warped to previous one, making the cost function related to one motion parameter. It uses preweighted set of residuals (related to the nonlinearities and discontinuities, first and second order derivative). Some robust function are applied for denying outlier and moving objects. Some thresholds (related to the keyscans, the working regions) are from simulation with different sensors (like with different FOVs).","title":"Robust Planar Odometry Based on Symmetric Range Flow and Multiscan Alignment"},{"location":"2019/2019%20May/#vitamin-e-visual-tracking-and-mapping-with-extremely-dense-feature-points","text":"This paper proposes an indirect monocular SLAM algorithm with extremely dense feature points. It first tracks extremely dense feature points, which are from local extrema of curvature points. The definition of curvature is \\[ \\kappa = f^2_yf_{xx}-2f_xf_yf_{xy}+f^2_xf_{yy}. \\] The tracking is then informed by the dominant flow estimation (the affine transformation is fit from subsampled images). Then curvature extrema points are corrected by maximizing an evaluation function w.r.t to curvature and regularization terms. Then bundle adjustment for dense tracking is formed by a variant of Schur complement. A subspace Gauss-Newton method fixing all variables except the variables to be optimized (an extension of Gauss-Seidel). Dense reconstruction is used by project the 3D points onto an image and apple Delaunay triangulation to generate triangular meshes. Then NLTGV minimization is used to remove noise on the meshes. The meshes are finally integrated in TSDF by OpenChisel.","title":"VITAMIN-E: VIsual Tracking And MappINg with Extremely Dense Feature Points"},{"location":"2019/2019%20May/#visual-inertial-mapping-with-non-linear-factor-recovery","text":"This paper introduces a novel two-layered visual-inertial mapping approach, which integrates keypoint-based bundle adjustment with inertial and short-term visual tracking through non-linear factor recovery. The VIO system uses sparse optical flow estimation (FAST corner detector, locally-scaled sum of squared differences (LSSD), pyramidal), estimating \\(\\mathbf{T}\\in\\text{SE}(2)\\) . Then it uses unit vectors in 3D to calculate reporjection error. The remaining part of VIO contains IMU error and optimization and partial marginalization. The visual-inertial mapping (loop closures) prevents the drifts in the previous VIO system. The optimization includes the bundle adjustment (camera poses and landmarks constraints) and non-linear factor recovery (NFR) items. The non-linear factor recovery is similar to the information sparsi\ufb01cation. The recovered measurements from marginalization priors consist of relative pose \\(\\mathbf{z}_\\text{rel}\\) , roll-pitch \\(\\mathbf{z}_\\text{rp}\\) , position \\(\\mathbf{z}_\\text{pos}\\) and yaw \\(\\mathbf{z}_\\text{yaw}\\) residuals to constrain the poses \\(\\mathbf{T}\\) (the first two are used). The approximation of NFR benefits from the full-rank invertable Jacobian \\(\\mathbf{J}_\\text{r}\\) from the recovering information matrices.","title":"Visual-Inertial Mapping with Non-Linear Factor Recovery"},{"location":"2019/2019%20May/#dense-surface-reconstruction-from-monocular-vision-and-lidar","text":"This paper proposes anew pipeline combining LiDAR and camera in a MVS reconstruction framework to create a dense point cloud of indoor environments. It first densifies the MVS points by (inaccurate) lidar points, which improves the depth estimation in textureless areas. An extension of the Delaunay tetrahedra and graph-cut framework described in [9] to include both LiDAR and camera measurements in surface mesh extraction. In detail, all \\(\\boldsymbol{P}_c\\) , and parts of \\(\\boldsymbol{P}_d\\) , which are not in any depth map \\(\\boldsymbol{D}_p\\) , are inserted. The edge value is changed to \\[ E(\\mathbf{T}) = E_{visibility} + \\lambda_{quality}E_{quality} + \\lambda_{lidar}E_{lidar}. \\] The first two terms are the same as [10], except the visibility from lidar points. The third lidar smoothing term is determined by the composition of points (both from camera or lidar, or mixed). [9] Labatut, P., Pons, J. P., & Keriven, R. (2007, October). Efficient multi-view reconstruction of large-scale scenes using interest points, delaunay triangulation and graph cuts. In 2007 IEEE 11th international conference on computer vision (pp. 1-8). IEEE. [10] Labatut, P., Pons, J. P., & Keriven, R. (2009, December). Robust and efficient surface reconstruction from range data. In Computer graphics forum (Vol. 28, No. 8, pp. 2275-2290). Oxford, UK: Blackwell Publishing Ltd.","title":"Dense Surface Reconstruction from Monocular Vision and LiDAR"},{"location":"2019/2019%20Nov/","text":"2019 November Reading Reports LiDAR-enhanced Structure-from-Motion This work introduces a pipeline, which processes Lidar and stereo SfM. Detailed initialization by stereo and LiDAR occupancy is introduced for the limited overlapped areas. LiDAR data are used to reject invalid matches of images. It extends the previous proposed joint optimization 1 pipeline by considering the shared structures of the stereo camera and LiDAR. Dense 3D Reconstruction for Visual Tunnel Inspection using Unmanned Aerial Vehicle This is not an interesting work. It uses range sensors to localize in the tunnel and uses RepMatch 2 to estimate camera pose. The tunnel geometry prior is used to prune points. A cylindrical projection 2D image for tunnels and multiple planes for indoor rooms and underpasses are used for texture mapping. Vision-Aided Localization For Ground Robots This paper introduces an algorithm for approximating the motion manifold for ground robots by parametric representation and performing pose integrating via IMU and wheel odometer measurements; a complete localization algorithm, by using a sliding-window based estimator. The manifold representation uses 6 parameters to present the local manifold as quadratic polynomial. These parameters are the additional states to be estimated for each keyframe. Zhen, W., Hu, Y., Liu, J., & Scherer, S. (2019). A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3-D Reconstructions. IEEE Robotics and Automation Letters, 4(4), 3585-3592. \u21a9 Lin, W. Y., Liu, S., Jiang, N., Do, M. N., Tan, P., & Lu, J. (2016, October). Repmatch: Robust feature matching and pose for reconstructing modern cities. In European Conference on Computer Vision (pp. 562-579). Springer, Cham. \u21a9","title":"2019 November Reading Reports"},{"location":"2019/2019%20Nov/#2019-november-reading-reports","text":"","title":"2019 November Reading Reports"},{"location":"2019/2019%20Nov/#lidar-enhanced-structure-from-motion","text":"This work introduces a pipeline, which processes Lidar and stereo SfM. Detailed initialization by stereo and LiDAR occupancy is introduced for the limited overlapped areas. LiDAR data are used to reject invalid matches of images. It extends the previous proposed joint optimization 1 pipeline by considering the shared structures of the stereo camera and LiDAR.","title":"LiDAR-enhanced Structure-from-Motion"},{"location":"2019/2019%20Nov/#dense-3d-reconstruction-for-visual-tunnel-inspection-using-unmanned-aerial-vehicle","text":"This is not an interesting work. It uses range sensors to localize in the tunnel and uses RepMatch 2 to estimate camera pose. The tunnel geometry prior is used to prune points. A cylindrical projection 2D image for tunnels and multiple planes for indoor rooms and underpasses are used for texture mapping.","title":"Dense 3D Reconstruction for Visual Tunnel Inspection using Unmanned Aerial Vehicle"},{"location":"2019/2019%20Nov/#vision-aided-localization-for-ground-robots","text":"This paper introduces an algorithm for approximating the motion manifold for ground robots by parametric representation and performing pose integrating via IMU and wheel odometer measurements; a complete localization algorithm, by using a sliding-window based estimator. The manifold representation uses 6 parameters to present the local manifold as quadratic polynomial. These parameters are the additional states to be estimated for each keyframe. Zhen, W., Hu, Y., Liu, J., & Scherer, S. (2019). A Joint Optimization Approach of LiDAR-Camera Fusion for Accurate Dense 3-D Reconstructions. IEEE Robotics and Automation Letters, 4(4), 3585-3592. \u21a9 Lin, W. Y., Liu, S., Jiang, N., Do, M. N., Tan, P., & Lu, J. (2016, October). Repmatch: Robust feature matching and pose for reconstructing modern cities. In European Conference on Computer Vision (pp. 562-579). Springer, Cham. \u21a9","title":"Vision-Aided Localization For Ground Robots"},{"location":"2019/2019%20Oct/","text":"2019 October Reading Reports Pose Estimation for Omni-directional Cameras using Sinusoid Fitting This paper proposes a pose estimation method based on geometric vision and fitting of pixel displacement values to sinusoidal functions. It formulate displacement of the point on cylinder model as two sinusoidal functions and fit the curve by iFMI algorithm 1 . In the evaluation is unfair since it only tests the rotation at one single axis and no translation test. It also mentions that the translation can be calculated only when an equal distance from the camera to all the points, which is not generalized. Improvements to Target-Based 3D LiDAR to Camera Calibration This paper introduces a target-based lidar to camera calibration method. It makes use of the target\u2019s geometry when estimating its vertices. It uses the sum of the distance of a point to a 3D-reference target ( fit a reference target with soft \\(L_1\\) norm) in the LiDAR frame to calculate the target to lidar transform ( \\(GL_1\\) ). The edges of the lidar points a fitted by four lines with square constraints ( \\(GN\\) ). Finally, PnP or IoU optimization ( \\(O\\) ) is used for refining LiDAR to camera transformation based on the regressed target vertices. The optimization is an alternating two-step process, which contains adjusting the vertices with \\(O\\) & \\(GL_1\\) / \\(GN\\) and updating transform with the updated vertices. In the result, the refinement method did not provide significant improvement to \\(GL_1\\) . The target is planar, square, and rotated in the frame of the LiDAR by roughly 45\u00b0 to form a diamond. I conjecture that the result is small because it uses are true reference target, whose vertices are fit the ones in the image better. Ref: Joint intrinsic and extrinsic calibration 2 . Object-Based Visual-Inertial Tracking for Additive Fabrication This paper introduces a method to track the motion of a sensor-head relative to multiple objects of known geometry. The sensor fusion consists of IMU state and object pose. The 3D geometry of the scene is projected into the image plane. The object edges are tracked by finding the closest orientation and pixels nearby. After tracking loss, a restart way is proposed, where the detection exploits the information from IMU. Point-Based Multi-View Stereo Network This paper proposes a method of multi-view stereo network. It first generates a coarse depth map for reference image. A lighter MVSNet 3 is used to obtain coarse depth. The 2D-3D feature lifting associates the 2D image information with 3D geometry priors. It contains image feature pyramid and dynamic feature fetching. The augmented point concatenating of the fetched image feature (with variance metric) and the normalized point coordinate is the input to PointFlow . The novel PointFlow module iteratively refine the depth map. The point hypotheses are generated along the direction along the camera. Edge convolution is defined as a channel-wise symmetric aggregation operation. The flow prediction provides the probabilistic weighted sum of displacement. The depth residual map will be added to the initial input depth for depth refinement. The decrease the depth interval s at each iteration. Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM This paper uses a method to select features in VO/VSLAM. It uses Max-logDet metric, solved by stochastic greedy 4 , to select subset of features. The method is aimed at reducing bias of pose optimization, while an acceptable amount of variance is introduced (from using few features). The improvement is marginal in the simulation and real-world results. Schwertfeger, S., Birk, A., & B\u00fclow, H. (2011, November). Using iFMI spectral registration for video stabilization and motion detection by an Unmanned Aerial Vehicle (UAV). In 2011 IEEE International Symposium on Safety, Security, and Rescue Robotics (pp. 61-67). IEEE. \u21a9 Mirzaei, F. M., Kottas, D. G., & Roumeliotis, S. I. (2012). 3D LIDAR\u2013camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization. The International Journal of Robotics Research, 31(4), 452-467. \u21a9 Yao, Y., Luo, Z., Li, S., Fang, T., & Quan, L. (2018). Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 767-783). \u21a9 Mirzasoleiman, B., Badanidiyuru, A., Karbasi, A., Vondr\u00e1k, J., & Krause, A. (2015, February). Lazier than lazy greedy. In Twenty-Ninth AAAI Conference on Artificial Intelligence. \u21a9","title":"2019 October Reading Reports"},{"location":"2019/2019%20Oct/#2019-october-reading-reports","text":"","title":"2019 October Reading Reports"},{"location":"2019/2019%20Oct/#pose-estimation-for-omni-directional-cameras-using-sinusoid-fitting","text":"This paper proposes a pose estimation method based on geometric vision and fitting of pixel displacement values to sinusoidal functions. It formulate displacement of the point on cylinder model as two sinusoidal functions and fit the curve by iFMI algorithm 1 . In the evaluation is unfair since it only tests the rotation at one single axis and no translation test. It also mentions that the translation can be calculated only when an equal distance from the camera to all the points, which is not generalized.","title":"Pose Estimation for Omni-directional Cameras using Sinusoid Fitting"},{"location":"2019/2019%20Oct/#improvements-to-target-based-3d-lidar-to-camera-calibration","text":"This paper introduces a target-based lidar to camera calibration method. It makes use of the target\u2019s geometry when estimating its vertices. It uses the sum of the distance of a point to a 3D-reference target ( fit a reference target with soft \\(L_1\\) norm) in the LiDAR frame to calculate the target to lidar transform ( \\(GL_1\\) ). The edges of the lidar points a fitted by four lines with square constraints ( \\(GN\\) ). Finally, PnP or IoU optimization ( \\(O\\) ) is used for refining LiDAR to camera transformation based on the regressed target vertices. The optimization is an alternating two-step process, which contains adjusting the vertices with \\(O\\) & \\(GL_1\\) / \\(GN\\) and updating transform with the updated vertices. In the result, the refinement method did not provide significant improvement to \\(GL_1\\) . The target is planar, square, and rotated in the frame of the LiDAR by roughly 45\u00b0 to form a diamond. I conjecture that the result is small because it uses are true reference target, whose vertices are fit the ones in the image better. Ref: Joint intrinsic and extrinsic calibration 2 .","title":"Improvements to Target-Based 3D LiDAR to Camera Calibration"},{"location":"2019/2019%20Oct/#object-based-visual-inertial-tracking-for-additive-fabrication","text":"This paper introduces a method to track the motion of a sensor-head relative to multiple objects of known geometry. The sensor fusion consists of IMU state and object pose. The 3D geometry of the scene is projected into the image plane. The object edges are tracked by finding the closest orientation and pixels nearby. After tracking loss, a restart way is proposed, where the detection exploits the information from IMU.","title":"Object-Based Visual-Inertial Tracking for Additive Fabrication"},{"location":"2019/2019%20Oct/#point-based-multi-view-stereo-network","text":"This paper proposes a method of multi-view stereo network. It first generates a coarse depth map for reference image. A lighter MVSNet 3 is used to obtain coarse depth. The 2D-3D feature lifting associates the 2D image information with 3D geometry priors. It contains image feature pyramid and dynamic feature fetching. The augmented point concatenating of the fetched image feature (with variance metric) and the normalized point coordinate is the input to PointFlow . The novel PointFlow module iteratively refine the depth map. The point hypotheses are generated along the direction along the camera. Edge convolution is defined as a channel-wise symmetric aggregation operation. The flow prediction provides the probabilistic weighted sum of displacement. The depth residual map will be added to the initial input depth for depth refinement. The decrease the depth interval s at each iteration.","title":"Point-Based Multi-View Stereo Network"},{"location":"2019/2019%20Oct/#good-feature-selection-for-least-squares-pose-optimization-in-vovslam","text":"This paper uses a method to select features in VO/VSLAM. It uses Max-logDet metric, solved by stochastic greedy 4 , to select subset of features. The method is aimed at reducing bias of pose optimization, while an acceptable amount of variance is introduced (from using few features). The improvement is marginal in the simulation and real-world results. Schwertfeger, S., Birk, A., & B\u00fclow, H. (2011, November). Using iFMI spectral registration for video stabilization and motion detection by an Unmanned Aerial Vehicle (UAV). In 2011 IEEE International Symposium on Safety, Security, and Rescue Robotics (pp. 61-67). IEEE. \u21a9 Mirzaei, F. M., Kottas, D. G., & Roumeliotis, S. I. (2012). 3D LIDAR\u2013camera intrinsic and extrinsic calibration: Identifiability and analytical least-squares-based initialization. The International Journal of Robotics Research, 31(4), 452-467. \u21a9 Yao, Y., Luo, Z., Li, S., Fang, T., & Quan, L. (2018). Mvsnet: Depth inference for unstructured multi-view stereo. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 767-783). \u21a9 Mirzasoleiman, B., Badanidiyuru, A., Karbasi, A., Vondr\u00e1k, J., & Krause, A. (2015, February). Lazier than lazy greedy. In Twenty-Ninth AAAI Conference on Artificial Intelligence. \u21a9","title":"Good Feature Selection for Least Squares Pose Optimization in VO/VSLAM"},{"location":"2019/2019%20Sep/","text":"2019 September Reading Reports Omnidirectional DSO: Direct Sparse Odometry With Fisheye Cameras This paper used MEI camera model and inverse distance to achieve fisheye DSO 1 . Engel, J., Koltun, V., & Cremers, D. (2017). Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3), 611-625. \u21a9","title":"2019 September Reading Reports"},{"location":"2019/2019%20Sep/#2019-september-reading-reports","text":"","title":"2019 September Reading Reports"},{"location":"2019/2019%20Sep/#omnidirectional-dso-direct-sparse-odometry-with-fisheye-cameras","text":"This paper used MEI camera model and inverse distance to achieve fisheye DSO 1 . Engel, J., Koltun, V., & Cremers, D. (2017). Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3), 611-625. \u21a9","title":"Omnidirectional DSO: Direct Sparse Odometry With Fisheye Cameras"},{"location":"2020/2020%20Apr/","text":"2020 April Reading Reports SuperGlue: Learning Feature Matching with Graph Neural Networks This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. It acts as a middle-end between handcrafted or learned front-end and back-end. SuperGlue is made up of two major components: the attentional graph neural network, and the optimal matching layer. Deep Closest Point: Learning Representations for Point Cloud Registration It embeds point clouds into high-dimensional space using PointNet or DGCNN, encode contextual information using an attention-based module, and finally estimate an alignment using a differentiable SVD layer. Code available at dcp . Dynamic Graph CNN for Learning on Point Clouds It proposes a novel operation (EdgeConv) for learning from point clouds, which dynamically updates a graph of relationships from layer to layer. Code available at dgcnn . OneShot Global Localization: Instant LiDAR-Visual Pose Estimation This paper presented OneShot , a global localization algorithm based on sparse 3D point cloud segmentation. It is an extension of SegMap 1 with sparse point cloud as input, which is segmented by 2 . It is further combined with NetVLAD for the corresponding image segement. Voxel Map for Visual SLAM This paper uses (voxel hashing) voxel map to store visual points inside the voxel. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner. It shows lower computational cost compared to keyframe-based method and avoids the occlusion problem. Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map This paper takes both individual 2D-3D matches and their global compatibilities into consideration. It contains 4 steps, 1) build a map-graph with pairwise covisibility edges, 2) compute query vector q, 3) random walk on map-graph, 4) camera pose computation. After step 1-3 it finds a set-to-set global matching; in the step 4, the points ranked highly and passed a ratio-test will be sent to PnP-RANSAC to calculate the final pose. 3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration This paper introduces detect and describe 3D features points for point cloud. It first separates the point cloud into clusters by PointNet++ fashion sampling. The weights and orientations of clusters are calculated by detector and the descriptors with contextual information are calculated based on the transformed clusters. Feature alignment is achieved by finding the the cluster with the minimum feature distance. D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features It first adopts KPConv as its backbone network to extract dense feature. With modified convolution, saliency score function and detector loss, it aims at solving invariance of sparsity of point cloud. It trains the detector and descriptor jointly using contrastive margin loss. Dub\u00e9, R., Cramariuc, A., Dugas, D., Nieto, J., Siegwart, R., & Cadena, C. SegMap: 3D Segment Mapping using Data-Driven Descriptors. \u21a9 Bogoslavskyi, I., & Stachniss, C. (2016, October). Fast range image-based segmentation of sparse 3D laser scans for online operation. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 163-169). IEEE. \u21a9","title":"2020 Apr"},{"location":"2020/2020%20Apr/#2020-april-reading-reports","text":"","title":"2020 April Reading Reports"},{"location":"2020/2020%20Apr/#superglue-learning-feature-matching-with-graph-neural-networks","text":"This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. It acts as a middle-end between handcrafted or learned front-end and back-end. SuperGlue is made up of two major components: the attentional graph neural network, and the optimal matching layer.","title":"SuperGlue: Learning Feature Matching with Graph Neural Networks"},{"location":"2020/2020%20Apr/#deep-closest-point-learning-representations-for-point-cloud-registration","text":"It embeds point clouds into high-dimensional space using PointNet or DGCNN, encode contextual information using an attention-based module, and finally estimate an alignment using a differentiable SVD layer. Code available at dcp .","title":"Deep Closest Point: Learning Representations for Point Cloud Registration"},{"location":"2020/2020%20Apr/#dynamic-graph-cnn-for-learning-on-point-clouds","text":"It proposes a novel operation (EdgeConv) for learning from point clouds, which dynamically updates a graph of relationships from layer to layer. Code available at dgcnn .","title":"Dynamic Graph CNN for Learning on Point Clouds"},{"location":"2020/2020%20Apr/#oneshot-global-localization-instant-lidar-visual-pose-estimation","text":"This paper presented OneShot , a global localization algorithm based on sparse 3D point cloud segmentation. It is an extension of SegMap 1 with sparse point cloud as input, which is segmented by 2 . It is further combined with NetVLAD for the corresponding image segement.","title":"OneShot Global Localization: Instant LiDAR-Visual Pose Estimation"},{"location":"2020/2020%20Apr/#voxel-map-for-visual-slam","text":"This paper uses (voxel hashing) voxel map to store visual points inside the voxel. Visible points from a camera pose are queried by sampling the camera frustum in a raycasting manner. It shows lower computational cost compared to keyframe-based method and avoids the occlusion problem.","title":"Voxel Map for Visual SLAM"},{"location":"2020/2020%20Apr/#efficient-global-2d-3d-matching-for-camera-localization-in-a-large-scale-3d-map","text":"This paper takes both individual 2D-3D matches and their global compatibilities into consideration. It contains 4 steps, 1) build a map-graph with pairwise covisibility edges, 2) compute query vector q, 3) random walk on map-graph, 4) camera pose computation. After step 1-3 it finds a set-to-set global matching; in the step 4, the points ranked highly and passed a ratio-test will be sent to PnP-RANSAC to calculate the final pose.","title":"Efficient Global 2D-3D Matching for Camera Localization in a Large-Scale 3D Map"},{"location":"2020/2020%20Apr/#3dfeat-net-weakly-supervised-local-3d-features-for-point-cloud-registration","text":"This paper introduces detect and describe 3D features points for point cloud. It first separates the point cloud into clusters by PointNet++ fashion sampling. The weights and orientations of clusters are calculated by detector and the descriptors with contextual information are calculated based on the transformed clusters. Feature alignment is achieved by finding the the cluster with the minimum feature distance.","title":"3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud Registration"},{"location":"2020/2020%20Apr/#d3feat-joint-learning-of-dense-detection-and-description-of-3d-local-features","text":"It first adopts KPConv as its backbone network to extract dense feature. With modified convolution, saliency score function and detector loss, it aims at solving invariance of sparsity of point cloud. It trains the detector and descriptor jointly using contrastive margin loss. Dub\u00e9, R., Cramariuc, A., Dugas, D., Nieto, J., Siegwart, R., & Cadena, C. SegMap: 3D Segment Mapping using Data-Driven Descriptors. \u21a9 Bogoslavskyi, I., & Stachniss, C. (2016, October). Fast range image-based segmentation of sparse 3D laser scans for online operation. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 163-169). IEEE. \u21a9","title":"D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features"},{"location":"2020/2020%20Aug/","text":"2020 August Reading Reports Elasticity Meets Continuous-Time: Map-Centric Dense 3D LiDAR SLAM This work combines elastic (surfel) and continuous-time mapping.","title":"2020 Aug"},{"location":"2020/2020%20Aug/#2020-august-reading-reports","text":"","title":"2020 August Reading Reports"},{"location":"2020/2020%20Aug/#elasticity-meets-continuous-time-map-centric-dense-3d-lidar-slam","text":"This work combines elastic (surfel) and continuous-time mapping.","title":"Elasticity Meets Continuous-Time: Map-Centric Dense 3D LiDAR SLAM"},{"location":"2020/2020%20Feb/","text":"2020 February Reading Reports PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation It fuses the appearance feature and geometry information from RGBD sensor. The learned feature (backbone: PointNet++) would be fed into a 3D keypoint detection module \\(\\mathcal{M_K}\\) for offset prediction as well as a semantic segmentation module \\(\\mathcal{M_S}\\) and center voting module \\(\\mathcal{M_C}\\) for instance-level segmentation. Multi-task loss is applied for these three tasks. Point Cloud Descriptors for Place Recognition using Sparse Visual Information It proposes a Neighbour-binary landmark density descriptor (NBLD) descriptor for sparse visual (OKVIS) to achieve place recognition. Voting based method is used to determine the final match. Place Recognition in Semi-Dense Maps: Geometric and Learning-Based Approaches It uses learning-based approach (Siamese network) to achieve place recognition on the semi-dense maps (DSO). PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation The innovation is in a new representation for 2D object keypoints as well as a modi\ufb01ed PnP algorithm for pose estimation. Specifically, the method uses a Pixel-wise Voting Network (PVNet) to detect 2D keypoints in a RANSAC-like fashion, which robustly handles occluded and truncated objects. In the implementation , each object is pre-trained and has its own PyTorch model. Robust 3D visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint and edge features This paper first generate keyframes \\(\\mathcal{F}\\) composed of \\(F\\) sets of SURF keypoints to initialize the system. Then, it refine the edge correspondences via an RANSAC approach. The approach can provide inlier probability (edge-based measurement likelihood) to fit the system into a Particle Filter based framework. Iteratively reweighted least squares (IRLS) is employed to obtain the optimized particles. The first-order AR state dynamics in the paper seems just linear prediction for the particles instead of a random walk model. Re-initialization is decided by an alternative effective particle size. Matching with PROSAC \u2013 Progressive Sample Consensus First, the growth function \\(n = g(t)\\) that defines the set \\(\\mathcal{U}_n\\) of \\(n\\) topranked correspondences that is sampled after \\(t\\) trials must be selected. Second, a stopping criterion contains a non-randomness constraint and a maximality constraint (the linear number should greater than a threshold and the number of samples that has to be drawn, which help to determine the termination length \\(n^*\\) ). It uses the same \\(T_N\\) samples from RANSAC and ordered the samples according to the sample quality. It progressively select larger subsets, and removes one parameter of the matching process \u2013 the threshold on the similarity function for selection of tentative correspondences. Learning to See the Wood for the Trees: Deep Laser Localization in Urban and Natural Environments on a CPU The paper uses Unordered point clouds as input, Feature space is capable of being generalized, and The network can estimate the quality of a match. It separates the problem into Segmentation, Description, Matching and Pose Estimation. \\(\\mathcal{X}\\) -conv operator from PointCNN is applied. Classification branch that estimates a measure of the quality of each descriptor, which classifies whether a feature is good for matching or not. Joint probability of all matched segments are used in the PROSAC 5 method to estimate the pose. Triplet loss and pairwise are used to train the descriptors of segments. Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure It contains Kinematic-inertial odometry from quadruped robots by Two-State Implicit Filter (TSIF) 1 , Autotuned ICP (AICP) 2 to register point clouds, Learned loop-closure proposal, Efficient Segment Matching (ESM) 3 , and geometric loop-closure detection, and Fast verification of point cloud registration (storing the centroids of planes in a kd-tree), the improved version of a previous work 4 . Bloesch, M., Burri, M., Sommer, H., Siegwart, R., & Hutter, M. (2017). The two-state implicit filter recursive estimation for mobile robots. IEEE Robotics and Automation Letters, 3(1), 573-580. \u21a9 Nobili, S., Scona, R., Caravagna, M., & Fallon, M. (2017, May). Overlap-based ICP tuning for robust localization of a humanoid robot. In 2017 IEEE International Conference on Robotics and Automation (ICRA) (pp. 4721-4728). IEEE. \u21a9 Tinchev, G., Penate-Sanchez, A., & Fallon, M. (2019). Learning to see the wood for the trees: Deep laser localization in urban and natural environments on a CPU. IEEE Robotics and Automation Letters, 4(2), 1327-1334. \u21a9 Nobili, S., Tinchev, G., & Fallon, M. (2018, May). Predicting alignment risk to prevent localization failure. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1003-1010). IEEE. \u21a9 Chum, O., & Matas, J. (2005, June). Matching with PROSAC-progressive sample consensus. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05) (Vol. 1, pp. 220-226). IEEE. \u21a9","title":"2020 February Reading Reports"},{"location":"2020/2020%20Feb/#2020-february-reading-reports","text":"","title":"2020 February Reading Reports"},{"location":"2020/2020%20Feb/#pvn3d-a-deep-point-wise-3d-keypoints-voting-network-for-6dof-pose-estimation","text":"It fuses the appearance feature and geometry information from RGBD sensor. The learned feature (backbone: PointNet++) would be fed into a 3D keypoint detection module \\(\\mathcal{M_K}\\) for offset prediction as well as a semantic segmentation module \\(\\mathcal{M_S}\\) and center voting module \\(\\mathcal{M_C}\\) for instance-level segmentation. Multi-task loss is applied for these three tasks.","title":"PVN3D: A Deep Point-wise 3D Keypoints Voting Network for 6DoF Pose Estimation"},{"location":"2020/2020%20Feb/#point-cloud-descriptors-for-place-recognition-using-sparse-visual-information","text":"It proposes a Neighbour-binary landmark density descriptor (NBLD) descriptor for sparse visual (OKVIS) to achieve place recognition. Voting based method is used to determine the final match.","title":"Point Cloud Descriptors for Place Recognition using Sparse Visual Information"},{"location":"2020/2020%20Feb/#place-recognition-in-semi-dense-maps-geometric-and-learning-based-approaches","text":"It uses learning-based approach (Siamese network) to achieve place recognition on the semi-dense maps (DSO).","title":"Place Recognition in Semi-Dense Maps: Geometric and Learning-Based Approaches"},{"location":"2020/2020%20Feb/#pvnet-pixel-wise-voting-network-for-6dof-pose-estimation","text":"The innovation is in a new representation for 2D object keypoints as well as a modi\ufb01ed PnP algorithm for pose estimation. Specifically, the method uses a Pixel-wise Voting Network (PVNet) to detect 2D keypoints in a RANSAC-like fashion, which robustly handles occluded and truncated objects. In the implementation , each object is pre-trained and has its own PyTorch model.","title":"PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation"},{"location":"2020/2020%20Feb/#robust-3d-visual-tracking-using-particle-filtering-on-the-special-euclidean-group-a-combined-approach-of-keypoint-and-edge-features","text":"This paper first generate keyframes \\(\\mathcal{F}\\) composed of \\(F\\) sets of SURF keypoints to initialize the system. Then, it refine the edge correspondences via an RANSAC approach. The approach can provide inlier probability (edge-based measurement likelihood) to fit the system into a Particle Filter based framework. Iteratively reweighted least squares (IRLS) is employed to obtain the optimized particles. The first-order AR state dynamics in the paper seems just linear prediction for the particles instead of a random walk model. Re-initialization is decided by an alternative effective particle size.","title":"Robust 3D visual tracking using particle filtering on the special Euclidean group: A combined approach of keypoint and edge features"},{"location":"2020/2020%20Feb/#matching-with-prosac-progressive-sample-consensus","text":"First, the growth function \\(n = g(t)\\) that defines the set \\(\\mathcal{U}_n\\) of \\(n\\) topranked correspondences that is sampled after \\(t\\) trials must be selected. Second, a stopping criterion contains a non-randomness constraint and a maximality constraint (the linear number should greater than a threshold and the number of samples that has to be drawn, which help to determine the termination length \\(n^*\\) ). It uses the same \\(T_N\\) samples from RANSAC and ordered the samples according to the sample quality. It progressively select larger subsets, and removes one parameter of the matching process \u2013 the threshold on the similarity function for selection of tentative correspondences.","title":"Matching with PROSAC \u2013 Progressive Sample Consensus"},{"location":"2020/2020%20Feb/#learning-to-see-the-wood-for-the-trees-deep-laser-localization-in-urban-and-natural-environments-on-a-cpu","text":"The paper uses Unordered point clouds as input, Feature space is capable of being generalized, and The network can estimate the quality of a match. It separates the problem into Segmentation, Description, Matching and Pose Estimation. \\(\\mathcal{X}\\) -conv operator from PointCNN is applied. Classification branch that estimates a measure of the quality of each descriptor, which classifies whether a feature is good for matching or not. Joint probability of all matched segments are used in the PROSAC 5 method to estimate the pose. Triplet loss and pairwise are used to train the descriptors of segments.","title":"Learning to See the Wood for the Trees: Deep Laser Localization in Urban and Natural Environments on a CPU"},{"location":"2020/2020%20Feb/#online-lidar-slam-for-legged-robots-with-robust-registration-and-deep-learned-loop-closure","text":"It contains Kinematic-inertial odometry from quadruped robots by Two-State Implicit Filter (TSIF) 1 , Autotuned ICP (AICP) 2 to register point clouds, Learned loop-closure proposal, Efficient Segment Matching (ESM) 3 , and geometric loop-closure detection, and Fast verification of point cloud registration (storing the centroids of planes in a kd-tree), the improved version of a previous work 4 . Bloesch, M., Burri, M., Sommer, H., Siegwart, R., & Hutter, M. (2017). The two-state implicit filter recursive estimation for mobile robots. IEEE Robotics and Automation Letters, 3(1), 573-580. \u21a9 Nobili, S., Scona, R., Caravagna, M., & Fallon, M. (2017, May). Overlap-based ICP tuning for robust localization of a humanoid robot. In 2017 IEEE International Conference on Robotics and Automation (ICRA) (pp. 4721-4728). IEEE. \u21a9 Tinchev, G., Penate-Sanchez, A., & Fallon, M. (2019). Learning to see the wood for the trees: Deep laser localization in urban and natural environments on a CPU. IEEE Robotics and Automation Letters, 4(2), 1327-1334. \u21a9 Nobili, S., Tinchev, G., & Fallon, M. (2018, May). Predicting alignment risk to prevent localization failure. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 1003-1010). IEEE. \u21a9 Chum, O., & Matas, J. (2005, June). Matching with PROSAC-progressive sample consensus. In 2005 IEEE computer society conference on computer vision and pattern recognition (CVPR'05) (Vol. 1, pp. 220-226). IEEE. \u21a9","title":"Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure"},{"location":"2020/2020%20Jan/","text":"2020 January Reading Reports SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory This paper introduces a method of semantic LOAM. The cylinder parameters of trees are estimated along with the motion parameters. The detection is based on range images and the trellis graph is used for data association. VR is used for efficient cylinder labeling. Learning Meshes for Dense Visual SLAM This paper uses use triangular meshes to represent the dense camera frame structure. The in-plane vertex coordinates are trained, but the vertex depth components as free variables and are optimisable based on learnable residuals. It first trains the in-plane vertices with the reconstruction error. A prior factor and a stereo factor as residuals are learned by the end-to-end method. KPConv: Flexible and Deformable Convolution for Point Clouds Kernel Point Convolution (KPConv) is proposed in this paper. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. KPConv takes radius neighborhoods as input and processes them with weights spatially located by a small set of kernel points. It is further extended to deformable convolutions for complex tasks. It uses points as the coordinates (like pixel in image). Then the kernel points use linear correlation to calculate the interpolation within a radius. For the deformable convolution, \"fitting\" regularization loss is required. PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization This paper proposes an approach for long-term localization using 3D LiDARs, coined PoseMap. It consists of local views of features which are used for localization and can be updated or extended at run-time. The method uses adapted C-SLAM as baseline method for localization, where features from both map and sliding window are matched, and window shift size is dynamic. The main contribution is the a map representation, which is a set of map nodes that contain their own sets of 3D surfels. This is not very interesting, more or less a sub-map selection (2 sub-maps) and matching method. Fast Nonlinear Approximation of Pose Graph Node Marginalization This paper proposes an approximation preserves the pose graph structure by re-parameterizing rom absolute- to relative-pose spaces. It joins the approximation process with a scaled version of pose-composition approach for the degenerate low-rank Gaussian distribution. This paper should be read with the the original pose composition paper [^2020 Jan 1]. Eade, E., Fong, P., & Munich, M. E. (2010, October). Monocular graph SLAM with complexity reduction. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (pp. 3017-3024). IEEE. \u21a9","title":"2020 January Reading Reports"},{"location":"2020/2020%20Jan/#2020-january-reading-reports","text":"","title":"2020 January Reading Reports"},{"location":"2020/2020%20Jan/#sloam-semantic-lidar-odometry-and-mapping-for-forest-inventory","text":"This paper introduces a method of semantic LOAM. The cylinder parameters of trees are estimated along with the motion parameters. The detection is based on range images and the trellis graph is used for data association. VR is used for efficient cylinder labeling.","title":"SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory"},{"location":"2020/2020%20Jan/#learning-meshes-for-dense-visual-slam","text":"This paper uses use triangular meshes to represent the dense camera frame structure. The in-plane vertex coordinates are trained, but the vertex depth components as free variables and are optimisable based on learnable residuals. It first trains the in-plane vertices with the reconstruction error. A prior factor and a stereo factor as residuals are learned by the end-to-end method.","title":"Learning Meshes for Dense Visual SLAM"},{"location":"2020/2020%20Jan/#kpconv-flexible-and-deformable-convolution-for-point-clouds","text":"Kernel Point Convolution (KPConv) is proposed in this paper. The convolution weights of KPConv are located in Euclidean space by kernel points, and applied to the input points close to them. KPConv takes radius neighborhoods as input and processes them with weights spatially located by a small set of kernel points. It is further extended to deformable convolutions for complex tasks. It uses points as the coordinates (like pixel in image). Then the kernel points use linear correlation to calculate the interpolation within a radius. For the deformable convolution, \"fitting\" regularization loss is required.","title":"KPConv: Flexible and Deformable Convolution for Point Clouds"},{"location":"2020/2020%20Jan/#posemap-lifelong-multi-environment-3d-lidar-localization","text":"This paper proposes an approach for long-term localization using 3D LiDARs, coined PoseMap. It consists of local views of features which are used for localization and can be updated or extended at run-time. The method uses adapted C-SLAM as baseline method for localization, where features from both map and sliding window are matched, and window shift size is dynamic. The main contribution is the a map representation, which is a set of map nodes that contain their own sets of 3D surfels. This is not very interesting, more or less a sub-map selection (2 sub-maps) and matching method.","title":"PoseMap: Lifelong, Multi-Environment 3D LiDAR Localization"},{"location":"2020/2020%20Jan/#fast-nonlinear-approximation-of-pose-graph-node-marginalization","text":"This paper proposes an approximation preserves the pose graph structure by re-parameterizing rom absolute- to relative-pose spaces. It joins the approximation process with a scaled version of pose-composition approach for the degenerate low-rank Gaussian distribution. This paper should be read with the the original pose composition paper [^2020 Jan 1]. Eade, E., Fong, P., & Munich, M. E. (2010, October). Monocular graph SLAM with complexity reduction. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (pp. 3017-3024). IEEE. \u21a9","title":"Fast Nonlinear Approximation of Pose Graph Node Marginalization"},{"location":"2020/2020%20Jul/","text":"2020 July Reading Reports VDO-SLAM: A Visual Dynamic Object-aware SLAM System This paper proposes a method for dynamic object-aware SLAM system (RGB-D system, depth from stereo or mono-learning). It represents the object points by a homogeneous transformation ( a frame change of a pose transformation , global reference frame pose change). This is a model-free manner for the points that reside on the object. Both camera pose and object motion are estimated by re-projection of background or object points. It also combines the initial optical flow to the re-projection error to estimate a refined optical flow. In the graph optimization, odometry binary factors, point measurement binary factors, point motion ternary factors, smooth motion binary factor are used. It adopts Mask R-CNN for segmentation masks, PWC-Net for optical flow, MonoDepth2 for monocular depth estimation, FAST for detect corner points. Real-Time Wide-Baseline Place Recognition Using Depth Completion This paper first uses the framework of BoBW 1 to find loop-closure candidates and filter the candidates more permissively. Then, it densifies the sparse OKVIS feature points by interpolating 3D landmarks. 2D keypoints that have no depth-estimates will be assigned interpolated depths. Geometric check from 2 is adapted to first estimate 3D-3D matches, if not then 3D-2D matches, which helps to select the candidate with biggest number of inliers. Some real and synthetic dataset with large wide-baseline is available at link . G\u00e1lvez-L\u00f3pez, D., & Tardos, J. D. (2012). Bags of binary words for fast place recognition in image sequences. IEEE Transactions on Robotics , 28 (5), 1188-1197. \u21a9 Maffra, F., Chen, Z., & Chli, M. (2018, May). tolerant Place Recognition combining 2D and 3D information for UAV navigation. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 2542-2549). IEEE. \u21a9","title":"2020 Jul"},{"location":"2020/2020%20Jul/#2020-july-reading-reports","text":"","title":"2020 July Reading Reports"},{"location":"2020/2020%20Jul/#vdo-slam-a-visual-dynamic-object-aware-slam-system","text":"This paper proposes a method for dynamic object-aware SLAM system (RGB-D system, depth from stereo or mono-learning). It represents the object points by a homogeneous transformation ( a frame change of a pose transformation , global reference frame pose change). This is a model-free manner for the points that reside on the object. Both camera pose and object motion are estimated by re-projection of background or object points. It also combines the initial optical flow to the re-projection error to estimate a refined optical flow. In the graph optimization, odometry binary factors, point measurement binary factors, point motion ternary factors, smooth motion binary factor are used. It adopts Mask R-CNN for segmentation masks, PWC-Net for optical flow, MonoDepth2 for monocular depth estimation, FAST for detect corner points.","title":"VDO-SLAM: A Visual Dynamic Object-aware SLAM System"},{"location":"2020/2020%20Jul/#real-time-wide-baseline-place-recognition-using-depth-completion","text":"This paper first uses the framework of BoBW 1 to find loop-closure candidates and filter the candidates more permissively. Then, it densifies the sparse OKVIS feature points by interpolating 3D landmarks. 2D keypoints that have no depth-estimates will be assigned interpolated depths. Geometric check from 2 is adapted to first estimate 3D-3D matches, if not then 3D-2D matches, which helps to select the candidate with biggest number of inliers. Some real and synthetic dataset with large wide-baseline is available at link . G\u00e1lvez-L\u00f3pez, D., & Tardos, J. D. (2012). Bags of binary words for fast place recognition in image sequences. IEEE Transactions on Robotics , 28 (5), 1188-1197. \u21a9 Maffra, F., Chen, Z., & Chli, M. (2018, May). tolerant Place Recognition combining 2D and 3D information for UAV navigation. In 2018 IEEE International Conference on Robotics and Automation (ICRA) (pp. 2542-2549). IEEE. \u21a9","title":"Real-Time Wide-Baseline Place Recognition Using Depth Completion"},{"location":"2020/2020%20Jun/","text":"2020 June Reading Reports Depth-aware CNN for RGB-D Segmentation This paper introduces two depth-aware network blocks, the depth-aware convolution and depth-aware average pooling. It uses the depth similarity \\(F_\\mathbf{D}\\) to weight the convolution and pooling operations. For the depth-aware convolution, \\[\\mathbf{y}(\\mathbf{p}_0) = \\sum_{\\mathbf{p}_n \\in \\mathcal{R}}{\\mathbf{w}(\\mathbf{p}_n) \\cdot F_\\mathbf{D}(\\mathbf{p}_0, \\mathbf{p}_0 + \\mathbf{p}_n) \\cdot \\mathbf{x}(\\mathbf{p}_0 + \\mathbf{p}_n)}\\] For the depth-aware average pooling, \\[\\mathbf{y}(\\mathbf{p}_0) = \\frac{1}{\\sum_{\\mathbf{p}_n \\in \\mathcal{R}}{F_\\mathbf{D}(\\mathbf{p}_0 + \\mathbf{p}_n)} } \\sum_{\\mathbf{p}_n \\in \\mathcal{R}}{F_\\mathbf{D}(\\mathbf{p}_0, \\mathbf{p}_0 + \\mathbf{p}_n) \\cdot \\mathbf{x}(\\mathbf{p}_0 + \\mathbf{p}_n)}\\] End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds It uses the viewpoints are optimizable via in-network differentiable rendering (Soft Rasterizer for points) with a hard-forward soft-backward scheme , where the viewpoints are optimizable; feature extraction from L2-Net for multi-view patches; a soft-view pooling module fuses features across views attentively with a better gradient flow. Fully Convolutional Geometric Features This paper proposes a 3D fully-convolutional network for geometric feature extraction and new losses (hardest-contrastive and hardest-triplet) for fully-convolutional feature learning, when features are independent and identically distributed (i.i.d.) within a batch no longer holds. Source code is available. The backbone of this paper is MinkowskiEngine 1 . Hash-based negative filtering helps to remove false negatives from the hard negative mining step. GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization It convert images into a multi-dimensional feature map which is invariant to lighting/weather changes. The deep features are trained with a novel Gauss-Newton loss formulation in a self-supervised manner. A Siamese network trained with labels obtained either from simulation data or any state-of-the-art SLAM algorithm. Gauss-Newton loss which is designed to maximize the probability of identifying the correct pixel correspondence, which admits a large basin of convergence for the subsequent Gauss-Newton optimization. Camera Pose Voting for Large-Scale Image-Based Localization This paper proposes a voting-based camera pose estimation. It converts the voting shapes to the global coordinates and uses several filters to reduce the location shapes. Choy, C., Gwak, J., & Savarese, S. (2019). 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3075-3084). \u21a9","title":"2020 Jun"},{"location":"2020/2020%20Jun/#2020-june-reading-reports","text":"","title":"2020 June Reading Reports"},{"location":"2020/2020%20Jun/#depth-aware-cnn-for-rgb-d-segmentation","text":"This paper introduces two depth-aware network blocks, the depth-aware convolution and depth-aware average pooling. It uses the depth similarity \\(F_\\mathbf{D}\\) to weight the convolution and pooling operations. For the depth-aware convolution, \\[\\mathbf{y}(\\mathbf{p}_0) = \\sum_{\\mathbf{p}_n \\in \\mathcal{R}}{\\mathbf{w}(\\mathbf{p}_n) \\cdot F_\\mathbf{D}(\\mathbf{p}_0, \\mathbf{p}_0 + \\mathbf{p}_n) \\cdot \\mathbf{x}(\\mathbf{p}_0 + \\mathbf{p}_n)}\\] For the depth-aware average pooling, \\[\\mathbf{y}(\\mathbf{p}_0) = \\frac{1}{\\sum_{\\mathbf{p}_n \\in \\mathcal{R}}{F_\\mathbf{D}(\\mathbf{p}_0 + \\mathbf{p}_n)} } \\sum_{\\mathbf{p}_n \\in \\mathcal{R}}{F_\\mathbf{D}(\\mathbf{p}_0, \\mathbf{p}_0 + \\mathbf{p}_n) \\cdot \\mathbf{x}(\\mathbf{p}_0 + \\mathbf{p}_n)}\\]","title":"Depth-aware CNN for RGB-D Segmentation"},{"location":"2020/2020%20Jun/#end-to-end-learning-local-multi-view-descriptors-for-3d-point-clouds","text":"It uses the viewpoints are optimizable via in-network differentiable rendering (Soft Rasterizer for points) with a hard-forward soft-backward scheme , where the viewpoints are optimizable; feature extraction from L2-Net for multi-view patches; a soft-view pooling module fuses features across views attentively with a better gradient flow.","title":"End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds"},{"location":"2020/2020%20Jun/#fully-convolutional-geometric-features","text":"This paper proposes a 3D fully-convolutional network for geometric feature extraction and new losses (hardest-contrastive and hardest-triplet) for fully-convolutional feature learning, when features are independent and identically distributed (i.i.d.) within a batch no longer holds. Source code is available. The backbone of this paper is MinkowskiEngine 1 . Hash-based negative filtering helps to remove false negatives from the hard negative mining step.","title":"Fully Convolutional Geometric Features"},{"location":"2020/2020%20Jun/#gn-net-the-gauss-newton-loss-for-multi-weather-relocalization","text":"It convert images into a multi-dimensional feature map which is invariant to lighting/weather changes. The deep features are trained with a novel Gauss-Newton loss formulation in a self-supervised manner. A Siamese network trained with labels obtained either from simulation data or any state-of-the-art SLAM algorithm. Gauss-Newton loss which is designed to maximize the probability of identifying the correct pixel correspondence, which admits a large basin of convergence for the subsequent Gauss-Newton optimization.","title":"GN-Net: The Gauss-Newton Loss for Multi-Weather Relocalization"},{"location":"2020/2020%20Jun/#camera-pose-voting-for-large-scale-image-based-localization","text":"This paper proposes a voting-based camera pose estimation. It converts the voting shapes to the global coordinates and uses several filters to reduce the location shapes. Choy, C., Gwak, J., & Savarese, S. (2019). 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 3075-3084). \u21a9","title":"Camera Pose Voting for Large-Scale Image-Based Localization"},{"location":"2020/2020%20Mar/","text":"2020 March Reading Reports NetVLAD: CNN architecture for weakly supervised place recognition It mimics VLAD in a CNN framework. The hard assignment for clusters is replaced by soft assignment with two additional trainable variables \\(\\{\\mathbf{w}_k\\}\\) , \\(\\{b_k\\}\\) for each cluster center \\(\\{\\mathbf{c}_k\\}\\) . In training, a set of potential positives \\(\\{p_i^q\\}\\) and the set of definite negatives \\(\\{n_j^q\\}\\) are used. From Coarse to Fine: Robust Hierarchical Localization at Large Scale It proposed a coarse-to-fine localization paradigm. It first performs a global image retrieval to obtain prior retrievals (clustered into places using the covisibility graph). Then, it performs local 2D-3D matching within the candidate places to obtain an accurate 6-DoF camera pose. The network (HF-Net) has three heads predicting: i) keypoint detection scores, ii) dense local descriptors and iii) a global image-wide descriptor. MobileNet is used as its encoder backbone and NetVLAD layer is used for global descriptor. Local features is based on SuperPoint architecture. It uses the idea of multi-task distillation to train the sub-networks from teacher networks, which is aimed at solving the data scarcity and augmentation problems. PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection PV-RCNN takes advantages from both the voxel-based feature learning (i.e., 3D sparse convolution) and PointNet-based feature learning (i.e., set abstraction operation) to enable both high-quality 3D proposal generation and flexible receptive fields for improving the 3D detection performance. The method integrates both the multi-scale 3D voxel CNN features and the PointNet-based features to a small set of keypoints by the Voxel Set Abstraction layer . The learned discriminative features of keypoints are then aggregated to the RoI-grid points with multiple receptive fields to capture much richer context information for the fine-grained proposal refinement. The final loss includes region proposal loss \\(L_{rpn}\\) , keypoint segmentation loss \\(L_{seg}\\) and the proposal refinement loss \\(L_{rcnn}\\) . PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation A novel deep neural network PointNet that directly consumes point cloud is proposed. It uses max pool as a symmetric function for unordered input. Local and global information aggregation added for segmentation network. Joint alignment network aligns input and feature to make it invariant to these transformations. Theoretical analysis also provides the universal approximation ability of the network to continuous set functions. It can be considered as summarizing a shape by a sparse set of key points. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space It extends PointNet with generating overlapping partitioning of a point set. The Sampling layer adapts farthest point sampling; the Grouping layer uses ball query and the PointNet layer extracts local regions' features. A Multi-resolution grouping (MRG) is proposed for efficiency; the Point Feature Propagation is used for set segmentation (including inverse distance weighted average for interpolation and unit PointNet similar to one-by-one convolution in CNNs, and a few shared FC and ReLU layers). Deep Hough Voting for 3D Object Detection in Point Clouds This paper uses a voting scheme for point cloud 3D Object Detection. It uses PointNet++ as its backbone to produce seeds and a network further provides votes (offsets to centers of objects and features). Then, farthest point sampling and grouping are used to obtain vote clusters. The vote clusters are independently processed by a \\(\\text{MLP}_1\\) before being max-pooled (channel-wise) to a single feature vector and passed to \\(\\text{MLP}_2\\) where information from different votes are further combined. The output is proposal \\(p\\) , a multidimensional vector, contains objectness score, bounding box parameters, and semantic classification scores, which are used for the final loss function. Code available at votenet . ELF: Embedded Localisation of Features in pre-trained CNN This paper uses feature specific saliency map and automatic data-adaptive thresholding (background, foreground) and a simple descriptor to achieved learning-based features for localization from a pre-trained CNN (for other tasks, e.g., classification). Unsupervised Depth Completion from Visual Inertial Odometry This paper takes advantages of VIO's metric sparse depth measurements \\(z_s\\) . It proposes a two-stage approach, first estimate a rough depth map, then refine it with network. Tessellation of the triangular mesh of these 3D points results scaffolding \\(z_i\\) , which is considered as an input accompanying with the raw image. These two are passed into an encoder-decoder based network. Logarithmic and exponential map layers are more beneficial for training the PoseNet in size. The total loss function consists of photometric consistency, sparse depth consistency (w.r.t \\(z_s\\) ), pose consistency and local smoothness related to the gradient of a pixel.","title":"2020 March Reading Reports"},{"location":"2020/2020%20Mar/#2020-march-reading-reports","text":"","title":"2020 March Reading Reports"},{"location":"2020/2020%20Mar/#netvlad-cnn-architecture-for-weakly-supervised-place-recognition","text":"It mimics VLAD in a CNN framework. The hard assignment for clusters is replaced by soft assignment with two additional trainable variables \\(\\{\\mathbf{w}_k\\}\\) , \\(\\{b_k\\}\\) for each cluster center \\(\\{\\mathbf{c}_k\\}\\) . In training, a set of potential positives \\(\\{p_i^q\\}\\) and the set of definite negatives \\(\\{n_j^q\\}\\) are used.","title":"NetVLAD: CNN architecture for weakly supervised place recognition"},{"location":"2020/2020%20Mar/#from-coarse-to-fine-robust-hierarchical-localization-at-large-scale","text":"It proposed a coarse-to-fine localization paradigm. It first performs a global image retrieval to obtain prior retrievals (clustered into places using the covisibility graph). Then, it performs local 2D-3D matching within the candidate places to obtain an accurate 6-DoF camera pose. The network (HF-Net) has three heads predicting: i) keypoint detection scores, ii) dense local descriptors and iii) a global image-wide descriptor. MobileNet is used as its encoder backbone and NetVLAD layer is used for global descriptor. Local features is based on SuperPoint architecture. It uses the idea of multi-task distillation to train the sub-networks from teacher networks, which is aimed at solving the data scarcity and augmentation problems.","title":"From Coarse to Fine: Robust Hierarchical Localization at Large Scale"},{"location":"2020/2020%20Mar/#pv-rcnn-point-voxel-feature-set-abstraction-for-3d-object-detection","text":"PV-RCNN takes advantages from both the voxel-based feature learning (i.e., 3D sparse convolution) and PointNet-based feature learning (i.e., set abstraction operation) to enable both high-quality 3D proposal generation and flexible receptive fields for improving the 3D detection performance. The method integrates both the multi-scale 3D voxel CNN features and the PointNet-based features to a small set of keypoints by the Voxel Set Abstraction layer . The learned discriminative features of keypoints are then aggregated to the RoI-grid points with multiple receptive fields to capture much richer context information for the fine-grained proposal refinement. The final loss includes region proposal loss \\(L_{rpn}\\) , keypoint segmentation loss \\(L_{seg}\\) and the proposal refinement loss \\(L_{rcnn}\\) .","title":"PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection"},{"location":"2020/2020%20Mar/#pointnet-deep-learning-on-point-sets-for-3d-classification-and-segmentation","text":"A novel deep neural network PointNet that directly consumes point cloud is proposed. It uses max pool as a symmetric function for unordered input. Local and global information aggregation added for segmentation network. Joint alignment network aligns input and feature to make it invariant to these transformations. Theoretical analysis also provides the universal approximation ability of the network to continuous set functions. It can be considered as summarizing a shape by a sparse set of key points.","title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation"},{"location":"2020/2020%20Mar/#pointnet-deep-hierarchical-feature-learning-on-point-sets-in-a-metric-space","text":"It extends PointNet with generating overlapping partitioning of a point set. The Sampling layer adapts farthest point sampling; the Grouping layer uses ball query and the PointNet layer extracts local regions' features. A Multi-resolution grouping (MRG) is proposed for efficiency; the Point Feature Propagation is used for set segmentation (including inverse distance weighted average for interpolation and unit PointNet similar to one-by-one convolution in CNNs, and a few shared FC and ReLU layers).","title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space"},{"location":"2020/2020%20Mar/#deep-hough-voting-for-3d-object-detection-in-point-clouds","text":"This paper uses a voting scheme for point cloud 3D Object Detection. It uses PointNet++ as its backbone to produce seeds and a network further provides votes (offsets to centers of objects and features). Then, farthest point sampling and grouping are used to obtain vote clusters. The vote clusters are independently processed by a \\(\\text{MLP}_1\\) before being max-pooled (channel-wise) to a single feature vector and passed to \\(\\text{MLP}_2\\) where information from different votes are further combined. The output is proposal \\(p\\) , a multidimensional vector, contains objectness score, bounding box parameters, and semantic classification scores, which are used for the final loss function. Code available at votenet .","title":"Deep Hough Voting for 3D Object Detection in Point Clouds"},{"location":"2020/2020%20Mar/#elf-embedded-localisation-of-features-in-pre-trained-cnn","text":"This paper uses feature specific saliency map and automatic data-adaptive thresholding (background, foreground) and a simple descriptor to achieved learning-based features for localization from a pre-trained CNN (for other tasks, e.g., classification).","title":"ELF: Embedded Localisation of Features in pre-trained CNN"},{"location":"2020/2020%20Mar/#unsupervised-depth-completion-from-visual-inertial-odometry","text":"This paper takes advantages of VIO's metric sparse depth measurements \\(z_s\\) . It proposes a two-stage approach, first estimate a rough depth map, then refine it with network. Tessellation of the triangular mesh of these 3D points results scaffolding \\(z_i\\) , which is considered as an input accompanying with the raw image. These two are passed into an encoder-decoder based network. Logarithmic and exponential map layers are more beneficial for training the PoseNet in size. The total loss function consists of photometric consistency, sparse depth consistency (w.r.t \\(z_s\\) ), pose consistency and local smoothness related to the gradient of a pixel.","title":"Unsupervised Depth Completion from Visual Inertial Odometry"},{"location":"2020/2020%20May/","text":"2020 May Reading Reports Learning multiview 3D point cloud registration This paper introduces an end-to-end multiview 3D registration algorithm, which contains 1) pairwise registration of point clouds, 2) differentiable transformation synchronization, and 3) iterative refinement of the registration by iterative reweighted least squares (IRLS). It uses FCGF descriptor to extract features from point clouds and replaces KNN with softNN and other weights / confidences in the initial stage or in the iteration stage with networks. Global visual localization in LiDAR-maps through shared 2D-3D embedding space This paper leverages the networks (VGG16+NetVLAD), and a modified version of (SECOND+ASPP+NetVLAD) in order to extract Embedding Vectors (EVs) from both 2D and 3D data. Deep Fundamental Matrix Estimation This paper replaces the weighting function in weighted least square problem by network. PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume.","title":"2020 May"},{"location":"2020/2020%20May/#2020-may-reading-reports","text":"","title":"2020 May Reading Reports"},{"location":"2020/2020%20May/#learning-multiview-3d-point-cloud-registration","text":"This paper introduces an end-to-end multiview 3D registration algorithm, which contains 1) pairwise registration of point clouds, 2) differentiable transformation synchronization, and 3) iterative refinement of the registration by iterative reweighted least squares (IRLS). It uses FCGF descriptor to extract features from point clouds and replaces KNN with softNN and other weights / confidences in the initial stage or in the iteration stage with networks.","title":"Learning multiview 3D point cloud registration"},{"location":"2020/2020%20May/#global-visual-localization-in-lidar-maps-through-shared-2d-3d-embedding-space","text":"This paper leverages the networks (VGG16+NetVLAD), and a modified version of (SECOND+ASPP+NetVLAD) in order to extract Embedding Vectors (EVs) from both 2D and 3D data.","title":"Global visual localization in LiDAR-maps through shared 2D-3D embedding space"},{"location":"2020/2020%20May/#deep-fundamental-matrix-estimation","text":"This paper replaces the weighting function in weighted least square problem by network.","title":"Deep Fundamental Matrix Estimation"},{"location":"2020/2020%20May/#pwc-net-cnns-for-optical-flow-using-pyramid-warping-and-cost-volume","text":"PWC-Net has been designed according to simple and well-established principles: pyramidal processing, warping, and the use of a cost volume.","title":"PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume"},{"location":"2020/2020%20Oct/","text":"2020 October Reading Reports Stochastic Bundle Adjustment for Efficient and Scalable 3D Reconstruction This paper introduces an approach to solve the reduced camera system (RCS) efficiently. It formulates the problem into a quadratic programming first by applying point splitting. Then, change constrained relaxation is used to solve the problem partialy. It corrects the direction of the optimization when the \\(\\lambda\\) in LM algorithm is large by approximating \\(\\mathbf{H}_\\lambda\\) as \\(\\text{diag}(\\mathbf{H}_\\lambda)\\) . Stochastic graph clustering is used to split the whole problem into several sub-problems.","title":"2020 Oct"},{"location":"2020/2020%20Oct/#2020-october-reading-reports","text":"","title":"2020 October Reading Reports"},{"location":"2020/2020%20Oct/#stochastic-bundle-adjustment-for-efficient-and-scalable-3d-reconstruction","text":"This paper introduces an approach to solve the reduced camera system (RCS) efficiently. It formulates the problem into a quadratic programming first by applying point splitting. Then, change constrained relaxation is used to solve the problem partialy. It corrects the direction of the optimization when the \\(\\lambda\\) in LM algorithm is large by approximating \\(\\mathbf{H}_\\lambda\\) as \\(\\text{diag}(\\mathbf{H}_\\lambda)\\) . Stochastic graph clustering is used to split the whole problem into several sub-problems.","title":"Stochastic Bundle Adjustment for Efficient and Scalable 3D Reconstruction"},{"location":"2020/2020%20Sep/","text":"2020 September Reading Reports GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized Gaussian process map reconstruction This work uses Gaussian process (GP) map reconstruction to provide state estimate and mapping. Each cell with valid map points are trained into three functions in the form of GP based on the relationship between the coordinates. Test locations for each function are set with fixed intervals to form a layer with a set of samples . A principled down-sample method is used to accelerate the training of GP. MLE based method taking the distance of projected target and source samples to the matched direction is utilized to perform alignment and refinement with the global map. Deep Probabilistic Feature-metric Tracking This paper first uses two-view to extract the spatial and temporal correlation between two frames by a spatio-temporal encoder pyramid network \\(\\phi_\\theta\\) . Then, feature encoder branch \\(\\phi_F\\) and the uncertainty encoder branch \\(\\phi_\\sigma\\) extract the feature and uncertainty, respectively. Inverse compositional formulation is used to facilitate the Gaussian Newton optimization. Combined with ICP and coarse-to-fine optimization, the network is trained by 3D EndPoint-Error (EPE) as the training loss. MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem This paper uses null-space projection, which solves the tangent space residuals linearly and followed by GN optimization. The nonsingular covariance is calculated by null-space matrix transformation. Distributed Variable-Baseline Stereo SLAM from two UAVs This paper introduces a method of two UAV stereo SLAM. It uses Asynchronous-parallel Alternating Direction Method of Multipliers (ADMM) to perform collaborative update of states. It contains EKF-based Pose Tracking, Mapping and Distributed Optimization Back-End, where UWB distance-measurements are used as a constraint and Z-spline is used for pose interpolation. Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry This paper extracts planes in the images and use RANSAC to filter the outlier. The parameterization makes the points and lines on the planes be represented by the plane coefficients, which is in \\(S^2 + \\mathbb{R}\\) (update the tangent space \\(S^2\\) ).","title":"2020 Sep"},{"location":"2020/2020%20Sep/#2020-september-reading-reports","text":"","title":"2020 September Reading Reports"},{"location":"2020/2020%20Sep/#gp-slam-real-time-3d-lidar-slam-based-on-improved-regionalized-gaussian-process-map-reconstruction","text":"This work uses Gaussian process (GP) map reconstruction to provide state estimate and mapping. Each cell with valid map points are trained into three functions in the form of GP based on the relationship between the coordinates. Test locations for each function are set with fixed intervals to form a layer with a set of samples . A principled down-sample method is used to accelerate the training of GP. MLE based method taking the distance of projected target and source samples to the matched direction is utilized to perform alignment and refinement with the global map.","title":"GP-SLAM+: real-time 3D lidar SLAM based on improved regionalized Gaussian process map reconstruction"},{"location":"2020/2020%20Sep/#deep-probabilistic-feature-metric-tracking","text":"This paper first uses two-view to extract the spatial and temporal correlation between two frames by a spatio-temporal encoder pyramid network \\(\\phi_\\theta\\) . Then, feature encoder branch \\(\\phi_F\\) and the uncertainty encoder branch \\(\\phi_\\sigma\\) extract the feature and uncertainty, respectively. Inverse compositional formulation is used to facilitate the Gaussian Newton optimization. Combined with ICP and coarse-to-fine optimization, the network is trained by 3D EndPoint-Error (EPE) as the training loss.","title":"Deep Probabilistic Feature-metric Tracking"},{"location":"2020/2020%20Sep/#mlpnp-a-real-time-maximum-likelihood-solution-to-the-perspective-n-point-problem","text":"This paper uses null-space projection, which solves the tangent space residuals linearly and followed by GN optimization. The nonsingular covariance is calculated by null-space matrix transformation.","title":"MLPnP - A Real-Time Maximum Likelihood Solution to the Perspective-n-Point Problem"},{"location":"2020/2020%20Sep/#distributed-variable-baseline-stereo-slam-from-two-uavs","text":"This paper introduces a method of two UAV stereo SLAM. It uses Asynchronous-parallel Alternating Direction Method of Multipliers (ADMM) to perform collaborative update of states. It contains EKF-based Pose Tracking, Mapping and Distributed Optimization Back-End, where UWB distance-measurements are used as a constraint and Z-spline is used for pose interpolation.","title":"Distributed Variable-Baseline Stereo SLAM from two UAVs"},{"location":"2020/2020%20Sep/#co-planar-parametrization-for-stereo-slam-and-visual-inertial-odometry","text":"This paper extracts planes in the images and use RANSAC to filter the outlier. The parameterization makes the points and lines on the planes be represented by the plane coefficients, which is in \\(S^2 + \\mathbb{R}\\) (update the tangent space \\(S^2\\) ).","title":"Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry"}]}